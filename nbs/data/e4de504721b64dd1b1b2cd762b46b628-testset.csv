,question,contexts,ground_truth,evolution_type,metadata,episode_done
0,What aspects of training strategies are discussed in the context of in-context learning (ICL)?,"['ment aims to improve the scalability and efficiency\nof ICL. As LMs continue to scale up, exploring\nways to effectively and efficiently utilize a larger\nnumber of demonstrations in ICL remains an ongo-\ning area of research.\n12 Conclusion\nIn this paper, we survey the existing ICL literature\nand provide an extensive review of advanced ICL\ntechniques, including training strategies, demon-\nstration designing strategies, evaluation datasets\nand resources, as well as related analytical studies.\nFurthermore, we highlight critical challenges and\npotential directions for future research. To the best\nof our knowledge, this is the first survey about ICL.\nWe hope this survey can highlight the current re-\nsearch status of ICL and shed light on future work\non this promising paradigm.\nReferences\nEkin Akyürek, Dale Schuurmans, Jacob An-\ndreas, Tengyu Ma, and Denny Zhou. 2022.\nWhat learning algorithm is in-context learn-\ning? investigations with linear models. CoRR ,\nabs/2211.15661.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,\nAntoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Mal-\ncolm Reynolds, et al. 2022. Flamingo: a vi-\nsual language model for few-shot learning. Ad-\nvances in Neural Information Processing Sys-\ntems, 35:23716–23736.\nShengnan An, Zeqi Lin, Qiang Fu, Bei Chen,\nNanning Zheng, Jian-Guang Lou, and Dong-\nmei Zhang. 2023. How do in-context exam-\nples affect compositional generalization? CoRR ,\nabs/2305.04835.\nAmir Bar, Yossi Gandelsman, Trevor Darrell,\nAmir Globerson, and Alexei Efros. 2022. Vi-\nsual prompting via image inpainting. Ad-\nvances in Neural Information Processing Sys-\ntems, 35:25005–25017.\nRichard Bellman. 1957. A markovian decision\nprocess. Journal of mathematics and mechanics ,\npages 679–684.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,Michael S. Bernstein, Jeannette Bohg, Antoine\nBosselut, Emma Brunskill, Erik Brynjolfsson,\nS. Buch, Dallas Card, Rodrigo Castellon, Ni-\nladri S. Chatterji, Annie S. Chen, Kathleen A.\nCreel, Jared Davis, Dora Demszky, Chris Don-\nahue, Moussa Doumbouya, Esin Durmus, Ste-\nfano Ermon, John Etchemendy, Kawin Etha-\nyarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale,\nLauren E. Gillespie, Karan Goel, Noah D.\nGoodman, Shelby Grossman, Neel Guha, Tat-\nsunori Hashimoto, Peter Henderson, John He-\nwitt, Daniel E. Ho, Jenny Hong, Kyle Hsu,\nJing Huang, Thomas F. Icard, Saahil Jain, Dan\nJurafsky, Pratyusha Kalluri, Siddharth Karam-\ncheti, Geoff Keeling, Fereshte Khani, O. Khat-\ntab, Pang Wei Koh, Mark S. Krass, Ranjay Kr-\nishna, Rohith Kuditipudi, Ananya Kumar, Faisal\nLadhak, Mina Lee, Tony Lee, Jure Leskovec,\nIsabelle Levent, Xiang Lisa Li, Xuechen Li,\nTengyu Ma, Ali Malik, Christopher D. Man-\nning, Suvir P. Mirchandani, Eric Mitchell,\nZanele Munyikwa, Suraj Nair, Avanika Narayan,\nDeepak Narayanan, Benjamin Newman, Allen\nNie, Juan Carlos Niebles, Hamed Nilforoshan,\nJ. F. Nyarko, Giray Ogut, Laurel Orr, Isabel\nPapadimitriou, Joon Sung Park, Chris Piech,\nEva Portelance, Christopher Potts, Aditi Raghu-\nnathan, Robert Reich, Hongyu Ren, Frieda\nRong, Yusuf H. Roohani, Camilo Ruiz, Jack\nRyan, Christopher R’e, Dorsa Sadigh, Shiori\n']","The context discusses aspects of training strategies in the context of in-context learning (ICL), including exploring ways to effectively and efficiently utilize a larger number of demonstrations.",simple,"[{'page_label': '13', 'file_name': '2301.00234v3.A_Survey_on_In_context_Learning.pdf', 'file_path': '/home/peter-legion-wsl2/peter-projects/regen-ai/nbs/data/prompt-engineering-papers/2301.00234v3.A_Survey_on_In_context_Learning.pdf', 'file_type': 'application/pdf', 'file_size': 4898135, 'creation_date': '2024-04-13', 'last_modified_date': '2024-04-13'}]",True
1,How did Li et al. (2023d) enhance long-range language modeling capabilities in their proposed EVaLM model?,"[' (Zhou et al., 2023), and itera-\ntive forward tuning (Yang et al., 2023). Addition-\nally, Li et al. (2023d) proposed EVaLM with longer\ncontext length and enhanced long-range language\nmodeling capabilities. This model-level improve-']",Li et al. (2023d) enhanced long-range language modeling capabilities in their proposed EVaLM model by increasing the context length and improving the model-level features.,simple,"[{'page_label': '12', 'file_name': '2301.00234v3.A_Survey_on_In_context_Learning.pdf', 'file_path': '/home/peter-legion-wsl2/peter-projects/regen-ai/nbs/data/prompt-engineering-papers/2301.00234v3.A_Survey_on_In_context_Learning.pdf', 'file_type': 'application/pdf', 'file_size': 4898135, 'creation_date': '2024-04-13', 'last_modified_date': '2024-04-13'}]",True
