{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "# from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "from dspy.retrieve.pgvector_rm import PgVectorRM\n",
    "from dspy.retrieve.chromadb_rm import ChromadbRM\n",
    "# import weaviate\n",
    "# import weaviate.classes as wvc\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions \n",
    "import fnmatch\n",
    "import uuid\n",
    "import torch\n",
    "from langchain_text_splitters import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn_string = f\"postgresql://daybreak:daybreak@localhost:5432/daybreak\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = psycopg2.connect(conn_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add pgvector extension and create test table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with conn.cursor() as cur:\n",
    "#     cur.execute('CREATE EXTENSION IF NOT EXISTS vector')\n",
    "#     register_vector(conn)\n",
    "#     cur.execute(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS text(\n",
    "#         id SERIAL PRIMARY KEY,\n",
    "#         text TEXT\n",
    "#         );\n",
    "#     \"\"\")\n",
    "# conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_md_files(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for basename in files:\n",
    "            if fnmatch.fnmatch(basename, '*.md'):\n",
    "                filename = os.path.join(root, basename)\n",
    "                yield filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in find_md_files(\"./repos\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = [filename for filename in find_md_files(\"./repos\")] \n",
    "doc_contents = []\n",
    "for doc in find_md_files(\"./repos\"):\n",
    "    with open(doc, 'r') as f:\n",
    "        doc_contents.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = \"\".join(doc_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "691"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_splitter.split_text(all_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = text_splitter.split_text(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up local llm and retriever client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = dspy.OllamaLocal(\"zephyr-7b-beta\", max_tokens=3000, stop=['\\n\\n'], model_type=\"chat\")\n",
    "chroma_client = client = chromadb.PersistentClient(path=\"./chroma\")\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "collection = chroma_client.get_or_create_collection(name=\"docs\", embedding_function=default_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(name=docs)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# DSPy Documentation\\n\\nThis website is built using [Docusaurus](https://docusaurus.io/), a modern static website generator.\\n\\n## Contributing to the `docs` Folder\\n\\nThis guide is for contributors looking to make changes to the documentation in the `dspy/docs` folder. \\n\\n1. **Pull the up-to-date version of the website**: Please pull the latest version of the live documentation site via its subt',\n",
       " ' latest version of the live documentation site via its subtree repository with the following command:\\n\\n```bash\\n#Ensure you are in the top-level dspy/ folder\\ngit subtree pull --prefix=docs https://github.com/krypticmouse/dspy-docs master\\n```\\n\\n2. **Push your new changes on a new branch**: Feel free to add or edit existing documentation and open a PR for your changes. Once your',\n",
       " ' and open a PR for your changes. Once your PR is reviewed and approved, the changes will be ready to merge into main. \\n\\n3. **Updating the website**: Once your changes are merged to main, they need to be pushed to the subtree repository that hosts the live documentation site. This step will eventually be done automatically, but for now, please run the following command to push the updated `docs` content to the website subtree repository:\\n\\n```bash',\n",
       " ' website subtree repository:\\n\\n```bash\\n#Ensure you are in the top-level dspy/ folder\\ngit subtree push --prefix=docs https://github.com/krypticmouse/dspy-docs master\\n```\\n---\\nsidebar_position: 7\\n---\\n\\n# DSPy Assertions\\n\\nLanguage models (LMs) have transformed how we interact with machine learning, offering vast capabilities in natural language understanding and',\n",
       " \" learning, offering vast capabilities in natural language understanding and generation. However, ensuring these models adhere to domain-specific constraints remains a challenge. Despite the growth of techniques like fine-tuning or “prompt engineering”, these approaches are extremely tedious and rely on heavy, manual hand-waving to guide the LMs in adhering to specific constraints. Even DSPy's modularity of programming prompting pipelines lacks mechanisms to effectively and automatically enforce these constraints. \\n\\nTo address\",\n",
       " ' automatically enforce these constraints. \\n\\nTo address this, we introduce DSPy Assertions, a feature within the DSPy framework designed to automate the enforcement of computational constraints on LMs. DSPy Assertions empower developers to guide LMs towards desired outcomes with minimal manual intervention, enhancing the reliability, predictability, and correctness of LM outputs.\\n\\n## dspy.Assert and dspy.Suggest API\\n\\nWe introduce two primary constructs within DSP',\n",
       " '\\n\\nWe introduce two primary constructs within DSPy Assertions:\\n\\n- **`dspy.Assert`**:\\n  - **Parameters**: \\n    - `constraint (bool)`: Outcome of Python-defined boolean validation check.\\n    - `msg (Optional[str])`: User-defined error message providing feedback or correction guidance.\\n    - `backtrack (Optional[module])`',\n",
       " \" - `backtrack (Optional[module])`: Specifies target module for retry attempts upon constraint failure. The default backtracking module is the last module before the assertion.\\n  - **Behavior**: Initiates retry  upon failure, dynamically adjusting the pipeline's execution. If failures persist, it halts execution and raises a `dspy.AssertionError`.\\n\\n- **`dspy.Suggest`**:\\n  - **Parameters**:\",\n",
       " '`**:\\n  - **Parameters**: Similar to `dspy.Assert`.\\n  - **Behavior**: Encourages self-refinement through retries without enforcing hard stops. Logs failures after maximum backtracking attempts and continues execution.\\n\\n- **dspy.Assert vs. Python Assertions**: Unlike conventional Python `assert` statements that terminate the program upon failure, `dspy.Assert` conducts a sophisticated retry',\n",
       " 'y.Assert` conducts a sophisticated retry mechanism, allowing the pipeline to adjust. \\n\\nSpecifically, when a constraint is not met:\\n\\n- Backtracking Mechanism: An under-the-hood backtracking is initiated, offering the model a chance to self-refine and proceed, which is done through\\n- Dynamic Signature Modification: internally modifying your DSPy program’s Signature by adding the following fields:\\n    - Past Output: your model',\n",
       " \"\\n    - Past Output: your model's past output that did not pass the validation_fn\\n    - Instruction: your user-defined feedback message on what went wrong and what possibly to fix\\n\\nIf the error continues past the `max_backtracking_attempts`, then `dspy.Assert` will halt the pipeline execution, altering you with an `dspy.AssertionError`. This ensures your program doesn't continue executing with �\",\n",
       " \" This ensures your program doesn't continue executing with “bad” LM behavior and immediately highlights sample failure outputs for user assessment. \\n\\n- **dspy.Suggest vs. dspy.Assert**: `dspy.Suggest` on the other hand offers a softer approach. It maintains the same retry backtracking as `dspy.Assert` but instead serves as a gentle nudger. If the model outputs cannot pass the model constraints after the `\",\n",
       " ' model outputs cannot pass the model constraints after the `max_backtracking_attempts`, `dspy.Suggest` will log the persistent failure and continue execution of the program on the rest of the data. This ensures the LM pipeline works in a \"best-effort\" manner without halting execution. \\n\\n- **`dspy.Suggest`** are best utilized as \"helpers\" during the evaluation phase, offering guidance and potential corrections without halting the pipeline.\\n',\n",
       " ' guidance and potential corrections without halting the pipeline.\\n- **`dspy.Assert`** are recommended during the development stage as \"checkers\" to ensure the LM behaves as expected, providing a robust mechanism for identifying and addressing errors early in the development cycle.\\n\\n\\n## Use Case: Including Assertions in DSPy Programs\\n\\nWe start with using an example of a multi-hop QA SimplifiedBaleen pipeline as defined in the intro walkthrough. ',\n",
       " ' pipeline as defined in the intro walkthrough. \\n\\n```python\\nclass SimplifiedBaleen(dspy.Module):\\n    def __init__(self, passages_per_hop=2, max_hops=2):\\n        super().__init__()\\n\\n        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _',\n",
       " 'Thought(GenerateSearchQuery) for _ in range(max_hops)]\\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\\n        self.max_hops = max_hops\\n\\n    def forward(self,',\n",
       " 'hops\\n\\n    def forward(self, question):\\n        context = []\\n        prev_queries = [question]\\n\\n        for hop in range(self.max_hops):\\n            query = self.generate_query[hop](context=context, question=question).query\\n         ',\n",
       " '\\n            prev_queries.append(query)\\n            passages = self.retrieve(query).passages\\n            context = deduplicate(context + passages)\\n        \\n        pred = self.generate_answer(context=context, question=question',\n",
       " '_answer(context=context, question=question)\\n        pred = dspy.Prediction(context=context, answer=pred.answer)\\n        return pred\\n\\nbaleen = SimplifiedBaleen()\\n\\nbaleen(question = \"Which award did Gary Zukav\\'s first book receive?\")\\n```\\n\\nTo include DSPy Assertions, we simply define our',\n",
       " 'SPy Assertions, we simply define our validation functions and declare our assertions following the respective model generation. \\n\\nFor this use case, suppose we want to impose the following constraints:\\n    1. Length - each query should be less than 100 characters\\n    2. Uniqueness - each generated query should differ from previously-generated queries. \\n    \\nWe can define these validation checks as boolean functions:\\n\\n```python\\n#sim',\n",
       " ' functions:\\n\\n```python\\n#simplistic boolean check for query length\\nlen(query) <= 100\\n\\n#Python function for validating distinct queries\\ndef validate_query_distinction_local(previous_queries, query):\\n    \"\"\"check if query is distinct from previous queries\"\"\"\\n    if previous_queries == []:\\n        return True\\n    if dspy.evaluate.',\n",
       " '    if dspy.evaluate.answer_exact_match_str(query, previous_queries, frac=0.8):\\n        return False\\n    return True\\n```\\n\\nWe can declare these validation checks through `dspy.Suggest` statements (as we want to test the program in a best-effort demonstration). We want to keep these after the query generation `query = self.gener',\n",
       " ' after the query generation `query = self.generate_query[hop](context=context, question=question).query`.\\n\\n```python\\ndspy.Suggest(\\n    len(query) <= 100,\\n    \"Query should be short and less than 100 characters\",\\n)\\n\\ndspy.Suggest(\\n    validate_query_distinction_local(prev_queries, query),\\n    \"Query',\n",
       " 'eries, query),\\n    \"Query should be distinct from: \"\\n    + \"; \".join(f\"{i+1}) {q}\" for i, q in enumerate(prev_queries)),\\n)\\n```\\n\\nIt is recommended to define a program with assertions separately than your original program if you are doing comparative evaluation for the effect of assertions. If not, feel free to set Assertions away!\\n\\nLet\\'s take a',\n",
       " \"ertions away!\\n\\nLet's take a look at how the SimplifiedBaleen program will look with Assertions included:\\n\\n```python\\nclass SimplifiedBaleenAssertions(dspy.Module):\\n    def __init__(self, passages_per_hop=2, max_hops=2):\\n        super().__init__()\\n        self.gener\",\n",
       " '        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\\n    ',\n",
       " '(GenerateAnswer)\\n        self.max_hops = max_hops\\n\\n    def forward(self, question):\\n        context = []\\n        prev_queries = [question]\\n\\n        for hop in range(self.max_hops):\\n            query = self.generate_query[',\n",
       " '  query = self.generate_query[hop](context=context, question=question).query\\n\\n            dspy.Suggest(\\n                len(query) <= 100,\\n                \"Query should be short and less than 100 characters\",\\n          ',\n",
       " '            )\\n\\n            dspy.Suggest(\\n                validate_query_distinction_local(prev_queries, query),\\n                \"Query should be distinct from: \"\\n             ',\n",
       " '             + \"; \".join(f\"{i+1}) {q}\" for i, q in enumerate(prev_queries)),\\n            )\\n\\n            prev_queries.append(query)\\n            passages = self.retrieve(query).passages\\n ',\n",
       " '.retrieve(query).passages\\n            context = deduplicate(context + passages)\\n        \\n        if all_queries_distinct(prev_queries):\\n            self.passed_suggestions += 1\\n\\n        pred = self.generate_answer(',\n",
       " '  pred = self.generate_answer(context=context, question=question)\\n        pred = dspy.Prediction(context=context, answer=pred.answer)\\n        return pred\\n```\\n\\nNow calling programs with DSPy Assertions requires one last step, and that is transforming the program to wrap it with internal assertions backtracking and Retry logic. \\n\\n```python',\n",
       " ' Retry logic. \\n\\n```python\\nfrom dspy.primitives.assertions import assert_transform_module, backtrack_handler\\n\\nbaleen_with_assertions = assert_transform_module(SimplifiedBaleenAssertions(), backtrack_handler)\\n\\n# backtrack_handler is parameterized over a few settings for the backtracking mechanism\\n# To change the number of max retry attempts, you can do\\nb',\n",
       " ' max retry attempts, you can do\\nbaleen_with_assertions_retry_once = assert_transform_module(SimplifiedBaleenAssertions(), \\n    functools.partial(backtrack_handler, max_backtracks=1))\\n```\\n\\nAlternatively, you can also directly call `activate_assertions` on the program with `dspy.Assert/Suggest` statements using the default backtracking',\n",
       " \"ert/Suggest` statements using the default backtracking mechanism (`max_backtracks=2`):\\n\\n```python\\nbaleen_with_assertions = SimplifiedBaleenAssertions().activate_assertions()\\n```\\n\\nNow let's take a look at the internal LM backtracking by inspecting the history of the LM query generations. Here we see that when a query fails to pass the validation check of being less than 100 characters, its internal `\",\n",
       " ' of being less than 100 characters, its internal `GenerateSearchQuery` signature is dynamically modified during the backtracking+Retry process to include the past query and the corresponding user-defined instruction: `\"Query should be short and less than 100 characters\"`.\\n\\n\\n```\\nWrite a simple search query that will help answer a complex question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: may contain relevant facts\\n\\nQuestion: ${question}\\n\\nReasoning',\n",
       " \"\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the query}. We ...\\n\\nQuery: ${query}\\n\\n---\\n\\nContext:\\n[1] «Kerry Condon | Kerry Condon (born 4 January 1983) is [...]»\\n[2] «Corona Riccardo | Corona Riccardo (c. 1878October 15, 1917) was [...]»\\n\\nQuestion: Who acted in the\",\n",
       " ' [...]»\\n\\nQuestion: Who acted in the shot film The Shore and is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of \"Hamlet.\" ?\\n\\nReasoning: Let\\'s think step by step in order to find the answer to this question. First, we need to identify the actress who played Ophelia in a Royal Shakespeare Company production of \"Hamlet.\" Then, we need to find out if this actress also acted in the short film \"',\n",
       " ' if this actress also acted in the short film \"The Shore.\"\\n\\nQuery: \"actress who played Ophelia in Royal Shakespeare Company production of Hamlet\" + \"actress in short film The Shore\"\\n\\n\\n\\nWrite a simple search query that will help answer a complex question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: may contain relevant facts\\n\\nQuestion: ${question}\\n\\nPast Query: past output with errors\\n\\nInstructions: Some',\n",
       " ' past output with errors\\n\\nInstructions: Some instructions you must satisfy\\n\\nQuery: ${query}\\n\\n---\\n\\nContext:\\n[1] «Kerry Condon | Kerry Condon (born 4 January 1983) is an Irish television and film actress, best known for her role as Octavia of the Julii in the HBO/BBC series \"Rome,\" as Stacey Ehrmantraut in AMC\\'s \"Better Call Saul\" and as the voice of F',\n",
       " 'Better Call Saul\" and as the voice of F.R.I.D.A.Y. in various films in the Marvel Cinematic Universe. She is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of \"Hamlet.\"»\\n[2] «Corona Riccardo | Corona Riccardo (c. 1878October 15, 1917) was an Italian born American actress who had a brief Broadway stage career before leaving to become a wife and',\n",
       " ' Broadway stage career before leaving to become a wife and mother. Born in Naples she came to acting in 1894 playing a Mexican girl in a play at the Empire Theatre. Wilson Barrett engaged her for a role in his play \"The Sign of the Cross\" which he took on tour of the United States. Riccardo played the role of Ancaria and later played Berenice in the same play. Robert B. Mantell in 1898 who struck by her beauty also cast her in two Shakespeare',\n",
       " ' struck by her beauty also cast her in two Shakespeare plays, \"Romeo and Juliet\" and \"Othello\". Author Lewis Strang writing in 1899 said Riccardo was the most promising actress in America at the time. Towards the end of 1898 Mantell chose her for another Shakespeare part, Ophelia im Hamlet. Afterwards she was due to join Augustin Daly\\'s Theatre Company but Daly died in 1899. In 1899 she gained her biggest fame by playing Iras in the first',\n",
       " ' her biggest fame by playing Iras in the first stage production of Ben-Hur.»\\n\\nQuestion: Who acted in the shot film The Shore and is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of \"Hamlet.\" ?\\n\\nPast Query: \"actress who played Ophelia in Royal Shakespeare Company production of Hamlet\" + \"actress in short film The Shore\"\\n\\nInstructions: Query should be short and less than 100 characters',\n",
       " ': Query should be short and less than 100 characters\\n\\nQuery: \"actress Ophelia RSC Hamlet\" + \"actress The Shore\"\\n\\n```\\n\\n\\n## Assertion-Driven Optimizations\\n\\nDSPy Assertions work with optimizations that DSPy offers, particularly with `BootstrapFewShotWithRandomSearch`, including the following settings:\\n\\n- Compilation with Assertions\\n    This includes assertion-driven example boot',\n",
       " '    This includes assertion-driven example bootstrapping and counterexample bootstrapping during compilation. The teacher model for bootstrapping few-shot demonstrations can make use of DSPy Assertions to offer robust bootstrapped examples for the student model to learn from during inference. In this setting, the student model does not perform assertion aware optimizations (backtracking and retry) during inference.\\n- Compilation + Inference with Assertions\\n    -',\n",
       " 'ference with Assertions\\n    -This includes assertion-driven optimizations in both compilation and inference. Now the teacher model offers assertion-driven examples but the student can further optimize with assertions of its own during inference time. \\n```python\\nteleprompter = BootstrapFewShotWithRandomSearch(\\n    metric=validate_context_and_answer_and_hops,\\n    max_bootstrapped_demos=max_bootstra',\n",
       " 'strapped_demos=max_bootstrapped_demos,\\n    num_candidate_programs=6,\\n)\\n\\n#Compilation with Assertions\\ncompiled_with_assertions_baleen = teleprompter.compile(student = baleen, teacher = baleen_with_assertions, trainset = trainset, valset = devset)\\n\\n#Compilation + Inference with Ass',\n",
       " '\\n\\n#Compilation + Inference with Assertions\\ncompiled_baleen_with_assertions = teleprompter.compile(student=baleen_with_assertions, teacher = baleen_with_assertions, trainset=trainset, valset=devset)\\n\\n```---\\nsidebar_position: 1\\n---\\n\\n# API References\\n\\nWelcome to the API References for DSPy! This',\n",
       " \" to the API References for DSPy! This is where you'll find easy-to-understand information about all the parts of DSPy that you can use in your projects. We've got guides on different tools and helpers that DSPy has, like modules and optimizers. Everything is sorted so you can quickly find what you need. If you're making something and need to quickly get started with DSPy to do certain tasks, this place will show you how to set it\",\n",
       " ', this place will show you how to set it up and get it working just right.---\\nsidebar_position: 9\\n---\\n\\n# dsp.Mistral\\n\\n### Usage\\n\\n```python\\nlm = dsp.Mistral(model=\\'mistral-medium-latest\\', api_key=\"your-mistralai-api-key\")\\n```\\n\\n### Constructor\\n\\nThe constructor initializes the base class `LM` and ver',\n",
       " ' initializes the base class `LM` and verifies the `api_key` provided or defined through the `MISTRAL_API_KEY` environment variable.\\n\\n```python\\nclass Mistral(LM):\\n    def __init__(\\n        self,\\n        model: str = \"mistral-medium-latest\",\\n        api_key: Optional[',\n",
       " '     api_key: Optional[str] = None,\\n        **kwargs,\\n    ):\\n```\\n\\n**Parameters:**\\n- `model` (_str_): Mistral AI pretrained models. Defaults to `mistral-medium-latest`.\\n- `api_key` (_Optional[str]_, _optional_): API provider from Mistral AI. Defaults to None.\\n-',\n",
       " 'ral AI. Defaults to None.\\n- `**kwargs`: Additional language model arguments to pass to the API provider.\\n\\n### Methods\\n\\nRefer to [`dspy.Mistral`](#) documentation.\\n---\\nsidebar_position: 8\\n---\\n\\n# dspy.Databricks\\n\\n### Usage\\n```python\\nlm = dspy.Databricks(model=\"databricks-mpt-30',\n",
       " 'model=\"databricks-mpt-30b-instruct\")\\n```\\n\\n### Constructor\\n\\nThe constructor inherits from the `GPT3` class and verifies the Databricks authentication credentials for using Databricks Model Serving API through the OpenAI SDK.\\nWe expect the following environment variables to be set:\\n- `openai.api_key`: Databricks API key.\\n- `openai.base_url`: Dat',\n",
       " ' `openai.base_url`: Databricks Model Endpoint url\\n\\nThe `kwargs` attribute is initialized with default values for relevant text generation parameters needed for communicating with the Databricks OpenAI SDK, such as `temperature`, `max_tokens`, `top_p`, and `n`. However, it removes the `frequency_penalty` and `presence_penalty` arguments as these are not currently supported by the Databricks API',\n",
       " ' are not currently supported by the Databricks API.\\n\\n```python\\nclass Databricks(GPT3):\\n    def __init__(\\n        self,\\n        model: str,\\n        api_key: Optional[str] = None,\\n        api_base: Optional[str] = None,\\n    ',\n",
       " 'str] = None,\\n        model_type: Literal[\"chat\", \"text\"] = None,\\n        **kwargs,\\n    ):\\n```\\n\\n**Parameters:**\\n- `model` (_str_): models hosted on Databricks.\\n- `stop` (_List[str]_, _optional_): List of stopping tokens to end generation.\\n- `api_key',\n",
       " ' to end generation.\\n- `api_key` (_Optional[str]_): Databricks API key. Defaults to None\\n- `api_base` (_Optional[str]_): Databricks Model Endpoint url Defaults to None.\\n- `model_type` (_Literal[\"chat\", \"text\", \"embeddings\"]_): Specified model type to use.\\n- `**kwargs`: Additional language model arguments to pass to',\n",
       " 'args`: Additional language model arguments to pass to the API provider.\\n\\n### Methods\\n\\nRefer to [`dspy.OpenAI`](https://dspy-docs.vercel.app/api/language_model_clients/OpenAI) documentation.---\\nsidebar_position: 4\\n---\\n\\n# dspy.HFClientTGI\\n\\n### Usage\\n\\n```python\\nlm = dspy.HFClientT',\n",
       " 'lm = dspy.HFClientTGI(model=\"meta-llama/Llama-2-7b-hf\", port=8080, url=\"http://localhost\")\\n```\\n\\n### Prerequisites\\n\\nRefer to the [Text Generation-Inference Server](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/local_models/HFClientTGI) section',\n",
       " 'local_models/HFClientTGI) section of the `Using Local Models` documentation.\\n\\n### Constructor\\n\\nThe constructor initializes the `HFModel` base class and configures the client for communicating with the TGI server. It requires a `model` instance, communication `port` for the server, and the `url` for the server to host generate requests. Additional configuration can be provided via keyword arguments in `**kwargs`.\\n\\n```python\\nclass',\n",
       " 'kwargs`.\\n\\n```python\\nclass HFClientTGI(HFModel):\\n    def __init__(self, model, port, url=\"http://future-hgx-1\", **kwargs):\\n```\\n\\n**Parameters:**\\n- `model` (_HFModel_): Instance of Hugging Face model connected to the TGI server.\\n- `port` (_int_): Port for TGI server.\\n- `',\n",
       " '): Port for TGI server.\\n- `url` (_str_): Base URL where the TGI server is hosted. \\n- `**kwargs`: Additional keyword arguments to configure the client.\\n\\n### Methods\\n\\nRefer to [`dspy.OpenAI`](https://dspy-docs.vercel.app/api/language_model_clients/OpenAI) documentation.---\\nsidebar_position: 3\\n---\\n\\n',\n",
       " \"sidebar_position: 3\\n---\\n\\n# dsp.Cohere\\n\\n### Usage\\n\\n```python\\nlm = dsp.Cohere(model='command-nightly')\\n```\\n\\n### Constructor\\n\\nThe constructor initializes the base class `LM` and verifies the `api_key` to set up Cohere request retrieval.\\n\\n```python\\nclass Cohere(LM):\\n    def __init__(\",\n",
       " '):\\n    def __init__(\\n        self,\\n        model: str = \"command-nightly\",\\n        api_key: Optional[str] = None,\\n        stop_sequences: List[str] = [],\\n    ):\\n```\\n\\n**Parameters:**\\n- `model` (_str_):',\n",
       " '**\\n- `model` (_str_): Cohere pretrained models. Defaults to `command-nightly`.\\n- `api_key` (_Optional[str]_, _optional_): API provider from Cohere. Defaults to None.\\n- `stop_sequences` (_List[str]_, _optional_): List of stopping tokens to end generation.\\n\\n### Methods\\n\\nRefer to [`dspy.OpenAI`](https',\n",
       " \"`dspy.OpenAI`](https://dspy-docs.vercel.app/api/language_model_clients/OpenAI) documentation.\\n---\\nsidebar_position: 2\\n---\\n\\n# dspy.AzureOpenAI\\n\\n### Usage\\n\\n```python\\nlm = dspy.AzureOpenAI(api_base='...', api_version='2023-12-01-preview',\",\n",
       " \"2023-12-01-preview', model='gpt-3.5-turbo')\\n```\\n\\n### Constructor\\n\\nThe constructor initializes the base class `LM` and verifies the provided arguments like the `api_provider`, `api_key`, and `api_base` to set up OpenAI request retrieval through Azure. The `kwargs` attribute is initialized with default values for relevant text generation parameters needed for communicating with the GPT\",\n",
       " ' text generation parameters needed for communicating with the GPT API, such as `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, and `n`.\\n\\n```python\\nclass AzureOpenAI(LM):\\n    def __init__(\\n        self,\\n        api_base: str,\\n      ',\n",
       " ': str,\\n        api_version: str,\\n        model: str = \"gpt-3.5-turbo-instruct\",\\n        api_key: Optional[str] = None,\\n        model_type: Literal[\"chat\", \"text\"] = None,\\n        **kwargs,\\n  ',\n",
       " '    **kwargs,\\n    ):\\n```\\n\\n\\n\\n**Parameters:** \\n- `api_base` (str): Azure Base URL.\\n- `api_version` (str): Version identifier for Azure OpenAI API.\\n- `api_key` (_Optional[str]_, _optional_): API provider authentication token. Retrieves from `AZURE_OPENAI_KEY` environment variable if None.\\n- `',\n",
       " 'KEY` environment variable if None.\\n- `model_type` (_Literal[\"chat\", \"text\"]_): Specified model type to use, defaults to \\'chat\\'.\\n- `**kwargs`: Additional language model arguments to pass to the API provider.\\n\\n### Methods\\n\\n#### `__call__(self, prompt: str, only_completed: bool = True, return_sorted: bool = False, **kwargs) -> List[',\n",
       " ' = False, **kwargs) -> List[Dict[str, Any]]`\\n\\nRetrieves completions from Azure OpenAI Endpoints by calling `request`. \\n\\nInternally, the method handles the specifics of preparing the request prompt and corresponding payload to obtain the response.\\n\\nAfter generation, the completions are post-processed based on the `model_type` parameter. If the parameter is set to \\'chat\\', the generated content look like `choice[\"',\n",
       " 'chat\\', the generated content look like `choice[\"message\"][\"content\"]`. Otherwise, the generated text will be `choice[\"text\"]`.\\n\\n**Parameters:**\\n- `prompt` (_str_): Prompt to send to Azure OpenAI.\\n- `only_completed` (_bool_, _optional_): Flag to return only completed responses and ignore completion due to length. Defaults to True.\\n- `return_sorted` (_bool_,',\n",
       " ' `return_sorted` (_bool_, _optional_): Flag to sort the completion choices using the returned averaged log-probabilities. Defaults to False.\\n- `**kwargs`: Additional keyword arguments for completion request.\\n\\n**Returns:**\\n- `List[Dict[str, Any]]`: List of completion choices.---\\nsidebar_position: 7\\n---\\n\\n# dspy.Together\\n\\n### Usage\\n\\n``',\n",
       " 'y.Together\\n\\n### Usage\\n\\n```python\\nlm = dspy.Together(model=\"mistralai/Mistral-7B-v0.1\")\\n```\\n\\n### Constructor\\n\\nThe constructor initializes the base class `LM` and verifies the `api_key` for using Together API.\\nWe expect the following environment variables to be set:\\n- `TOGETHER_API_KEY`: API key for',\n",
       " 'HER_API_KEY`: API key for Together.\\n- `TOGETHER_API_BASE`: API base URL for Together.\\n\\n\\n```python\\nclass Together(HFModel):\\n    def __init__(self, model, **kwargs):\\n```\\n\\n**Parameters:**\\n- `model` (_str_): models hosted on Together.\\n- `stop` (_List[str]_, _optional_):',\n",
       " 'List[str]_, _optional_): List of stopping tokens to end generation.\\n\\n### Methods\\n\\nRefer to [`dspy.OpenAI`](https://dspy-docs.vercel.app/api/language_model_clients/OpenAI) documentation.---\\nsidebar_position: 6\\n---\\n\\n# dspy.Anyscale\\n\\n### Usage\\n\\n```python\\nlm = dspy.',\n",
       " '`python\\nlm = dspy.Anyscale(model=\"mistralai/Mistral-7B-Instruct-v0.1\")\\n```\\n\\n### Constructor\\n\\nThe constructor initializes the base class `LM` and verifies the `api_key` for using Anyscale API.\\nWe expect the following environment variables to be set:\\n- `ANYSCALE_API_KEY`: API key for Together.\\n- `',\n",
       " '`: API key for Together.\\n- `ANYSCALE_API_BASE`: API base URL for Together.\\n\\n\\n```python\\nclass Anyscale(HFModel):\\n    def __init__(self, model, **kwargs):\\n```\\n\\n**Parameters:**\\n- `model` (_str_): models hosted on Together.\\n\\n### Methods\\n\\nRefer to [`dspy.OpenAI`](https://',\n",
       " \"dspy.OpenAI`](https://dspy-docs.vercel.app/api/language_model_clients/OpenAI) documentation.\\n---\\nsidebar_position: 1\\n---\\n\\n# dspy.OpenAI\\n\\n### Usage\\n\\n```python\\nlm = dspy.OpenAI(model='gpt-3.5-turbo')\\n```\\n\\n### Constructor\\n\\nThe constructor initial\",\n",
       " '\\n\\n### Constructor\\n\\nThe constructor initializes the base class `LM` and verifies the provided arguments like the `api_provider`, `api_key`, and `api_base` to set up OpenAI request retrieval. The `kwargs` attribute is initialized with default values for relevant text generation parameters needed for communicating with the GPT API, such as `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `',\n",
       " '_p`, `frequency_penalty`, `presence_penalty`, and `n`.\\n\\n```python\\nclass OpenAI(LM):\\n    def __init__(\\n        self,\\n        model: str = \"text-davinci-002\",\\n        api_key: Optional[str] = None,\\n        api',\n",
       " ',\\n        api_provider: Literal[\"openai\"] = \"openai\",\\n        model_type: Literal[\"chat\", \"text\"] = None,\\n        **kwargs,\\n    ):\\n```\\n\\n\\n\\n**Parameters:** \\n- `api_key` (_Optional[str]_, _optional_): API provider authentication token. Def',\n",
       " ' _optional_): API provider authentication token. Defaults to None.\\n- `api_provider` (_Literal[\"openai\"]_, _optional_): API provider to use. Defaults to \"openai\".\\n- `model_type` (_Literal[\"chat\", \"text\"]_): Specified model type to use.\\n- `**kwargs`: Additional language model arguments to pass to the API provider.\\n\\n### Methods\\n\\n####',\n",
       " ' API provider.\\n\\n### Methods\\n\\n#### `__call__(self, prompt: str, only_completed: bool = True, return_sorted: bool = False, **kwargs) -> List[Dict[str, Any]]`\\n\\nRetrieves completions from OpenAI by calling `request`. \\n\\nInternally, the method handles the specifics of preparing the request prompt and corresponding payload to obtain the response.\\n\\nAfter generation, the comple',\n",
       " ' the response.\\n\\nAfter generation, the completions are post-processed based on the `model_type` parameter. If the parameter is set to \\'chat\\', the generated content look like `choice[\"message\"][\"content\"]`. Otherwise, the generated text will be `choice[\"text\"]`.\\n\\n**Parameters:**\\n- `prompt` (_str_): Prompt to send to OpenAI.\\n- `only_completed` (_bool_, _optional',\n",
       " '_completed` (_bool_, _optional_): Flag to return only completed responses and ignore completion due to length. Defaults to True.\\n- `return_sorted` (_bool_, _optional_): Flag to sort the completion choices using the returned averaged log-probabilities. Defaults to False.\\n- `**kwargs`: Additional keyword arguments for completion request.\\n\\n**Returns:**\\n- `List[Dict[str, Any]]',\n",
       " ' `List[Dict[str, Any]]`: List of completion choices.---\\nsidebar_position: 5\\n---\\n\\n# dspy.HFClientVLLM\\n\\n### Usage\\n\\n```python\\nlm = dspy.HFClientVLLM(model=\"meta-llama/Llama-2-7b-hf\", port=8080, url=\"http://localhost\")\\n```\\n\\n### Prerequisites',\n",
       " 'localhost\")\\n```\\n\\n### Prerequisites\\n\\nRefer to the [vLLM Server](https://dspy-docs.vercel.app/api/language_model_clients/HFClientVLLM) section of the `Using Local Models` documentation.\\n\\n### Constructor\\n\\nRefer to [`dspy.TGI`](https://dspy-docs.vercel.app/api/language_model_clients/',\n",
       " '/api/language_model_clients/TGI) documentation. Replace with `HFClientVLLM`.\\n\\n### Methods\\n\\nRefer to [`dspy.OpenAI`](https://dspy-docs.vercel.app/api/language_model_clients/OpenAI) documentation.---\\nsidebar_position: 4\\n---\\n\\n# retrieve.FaissRM\\n\\n### Constructor\\n\\nInitialize an instance of Fa',\n",
       " ' Constructor\\n\\nInitialize an instance of FaissRM by providing it with a vectorizer and a list of strings\\n\\n```python\\nFaissRM(\\n    document_chunks: List[str],\\n    vectorizer: dsp.modules.sentence_vectorizer.BaseSentenceVectorizer,\\n    k: int = 3\\n)\\n```\\n\\n**Parameters:**\\n- `document_chunks` (_',\n",
       " '**\\n- `document_chunks` (_List[str]_): a list of strings that comprises the corpus to search. You cannot add/insert/upsert to this list after creating this FaissRM object.\\n- `vectorizer` (_dsp.modules.sentence_vectorizer.BaseSentenceVectorizer_, _optional_): If not provided, a dsp.modules.sentence_vectorizer.SentenceTransformersVectorizer object is created',\n",
       " '.SentenceTransformersVectorizer object is created and used.\\n- `k` (_int_, _optional_): The number of top passages to retrieve. Defaults to 3.\\n\\n### Methods\\n\\n#### `forward(self, query_or_queries: Union[str, List[str]]) -> dspy.Prediction`\\n\\nSearch the FaissRM vector database for the top `k` passages matching the given query or queries, using embed',\n",
       " ' passages matching the given query or queries, using embeddings generated via the vectorizer specified at FaissRM construction time\\n\\n**Parameters:**\\n- `query_or_queries` (_Union[str, List[str]]_): The query or list of queries to search for.\\n\\n**Returns:**\\n- `dspy.Prediction`: Contains the retrieved passages, each represented as a `dotdict` with a `long_text` attribute and',\n",
       " '` with a `long_text` attribute and an `index` attribute. The `index` attribute is the index in the document_chunks array provided to this FaissRM object at construction time.\\n\\n### Quickstart with the default vectorizer\\n\\nThe **FaissRM** module provides a retriever that uses an in-memory Faiss vector database. This module does not include a vectorizer; instead it supports any subclass of **dsp.modules.sentence_',\n",
       " ' of **dsp.modules.sentence_vectorizer.BaseSentenceVectorizer**. If a vectorizer is not provided, an instance of **dsp.modules.sentence_vectorizer.SentenceTransformersVectorizer** is created and used by **FaissRM**. Note that the default embedding model for **SentenceTransformersVectorizer** is **all-MiniLM-L6-v2**\\n\\n\\n```python\\nimport dspy',\n",
       " '\\n\\n\\n```python\\nimport dspy\\nfrom dspy.retrieve import FaissRM\\n\\ndocument_chunks = [\\n    \"The superbowl this year was played between the San Francisco 49ers and the Kanasas City Chiefs\",\\n    \"Pop corn is often served in a bowl\",\\n    \"The Rice Bowl is a Chinese Restaurant located in the city of Tucson, Arizona\",\\n    \"Mars is the fourth',\n",
       " '\",\\n    \"Mars is the fourth planet in the Solar System\",\\n    \"An aquarium is a place where children can learn about marine life\",\\n    \"The capital of the United States is Washington, D.C\",\\n    \"Rock and Roll musicians are honored by being inducted in the Rock and Roll Hall of Fame\",\\n    \"Music albums were published on Long Play Records in the 70s and 80s\",\\n  ',\n",
       " ' the 70s and 80s\",\\n    \"Sichuan cuisine is a spicy cuisine from central China\",\\n    \"The interest rates for mortgages is considered to be very high in 2024\",\\n]\\n\\nfrm = FaissRM(document_chunks)\\nturbo = dspy.OpenAI(model=\"gpt-3.5-turbo\")\\ndspy.settings.configure(lm=turbo, rm=',\n",
       " 'ure(lm=turbo, rm=frm)\\nprint(frm([\"I am in the mood for Chinese food\"]))\\n```\\n# retrieve.neo4j_rm\\n\\n### Constructor\\n\\nInitialize an instance of the `Neo4jRM` class.\\n\\n```python\\nNeo4jRM(\\n    index_name: str,\\n    text_node_property: str,\\n',\n",
       " '  text_node_property: str,\\n    k: int = 5,\\n    retrieval_query: str = None,\\n    embedding_provider: str = \"openai\",\\n    embedding_model: str = \"text-embedding-ada-002\",\\n)\\n```\\n\\n**Environment Variables:**\\n\\nYou need to define the credentials as environment variables:\\n\\n- `NEO4',\n",
       " ' environment variables:\\n\\n- `NEO4J_USERNAME` (_str_): Specifies the username required for authenticating with the Neo4j database. This is a crucial security measure to ensure that only authorized users can access the database.\\n\\n- `NEO4J_PASSWORD` (_str_): Defines the password associated with the `NEO4J_USERNAME` for authentication purposes. This password should be kept secure to prevent unauthorized access to the',\n",
       " ' should be kept secure to prevent unauthorized access to the database.\\n\\n- `NEO4J_URI` (_str_): Indicates the Uniform Resource Identifier (URI) used to connect to the Neo4j database. This URI typically includes the protocol, hostname, and port, providing the necessary information to establish a connection to the database.\\n\\n- `NEO4J_DATABASE` (_str_, optional): Specifies the name of the database to',\n",
       " ' optional): Specifies the name of the database to connect to within the Neo4j instance. If not set, the system defaults to using `\"neo4j\"` as the database name. This allows for flexibility in connecting to different databases within a single Neo4j server.\\n\\n- `OPENAI_API_KEY` (_str_): Specifies the API key required for authenticiating with OpenAI\\'s services.\\n\\n**Parameters:**\\n- `index_',\n",
       " '\\n**Parameters:**\\n- `index_name` (_str_): Specifies the name of the vector index to be used within Neo4j for organizing and querying data.\\n- `text_node_property` (_str_, _optional_): Defines the specific property of nodes that will be returned.\\n- `k` (_int_, _optional_): The number of top results to return from the retrieval operation. It defaults to 5 if not explicitly specified',\n",
       " ' operation. It defaults to 5 if not explicitly specified.\\n- `retrieval_query` (_str_, _optional_): A custom query string provided for retrieving data. If not provided, a default query tailored to the `text_node_property` will be used.\\n- `embedding_provider` (_str_, _optional_): The name of the service provider for generating embeddings. Defaults to \"openai\" if not specified.\\n-',\n",
       " ' \"openai\" if not specified.\\n- `embedding_model` (_str_, _optional_): The specific embedding model to use from the provider. By default, it uses the \"text-embedding-ada-002\" model from OpenAI.\\n\\n\\n### Methods\\n\\n#### `forward(self, query: [str], k: Optional[int] = None) -> dspy.Prediction`\\n\\nSearch the neo4j vector index for',\n",
       " '\\n\\nSearch the neo4j vector index for the top `k` passages matching the given query or queries, using embeddings generated via the specified `embedding_model`.\\n\\n**Parameters:**\\n- `query` (str_): The query.\\n- `k` (_Optional[int]_, _optional_): The number of results to retrieve. If not specified, defaults to the value set during initialization.\\n\\n**Returns:**\\n- `d',\n",
       " \"\\n\\n**Returns:**\\n- `dspy.Prediction`: Contains the retrieved passages as a list of string with the prediction signature.\\n\\nex:\\n```python\\nPrediction(\\n    passages=['Passage 1 Lorem Ipsum awesome', 'Passage 2 Lorem Ipsum Youppidoo', 'Passage 3 Lorem Ipsum Yassssss']\\n)\\n```\\n\\n### Quick Example how to use Neo4\",\n",
       " '\\n\\n### Quick Example how to use Neo4j in a local environment. \\n\\n\\n```python\\nfrom dspy.retrieve.neo4j_rm import Neo4jRM\\nimport os\\n\\nos.environ[\"NEO4J_URI\"] = \\'bolt://localhost:7687\\'\\nos.environ[\"NEO4J_USERNAME\"] = \\'neo4j\\'\\nos.environ[\"NEO4J_',\n",
       " 'os.environ[\"NEO4J_PASSWORD\"] = \\'password\\'\\nos.environ[\"OPENAI_API_KEY\"] = \\'sk-\\'\\n\\nretriever_model = Neo4jRM(\\n    index_name=\"vector\",\\n    text_node_property=\"text\"\\n)\\n\\nresults = retriever_model(\"Explore the significance of quantum computing\", k=3)\\n\\nfor passage in results:',\n",
       " '=3)\\n\\nfor passage in results:\\n    print(\"Document:\", passage, \"\\\\n\")\\n```\\n---\\nsidebar_position: 3\\n---\\n\\n# retrieve.AzureCognitiveSearch\\n\\n### Constructor\\n\\nThe constructor initializes an instance of the `AzureCognitiveSearch` class and sets up parameters for sending queries and retreiving results with the Azure Cognitive Search server.\\n\\n```python\\nclass AzureCognitive',\n",
       " '\\n\\n```python\\nclass AzureCognitiveSearch:\\n    def __init__(\\n        self,\\n        search_service_name: str,\\n        search_api_key: str,\\n        search_index_name: str,\\n        field_text: str,\\n      ',\n",
       " ': str,\\n        field_score: str, # required field to map with \"score\" field in dsp framework\\n    ):\\n```\\n\\n**Parameters:**\\n\\n- `search_service_name` (_str_): Name of Azure Cognitive Search server.\\n- `search_api_key` (_str_): API Authentication token for accessing Azure Cognitive Search server.\\n- `search_index_name` (_str',\n",
       " '- `search_index_name` (_str_): Name of search index in the Azure Cognitive Search server.\\n- `field_text` (_str_): Field name that maps to DSP \"content\" field.\\n- `field_score` (_str_): Field name that maps to DSP \"score\" field.\\n\\n### Methods\\n\\nRefer to [ColBERTv2](/api/retrieval_model_clients/ColBERTv',\n",
       " '_model_clients/ColBERTv2) documentation. Keep in mind there is no `simplify` flag for AzureCognitiveSearch.\\n\\nAzureCognitiveSearch supports sending queries and processing the received results, mapping content and scores to a correct format for the Azure Cognitive Search server.\\n\\n### Deprecation Notice\\n\\nThis module is scheduled for removal in future releases. Please use the AzureAISearchRM class from dspy.retrieve.az',\n",
       " ' class from dspy.retrieve.azureaisearch_rm instead.For more information, refer to the updated documentation(docs/docs/deep-dive/retrieval_models_clients/Azure.mdx).\\n---\\nsidebar_position: 1\\n---\\n\\n# dspy.ColBERTv2\\n\\n### Constructor\\n\\nThe constructor initializes the `ColBERTv2` class instance and sets up the request',\n",
       " 'v2` class instance and sets up the request parameters for interacting with the ColBERTv2 server.\\n\\n```python\\nclass ColBERTv2:\\n    def __init__(\\n        self,\\n        url: str = \"http://0.0.0.0\",\\n        port: Optional[Union[str, int]] = None,\\n ',\n",
       " '[str, int]] = None,\\n        post_requests: bool = False,\\n    ):\\n```\\n\\n**Parameters:**\\n- `url` (_str_): URL for ColBERTv2 server.\\n- `port` (_Union[str, int]_, _Optional_): Port endpoint for ColBERTv2 server. Defaults to `None`.\\n- `post_requests` (_',\n",
       " '`.\\n- `post_requests` (_bool_, _Optional_): Flag for using HTTP POST requests. Defaults to `False`.\\n\\n### Methods\\n\\n#### `__call__(self, query: str, k: int = 10, simplify: bool = False) -> Union[list[str], list[dotdict]]`\\n\\nEnables making queries to the ColBERTv2 server for retrieval. Internally, the method handles the specifics of preparing',\n",
       " ' Internally, the method handles the specifics of preparing the request prompt and corresponding payload to obtain the response. The function handles the retrieval of the top-k passages based on the provided query.\\n\\n**Parameters:**\\n- `query` (_str_): Query string used for retrieval.\\n- `k` (_int_, _optional_): Number of passages to retrieve. Defaults to 10.\\n- `simplify` (_bool_, _optional_): Flag for',\n",
       " ' (_bool_, _optional_): Flag for simplifying output to a list of strings. Defaults to False.\\n\\n**Returns:**\\n- `Union[list[str], list[dotdict]]`: Depending on `simplify` flag, either a list of strings representing the passage content (`True`) or a list of `dotdict` instances containing passage details (`False`).\\n\\n### Quickstart\\n\\n```python\\nimport dsp',\n",
       " \"start\\n\\n```python\\nimport dspy\\n\\ncolbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\\n\\nretrieval_response = colbertv2_wiki17_abstracts('When was the first FIFA World Cup held?', k=5)\\n\\nfor result in retrieval_response:\\n\",\n",
       " '\\n\\nfor result in retrieval_response:\\n    print(\"Text:\", result[\\'text\\'], \"\\\\n\")\\n```\\n---\\nsidebar_position: 2\\n---\\n\\n# retrieve.ChromadbRM\\n\\n### Constructor\\n\\nInitialize an instance of the `ChromadbRM` class, with the option to use OpenAI\\'s embeddings or any alternative supported by chromadb, as detailed in the official [chromad',\n",
       " 'b, as detailed in the official [chromadb embeddings documentation](https://docs.trychroma.com/embeddings).\\n\\n```python\\nChromadbRM(\\n    collection_name: str,\\n    persist_directory: str,\\n    embedding_function: Optional[EmbeddingFunction[Embeddable]] = OpenAIEmbeddingFunction(),\\n    k: int = 7',\n",
       " '(),\\n    k: int = 7,\\n)\\n```\\n\\n**Parameters:**\\n- `collection_name` (_str_): The name of the chromadb collection.\\n- `persist_directory` (_str_): Path to the directory where chromadb data is persisted.\\n- `embedding_function` (_Optional[EmbeddingFunction[Embeddable]]_, _optional_): The function used for embedding documents and',\n",
       " '_): The function used for embedding documents and queries. Defaults to `DefaultEmbeddingFunction()` if not specified.\\n- `k` (_int_, _optional_): The number of top passages to retrieve. Defaults to 7.\\n\\n### Methods\\n\\n#### `forward(self, query_or_queries: Union[str, List[str]], k: Optional[int] = None) -> dspy.Prediction`\\n\\n',\n",
       " ' -> dspy.Prediction`\\n\\nSearch the chromadb collection for the top `k` passages matching the given query or queries, using embeddings generated via the specified `embedding_function`.\\n\\n**Parameters:**\\n- `query_or_queries` (_Union[str, List[str]]_): The query or list of queries to search for.\\n- `k` (_Optional[int]_, _optional_): The number of',\n",
       " ']_, _optional_): The number of results to retrieve. If not specified, defaults to the value set during initialization.\\n\\n**Returns:**\\n- `dspy.Prediction`: Contains the retrieved passages, each represented as a `dotdict` with schema `[{\"id\": str, \"score\": float, \"long_text\": str, \"metadatas\": dict }]`\\n\\n### Quickstart with OpenAI Embeddings\\n\\n',\n",
       " ' Quickstart with OpenAI Embeddings\\n\\nChromadbRM have the flexibility from a variety of embedding functions as outlined in the [chromadb embeddings documentation](https://docs.trychroma.com/embeddings). While different options are available, this example demonstrates how to utilize OpenAI embeddings specifically.\\n\\n```python\\nfrom dspy.retrieve.chromadb_rm import ChromadbRM\\nimport os\\nimport',\n",
       " ' import ChromadbRM\\nimport os\\nimport openai\\nfrom chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\\n\\nembedding_function = OpenAIEmbeddingFunction(\\n    api_key=os.environ.get(\\'OPENAI_API_KEY\\'),\\n    model_name=\"text-embedding-ada-002\"\\n)\\n\\nretriever_model = Chromad',\n",
       " '\\n\\nretriever_model = ChromadbRM(\\n    \\'your_collection_name\\',\\n    \\'/path/to/your/db\\',\\n    embedding_function=embedding_function,\\n    k=5\\n)\\n\\nresults = retriever_model(\"Explore the significance of quantum computing\", k=5)\\n\\nfor result in results:\\n    print(\"Document:\", result.long_',\n",
       " '  print(\"Document:\", result.long_text, \"\\\\n\")\\n```\\n# dspy.OllamaLocal\\n\\n:::note\\nAdapted from documentation provided by https://github.com/insop\\n:::\\n\\nOllama is a good software tool that allows you to run LLMs locally, such as Mistral, Llama2, and Phi.\\nThe following are the instructions to install and run Ollama.\\n\\n### Pre',\n",
       " ' and run Ollama.\\n\\n### Prerequisites\\n\\nInstall Ollama by following the instructions from this page:\\n\\n- https://ollama.ai\\n\\nDownload model: `ollama pull`\\n\\nDownload a model by running the `ollama pull` command. You can download Mistral, Llama2, and Phi.\\n\\n```bash\\n# download mistral\\nollama pull mistral\\n```\\n\\nHere is the list of other models',\n",
       " '`\\n\\nHere is the list of other models you can download:\\n- https://ollama.ai/library\\n\\n### Running Ollama model\\n\\nRun model: `ollama run`\\n\\nYou can test a model by running the model with the `ollama run` command.\\n\\n```bash\\n# run mistral\\nollama run mistral\\n```\\n\\n### Sending requests to the server\\n\\nHere is the code to load a model through',\n",
       " \"\\nHere is the code to load a model through Ollama:\\n\\n```python\\nlm = dspy.OllamaLocal(model='mistral')\\n```\\n# dspy.HFModel\\n\\nInitialize `HFModel` within your program with the desired model to load in. Here's an example call:\\n\\n```python\\nllama = dspy.HFModel(model = 'meta-llama/Llama\",\n",
       " \" = 'meta-llama/Llama-2-7b-hf')\\n```# dspy.HFClientTGI\\n\\n## Prerequisites\\n\\n- Docker must be installed on your system. If you don't have Docker installed, you can get it from [here](https://docs.docker.com/get-docker/).\\n\\n## Setting up the Text-Generation-Inference Server\\n\\n1. Clone the Text-Generation\",\n",
       " '\\n\\n1. Clone the Text-Generation-Inference repository from GitHub by executing the following command:\\n\\n   ```\\n   git clone https://github.com/huggingface/text-generation-inference.git\\n   ```\\n\\n2. Change into the cloned repository directory:\\n\\n   ```\\n   cd text-generation-inference\\n   ```\\n\\n3. Execute the Docker command under the \"',\n",
       " '3. Execute the Docker command under the \"Get Started\" section to run the server:\\n\\n\\n   ```\\n   model=meta-llama/Llama-2-7b-hf # set to the specific Hugging Face model ID you wish to use.\\n   num_shard=2 # set to the number of shards you wish to use.\\n   volume=$PWD/data # share a volume with the Docker container to',\n",
       " 'data # share a volume with the Docker container to avoid downloading weights every run\\n\\n   docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:0.9 --model-id $model --num-shard $num_shard\\n   ```\\n\\n   This command will start the server and make it accessible at `http',\n",
       " ' start the server and make it accessible at `http://localhost:8080`.\\n\\nIf you want to connect to [Meta Llama 2 models](https://huggingface.co/meta-llama), make sure to use version 9.3 (or higher) of the docker image (ghcr.io/huggingface/text-generation-inference:0.9.3) and pass in your huggingface token as an environment variable.\\n\\n``',\n",
       " 'face token as an environment variable.\\n\\n```\\n   docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data -e HUGGING_FACE_HUB_TOKEN={your_token} ghcr.io/huggingface/text-generation-inference:0.9.3 --model-id $model --num-shard $num_shard\\n```\\n',\n",
       " 'ard $num_shard\\n```\\n\\n## Sending requests to the server\\n\\nAfter setting up the text-generation-inference server and ensuring that it displays \"Connected\" when it\\'s running, you can interact with it using the `HFClientTGI`.\\n\\nInitialize the `HFClientTGI` within your program with the desired parameters. Here is an example call:\\n\\n   ```python\\n   lm = dspy.HFClient',\n",
       " '  lm = dspy.HFClientTGI(model=\"meta-llama/Llama-2-7b-hf\", port=8080, url=\"http://localhost\")\\n   ```\\n\\n   Customize the `model`, `port`, and `url` according to your requirements. The `model` parameter should be set to the specific Hugging Face model ID you wish to use. \\n\\n\\n### FAQs\\n\\n1.',\n",
       " ' \\n\\n\\n### FAQs\\n\\n1. If your model doesn\\'t require any shards, you still need to set a value for `num_shard`, but you don\\'t need to include the parameter `--num-shard` on the command line.\\n\\n2. If your model runs into any \"token exceeded\" issues, you can set the following parameters on the command line to adjust the input length and token limit:\\n   - `--max-input-length',\n",
       " '   - `--max-input-length`: Set the maximum allowed input length for the text.\\n   - `--max-total-tokens`: Set the maximum total tokens allowed for text generation.\\n\\nPlease refer to the [official Text-Generation-Inference repository](https://github.com/huggingface/text-generation-inference) for more detailed information and documentation.\\n# dspy.HFClientVLLM',\n",
       " '# dspy.HFClientVLLM\\n\\n### Setting up the vLLM Server\\n\\nFollow these steps to set up the vLLM Server:\\n\\n1. Build the server from source by following the instructions provided in the [Build from Source guide](https://vllm.readthedocs.io/en/latest/getting_started/installation.html#build-from-source).\\n\\n2. Start the server by running the following',\n",
       " '\\n2. Start the server by running the following command, and specify your desired model, host, and port using the appropriate arguments. The default server address is http://localhost:8000.\\n\\nExample command:\\n\\n```bash\\n   python -m vllm.entrypoints.openai.api_server --model mosaicml/mpt-7b --port 8000\\n```\\n\\nThis will launch the vLLM server.\\n\\n### Sending requests to',\n",
       " 'LLM server.\\n\\n### Sending requests to the server\\n\\nAfter setting up the vLLM server and ensuring that it displays \"Connected\" when it\\'s running, you can interact with it using the `HFClientVLLM`.\\n\\nInitialize the `HFClientVLLM` within your program with the desired parameters. Here is an example call:\\n\\n```python\\n   lm = dspy.HFClientVLLM(model=\"m',\n",
       " '.HFClientVLLM(model=\"mosaicml/mpt-7b\", port=8000, url=\"http://localhost\")\\n```\\n\\nCustomize the `model`, `port`, `url`, and `max_tokens` according to your requirements. The `model` parameter should be set to the specific Hugging Face model ID you wish to use.\\n\\nPlease refer to the [official vLLM repository](https://github.com/',\n",
       " 'LLM repository](https://github.com/vllm-project/vllm) for more detailed information and documentation.\\n# dspy.ChatModuleClient\\n\\n## Prerequisites\\n\\n1. Install the required packages using the following commands:\\n   \\n   ```shell\\n   pip install --no-deps --pre --force-reinstall mlc-ai-nightly-cu118 mlc-chat-nightly-cu',\n",
       " '118 mlc-chat-nightly-cu118 -f https://mlc.ai/wheels\\n   pip install transformers\\n   git lfs install\\n   ```\\n   \\n   Adjust the pip wheels according to your OS/platform by referring to the provided commands in [MLC packages](https://mlc.ai/package/).\\n\\n## Running MLC Llama-2 models\\n\\n1. Create a directory for pre',\n",
       " ' models\\n\\n1. Create a directory for prebuilt models:\\n\\n   ```shell\\n   mkdir -p dist/prebuilt\\n   ```\\n   \\n2. Clone the necessary libraries from the repository:\\n\\n   ```shell\\n   git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib\\n   cd dist/prebuilt\\n ',\n",
       " '\\n   cd dist/prebuilt\\n   ```\\n   \\n3. Choose a Llama-2 model from [MLC LLMs](https://huggingface.co/mlc-ai) and clone the model repository:\\n\\n   ```shell\\n   git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4',\n",
       " \"7b-chat-hf-q4f16_1\\n   ```\\n\\n4. Initialize the `ChatModuleClient` within your program with the desired parameters. Here's an example call:\\n\\n   ```python\\n   llama = dspy.ChatModuleClient(model='dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1', model_\",\n",
       " \"-q4f16_1', model_path='dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so')\\n   ```\\nPlease refer to the [official MLC repository](https://github.com/mlc-ai/mlc-llm) for more detailed information and [documentation](https://mlc.ai/mlc-llm/\",\n",
       " 'c.ai/mlc-llm/docs/get_started/try_out.html).\\n# dspy.MultiChainComparison\\n\\n### Constructor\\n\\nThe constructor initializes the `MultiChainComparison` class and sets up its attributes. It inherits from the `Predict` class and adds specific functionality for multiple chain comparisons.\\n\\nThe class incorporates multiple student attempt reasonings and concludes with the selected best reasoning path out of the available attempts.',\n",
       " ' selected best reasoning path out of the available attempts.\\n\\n```python\\nfrom .predict import Predict\\nfrom ..primitives.program import Module\\n\\nimport dsp\\n\\nclass MultiChainComparison(Module):\\n    def __init__(self, signature, M=3, temperature=0.7, **config):\\n        super().__init__()\\n\\n        self.M = M\\n',\n",
       " '     self.M = M\\n        signature = Predict(signature).signature\\n        *keys, last_key = signature.kwargs.keys()\\n\\n        extended_kwargs = {key: signature.kwargs[key] for key in keys}\\n\\n        for idx in range(M):\\n       ',\n",
       " 'M):\\n            candidate_type = dsp.Type(prefix=f\"Student Attempt #{idx+1}:\", desc=\"${reasoning attempt}\")\\n            extended_kwargs.update({f\\'reasoning_attempt_{idx+1}\\': candidate_type})\\n        \\n        rationale',\n",
       " ' \\n        rationale_type = dsp.Type(prefix=\"Accurate Reasoning: Thank you everyone. Let\\'s now holistically\", desc=\"${corrected reasoning}\")\\n        extended_kwargs.update({\\'rationale\\': rationale_type, last_key: signature.kwargs[last_key]})\\n\\n        signature = dsp.Template(signature.instruct',\n",
       " ' dsp.Template(signature.instructions, **extended_kwargs)\\n        self.predict = Predict(signature, temperature=temperature, **config)\\n        self.last_key = last_key\\n```\\n\\n**Parameters:**\\n- `signature` (_Any_): Signature of predictive model.\\n- `M` (_int_, _optional_): Number',\n",
       " '` (_int_, _optional_): Number of student reasoning attempts. Defaults to `3`.\\n- `temperature` (_float_, _optional_): Temperature parameter for prediction. Defaults to `0.7`.\\n- `**config` (_dict_): Additional configuration parameters for model.\\n\\n### Method\\n\\n#### `forward(self, completions, **kwargs)`\\n\\nThis method aggregates all the student reasoning attempts and calls the predict method',\n",
       " ' all the student reasoning attempts and calls the predict method with extended signatures to get the best reasoning.\\n\\n**Parameters:**\\n- `completions`: List of completion objects which include student reasoning attempts.\\n- `**kwargs`: Additional keyword arguments.\\n\\n**Returns:**\\n- The result of the `predict` method for the best reasoning.\\n\\n### Examples\\n\\n```python\\nclass BasicQA(dspy.Signature):\\n',\n",
       " 'A(dspy.Signature):\\n    \"\"\"Answer questions with short factoid answers.\"\"\"\\n    question = dspy.InputField()\\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\\n\\n# Example completions generated by a model for reference\\ncompletions = [\\n    dspy.Prediction(rationale=\"I recall that during clear days, the sky often',\n",
       " 'I recall that during clear days, the sky often appears this color.\", answer=\"blue\"),\\n    dspy.Prediction(rationale=\"Based on common knowledge, I believe the sky is typically seen as this color.\", answer=\"green\"),\\n    dspy.Prediction(rationale=\"From images and depictions in media, the sky is frequently represented with this hue.\", answer=\"blue\"),\\n]\\n\\n# Pass signature to MultiChainComparison module\\n',\n",
       " '# Pass signature to MultiChainComparison module\\ncompare_answers = dspy.MultiChainComparison(BasicQA)\\n\\n# Call the MultiChainComparison on the completions\\nquestion = \\'What is the color of the sky?\\'\\nfinal_pred = compare_answers(completions, question=question)\\n\\nprint(f\"Question: {question}\")\\nprint(f\"Final Predicted Answer (after comparison): {',\n",
       " '\"Final Predicted Answer (after comparison): {final_pred.answer}\")\\nprint(f\"Final Rationale: {final_pred.rationale}\")\\n```\\n# dspy.ChainOfThought\\n\\n### Constructor\\n\\nThe constructor initializes the `ChainOfThought` class and sets up its attributes. It inherits from the `Predict` class and adds specific functionality for chain of thought processing. \\n\\nInternally, the',\n",
       " ' thought processing. \\n\\nInternally, the class initializes the `activated` attribute to indicate if chain of thought processing has been selected. It extends the `signature` to include additional reasoning steps and an updated `rationale_type` when chain of thought processing is activated.\\n\\n```python\\nclass ChainOfThought(Predict):\\n    def __init__(self, signature, rationale_type=None, activated=True, **config):\\n',\n",
       " 'None, activated=True, **config):\\n        super().__init__(signature, **config)\\n\\n        self.activated = activated\\n\\n        signature = self.signature\\n        *keys, last_key = signature.kwargs.keys()\\n\\n        DEFAULT_RATIONALE_TYPE = dsp.Type(',\n",
       " 'ATIONALE_TYPE = dsp.Type(prefix=\"Reasoning: Let\\'s think step by step in order to\",\\n                                          desc=\"${produce the \" + last_key + \"}. We ...\")\\n\\n        rationale_type = rationale_type or',\n",
       " \"   rationale_type = rationale_type or DEFAULT_RATIONALE_TYPE\\n        \\n        extended_kwargs = {key: signature.kwargs[key] for key in keys}\\n        extended_kwargs.update({'rationale': rationale_type, last_key: signature.kwargs[last_key]})\\n       \",\n",
       " ']})\\n        \\n        self.extended_signature = dsp.Template(signature.instructions, **extended_kwargs)\\n```\\n\\n**Parameters:**\\n- `signature` (_Any_): Signature of predictive model.\\n- `rationale_type` (_dsp.Type_, _optional_): Rationale type for reasoning steps. Defaults to `',\n",
       " \"e type for reasoning steps. Defaults to `None`.\\n- `activated` (_bool_, _optional_): Flag for activated chain of thought processing. Defaults to `True`.\\n- `**config` (_dict_): Additional configuration parameters for model.\\n\\n### Method\\n\\n#### `forward(self, **kwargs)`\\n\\nThis method extends the parent `Predict` class' forward pass while updating the signature when chain of thought reasoning is activated or if\",\n",
       " ' signature when chain of thought reasoning is activated or if the language model is a GPT3 model.\\n\\n**Parameters:**\\n- `**kwargs`: Keyword arguments required for prediction.\\n\\n**Returns:**\\n- The result of the `forward` method.\\n\\n### Examples\\n\\n```python\\n#Define a simple signature for basic question answering\\nclass BasicQA(dspy.Signature):\\n    \"\"\"Answer questions with short',\n",
       " '):\\n    \"\"\"Answer questions with short factoid answers.\"\"\"\\n    question = dspy.InputField()\\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\\n\\n#Pass signature to ChainOfThought module\\ngenerate_answer = dspy.ChainOfThought(BasicQA)\\n\\n# Call the predictor on a particular input.\\nquestion=\\'What is the color of the',\n",
       " '.\\nquestion=\\'What is the color of the sky?\\'\\npred = generate_answer(question=question)\\n\\nprint(f\"Question: {question}\")\\nprint(f\"Predicted Answer: {pred.answer}\")\\n```\\n# dspy.Predict\\n\\n### Constructor\\n\\nThe constructor initializes the `Predict` class and sets up its attributes, taking in the `signature` and additional config options. If the `sign',\n",
       " '` and additional config options. If the `signature` is a string, it processes the input and output fields, generates instructions, and creates a template for the specified `signature` type.\\n\\n```python\\nclass Predict(Parameter):\\n    def __init__(self, signature, **config):\\n        self.stage = random.randbytes(8).hex()\\n        self.signature',\n",
       " '       self.signature = signature\\n        self.config = config\\n        self.reset()\\n\\n        if isinstance(signature, str):\\n            inputs, outputs = signature.split(\"->\")\\n            inputs, outputs = inputs.split(\",\"), outputs.',\n",
       " ' outputs = inputs.split(\",\"), outputs.split(\",\")\\n            inputs, outputs = [field.strip() for field in inputs], [field.strip() for field in outputs]\\n\\n            assert all(len(field.split()) == 1 for field in (inputs + outputs))\\n\\n            inputs_ = \\', \\'.join',\n",
       " '     inputs_ = \\', \\'.join([f\"`{field}`\" for field in inputs])\\n            outputs_ = \\', \\'.join([f\"`{field}`\" for field in outputs])\\n\\n            instructions = f\"\"\"Given the fields {inputs_}, produce the fields {outputs_}.\"\"\"\\n\\n         ',\n",
       " '\\n\\n            inputs = {k: InputField() for k in inputs}\\n            outputs = {k: OutputField() for k in outputs}\\n\\n            for k, v in inputs.items():\\n                v.finalize(k, infer_prefix(k))',\n",
       " 'ize(k, infer_prefix(k))\\n            \\n            for k, v in outputs.items():\\n                v.finalize(k, infer_prefix(k))\\n\\n            self.signature = dsp.Template(instructions, **',\n",
       " ' dsp.Template(instructions, **inputs, **outputs)\\n```\\n\\n**Parameters:**\\n- `signature` (_Any_): Signature of predictive model.\\n- `**config` (_dict_): Additional configuration parameters for model.\\n\\n### Method\\n\\n#### `__call__(self, **kwargs)`\\n\\nThis method serves as a wrapper for the `forward` method. It allows making predictions using the `Predict',\n",
       " '. It allows making predictions using the `Predict` class by providing keyword arguments.\\n\\n**Paramters:**\\n- `**kwargs`: Keyword arguments required for prediction.\\n\\n**Returns:**\\n- The result of `forward` method.\\n\\n### Examples\\n\\n```python\\n#Define a simple signature for basic question answering\\nclass BasicQA(dspy.Signature):\\n    \"\"\"Answer questions with short factoid answers',\n",
       " '   \"\"\"Answer questions with short factoid answers.\"\"\"\\n    question = dspy.InputField()\\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\\n\\n#Pass signature to Predict module\\ngenerate_answer = dspy.Predict(BasicQA)\\n\\n# Call the predictor on a particular input.\\nquestion=\\'What is the color of the sky?\\'\\npred = generate_answer',\n",
       " ' of the sky?\\'\\npred = generate_answer(question=question)\\n\\nprint(f\"Question: {question}\")\\nprint(f\"Predicted Answer: {pred.answer}\")\\n```\\n# dspy.ProgramOfThought\\n\\n### Constructor\\n\\nThe constructor initializes the `ProgramOfThought` class and sets up its attributes. It is designed to generate and execute Python code based on input fields, producing a single output field through',\n",
       " ' on input fields, producing a single output field through iterative refinement. It supports multiple iterations to refine the code in case of errors.\\n\\n```python\\nimport dsp\\nimport dspy\\nfrom ..primitives.program import Module\\nfrom ..primitives.python_interpreter import CodePrompt, PythonInterpreter\\nimport re\\n\\nclass ProgramOfThought(Module):\\n    def __init__(self, signature, max_iters',\n",
       " '__(self, signature, max_iters=3):\\n        ...\\n```\\n\\n**Parameters:**\\n- `signature` (_dspy.Signature_): Signature defining the input and output fields for the program.\\n- `max_iters` (_int_, _optional_): Maximum number of iterations for refining the generated code. Defaults to `3`.\\n\\n### Methods\\n\\n#### `_generate',\n",
       " '\\n### Methods\\n\\n#### `_generate_signature(self, mode)`\\n\\nGenerates a signature dict for different modes: `generate`, `regenerate`, and `answer`.\\n\\nThe `generate` mode serves as an initial generation of Python code with the signature (`question -> generated_code`).\\nThe `regenerate` mode serves as a refining generation of Python code, accounting for the past generated code and existing error with the signature',\n",
       " ' the past generated code and existing error with the signature (`question, previous_code, error -> generated_code`).\\nThe `answer` mode serves to execute the last stored generated code and output the final answer to the question with the signature (`question, final_generated_code, code_output -> answer`).\\n\\n**Parameters:**\\n- `mode` (_str_): Mode of operation of Program of Thought.\\n\\n**Returns:**\\n- A dictionary',\n",
       " '\\n\\n**Returns:**\\n- A dictionary representing the signature for specified mode.\\n\\n#### `_generate_instruction(self, mode)`\\n\\nGenerates instructions for code generation based on the mode. This ensures the signature accounts for relevant instructions in generating an answer to the inputted question by producing executable Python code.\\n\\nThe instructional modes mirror the signature modes: `generate`, `regenerate`, `answer`\\n\\n**Parameters:**\\n- `',\n",
       " '`\\n\\n**Parameters:**\\n- `mode` (_str_): Mode of operation.\\n\\n**Returns:**\\n- A string representing instructions for specified mode.\\n\\n#### `parse_code(self, code_data)`\\n\\nParses the generated code and checks for formatting errors.\\n\\n**Parameters:**\\n- `code_data` (_dict_): Data containing the generated code. `key - generated_code`, `val - {Python',\n",
       " ' - generated_code`, `val - {Python code string}`\\n\\n**Returns:**\\n- Tuple containing parsed code and any error message.\\n\\n#### `execute_code(self, code)`\\n\\nExecutes the parsed code and captures the output or error.\\n\\n**Parameters:**\\n- `code` (_str_): Code to be executed.\\n\\n**Returns:**\\n- Tuple containing the code, its output, and any error message',\n",
       " ' the code, its output, and any error message.\\n\\n#### `forward(self, **kwargs)`\\n\\nMain method to execute the code generation and refinement process.\\n\\n**Parameters:**\\n- `**kwargs`: Keyword arguments corresponding to input fields.\\n\\n**Returns:**\\n- The final answer generated by the program or `None` in case of persistent errors.\\n\\n### Examples\\n\\n```python\\n#Define a simple signature',\n",
       " '```python\\n#Define a simple signature for basic question answering\\nclass GenerateAnswer(dspy.Signature):\\n    \"\"\"Answer questions with short factoid answers.\"\"\"\\n    question = dspy.InputField()\\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\\n\\n# Pass signature to ProgramOfThought Module\\npot = dspy.ProgramOfThought(Gener',\n",
       " ' dspy.ProgramOfThought(GenerateAnswer)\\n\\n#Call the ProgramOfThought module on a particular input\\nquestion = \\'Sarah has 5 apples. She buys 7 more apples from the store. How many apples does Sarah have now?\\'\\nresult = pot(question=question)\\n\\nprint(f\"Question: {question}\")\\nprint(f\"Final Predicted Answer (after ProgramOfThought process): {result.answer}\")\\n```',\n",
       " '): {result.answer}\")\\n```\\n# dspy.Retrieve\\n\\n### Constructor\\n\\nThe constructor initializes the `Retrieve` class and sets up its attributes, taking in `k` number of retrieval passages to return for a query.\\n\\n```python\\nclass Retrieve(Parameter):\\n    def __init__(self, k=3):\\n        self.stage = random.randbytes(',\n",
       " '  self.stage = random.randbytes(8).hex()\\n        self.k = k\\n```\\n\\n**Parameters:**\\n- `k` (_Any_): Number of retrieval responses\\n\\n### Method\\n\\n#### `__call__(self, *args, **kwargs):`\\n\\nThis method serves as a wrapper for the `forward` method. It allows making retrievals on an input query using the `Ret',\n",
       " \"evals on an input query using the `Retrieve` class.\\n\\n**Parameters:**\\n- `**args`: Arguments required for retrieval.\\n- `**kwargs`: Keyword arguments required for retrieval.\\n\\n**Returns:**\\n- The result of the `forward` method.\\n\\n### Examples\\n\\n```python\\nquery='When was the first FIFA World Cup held?'\\n\\n# Call the retriever on a particular query.\\nret\",\n",
       " ' the retriever on a particular query.\\nretrieve = dspy.Retrieve(k=3)\\ntopK_passages = retrieve(query).passages\\n\\nprint(f\"Top {retrieve.k} passages for question: {query} \\\\n\", \\'-\\' * 30, \\'\\\\n\\')\\n\\nfor idx, passage in enumerate(topK_passages):\\n    print(f\\'{idx+1}]',\n",
       " \"(f'{idx+1}]', passage, '\\\\n')\\n```\\n# dspy.ChainOfThoughtWithHint\\n\\n### Constructor\\n\\nThe constructor initializes the `ChainOfThoughtWithHint` class and sets up its attributes, inheriting from the `Predict` class. This class enhances the `ChainOfThought` class by offering an additional option to provide hints for reasoning. Two distinct signature templates are created internally\",\n",
       " ' for reasoning. Two distinct signature templates are created internally depending on the presence of the hint.\\n\\n```python\\nclass ChainOfThoughtWithHint(Predict):\\n    def __init__(self, signature, rationale_type=None, activated=True, **config):\\n        super().__init__(signature, **config)\\n\\n        self.activated = activated\\n\\n    ',\n",
       " ' self.activated = activated\\n\\n        signature = self.signature\\n        *keys, last_key = signature.kwargs.keys()\\n\\n        DEFAULT_HINT_TYPE = dsp.Type(prefix=\"Hint:\", desc=\"${hint}\")\\n\\n        DEFAULT_RATIONALE_TYPE = dsp.Type(prefix=\"Reason',\n",
       " 'TYPE = dsp.Type(prefix=\"Reasoning: Let\\'s think step by step in order to\",\\n                                          desc=\"${produce the \" + last_key + \"}. We ...\")\\n\\n        rationale_type = rationale_type or DEFAULT_',\n",
       " \"_type = rationale_type or DEFAULT_RATIONALE_TYPE\\n        \\n        extended_kwargs1 = {key: signature.kwargs[key] for key in keys}\\n        extended_kwargs1.update({'rationale': rationale_type, last_key: signature.kwargs[last_key]})\\n\\n        extended\",\n",
       " \"})\\n\\n        extended_kwargs2 = {key: signature.kwargs[key] for key in keys}\\n        extended_kwargs2.update({'hint': DEFAULT_HINT_TYPE, 'rationale': rationale_type, last_key: signature.kwargs[last_key]})\\n        \\n        self.ext\",\n",
       " '        self.extended_signature1 = dsp.Template(signature.instructions, **extended_kwargs1)\\n        self.extended_signature2 = dsp.Template(signature.instructions, **extended_kwargs2)\\n```\\n\\n**Parameters:**\\n- `signature` (_Any_): Signature of predictive model.\\n- `',\n",
       " '_): Signature of predictive model.\\n- `rationale_type` (_dsp.Type_, _optional_): Rationale type for reasoning steps. Defaults to `None`.\\n- `activated` (_bool_, _optional_): Flag for activated chain of thought processing. Defaults to `True`.\\n- `**config` (_dict_): Additional configuration parameters for model.\\n\\n### Method\\n\\n#### `forward(self, **kwargs)`\\n',\n",
       " \"forward(self, **kwargs)`\\n\\nThis method extends the parent `Predict` class's forward pass, updating the signature dynamically based on the presence of `hint` in the keyword arguments and the `activated` attribute.\\n\\n**Parameters:**\\n- `**kwargs`: Keyword arguments required for prediction.\\n\\n**Returns:**\\n- The result of the `forward` method in the parent `Predict` class.\\n\\n### Examples\\n\",\n",
       " 'Predict` class.\\n\\n### Examples\\n\\n```python\\n#Define a simple signature for basic question answering\\nclass BasicQA(dspy.Signature):\\n    \"\"\"Answer questions with short factoid answers.\"\"\"\\n    question = dspy.InputField()\\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\\n\\n#Pass signature to ChainOfThought module\\ngener',\n",
       " 'Pass signature to ChainOfThought module\\ngenerate_answer = dspy.ChainOfThoughtWithHint(BasicQA)\\n\\n# Call the predictor on a particular input alongside a hint.\\nquestion=\\'What is the color of the sky?\\'\\nhint = \"It\\'s what you often see during a sunny day.\"\\npred = generate_answer(question=question, hint=hint)\\n\\nprint(f\"Question: {question}\")\\n',\n",
       " '(f\"Question: {question}\")\\nprint(f\"Predicted Answer: {pred.answer}\")\\n```\\n# dspy.ReAct\\n\\n### Constructor\\n\\nThe constructor initializes the `ReAct` class and sets up its attributes. It is specifically designed to compose the interleaved steps of Thought, Action, and Observation.\\n\\n\\n\\n```python\\nimport dsp\\nimport dspy\\nfrom ..primitives.program',\n",
       " ' dspy\\nfrom ..primitives.program import Module\\nfrom .predict import Predict\\n\\nclass ReAct(Module):\\n    def __init__(self, signature, max_iters=5, num_results=3, tools=None):\\n        ...\\n```\\n\\n**Parameters:**\\n- `signature` (_Any_): Signature of the predictive model.\\n- `max_iters` (_',\n",
       " '.\\n- `max_iters` (_int_, _optional_): Maximum number of iterations for the Thought-Action-Observation cycle. Defaults to `5`.\\n- `num_results` (_int_, _optional_): Number of results to retrieve in the action step. Defaults to `3`.\\n- `tools` (_List[dspy.Tool]_, _optional_): List of tools available for actions. If none is provided,',\n",
       " ' tools available for actions. If none is provided, a default `Retrieve` tool with `num_results` is used.\\n\\n### Methods\\n\\n#### `_generate_signature(self, iters)`\\n\\nGenerates a signature for the Thought-Action-Observation cycle based on the number of iterations.\\n\\n**Parameters:**\\n- `iters` (_int_): Number of iterations.\\n\\n**Returns:**\\n- A dictionary representation',\n",
       " '\\n**Returns:**\\n- A dictionary representation of the signature.\\n\\n***\\n\\n#### `act(self, output, hop)`\\n\\nProcesses an action and returns the observation or final answer.\\n\\n**Parameters:**\\n- `output` (_dict_): Current output from the Thought.\\n- `hop` (_int_): Current iteration number.\\n\\n**Returns:**\\n- A string representing the final answer or `None`.\\n\\n***\\n',\n",
       " ' final answer or `None`.\\n\\n***\\n\\n#### `forward(self, **kwargs)`\\n\\nMain method to execute the Thought-Action-Observation cycle for a given set of input fields.\\n\\n**Parameters:**\\n- `**kwargs`: Keyword arguments corresponding to input fields.\\n\\n**Returns:**\\n- A `dspy.Prediction` object containing the result of the ReAct process.\\n\\n### Examples\\n\\n``',\n",
       " 'Act process.\\n\\n### Examples\\n\\n```python\\n# Define a simple signature for basic question answering\\nclass BasicQA(dspy.Signature):\\n    \"\"\"Answer questions with short factoid answers.\"\"\"\\n    question = dspy.InputField()\\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\\n\\n# Pass signature to ReAct module\\nreact_module = d',\n",
       " ' to ReAct module\\nreact_module = dspy.ReAct(BasicQA)\\n\\n# Call the ReAct module on a particular input\\nquestion = \\'What is the color of the sky?\\'\\nresult = react_module(question=question)\\n\\nprint(f\"Question: {question}\")\\nprint(f\"Final Predicted Answer (after ReAct process): {result.answer}\")\\n```---\\nsidebar_position: 1\\n',\n",
       " '`---\\nsidebar_position: 1\\n---\\n\\n# dspy.TypedPredictor\\n\\nThe `TypedPredictor` class is a sophisticated module designed for making predictions with strict type validations. It leverages a signature to enforce type constraints on inputs and outputs, ensuring that the data follows to the expected schema.\\n\\n### Constructor\\n\\n```python\\nTypedPredictor(\\n    CodeSignature\\n   ',\n",
       " '    CodeSignature\\n    max_retries=3\\n)\\n```\\n\\nParameters:\\n* `signature` (dspy.Signature): The signature that defines the input and output fields along with their types.\\n* `max_retries` (int, optional): The maximum number of retries for generating a valid prediction output. Defaults to 3.\\n\\n### Methods\\n\\n#### `copy() -> \"TypedPredict',\n",
       " '#### `copy() -> \"TypedPredictor\"`\\n\\nCreates and returns a deep copy of the current TypedPredictor instance.\\n\\n**Returns:** A new instance of TypedPredictor that is a deep copy of the original instance.\\n\\n#### `_make_example(type_: Type) -> str`\\n\\nA static method that generates a JSON object example based pn the schema of the specified Pydantic model type. This',\n",
       " \" of the specified Pydantic model type. This JSON object serves as an example for the expected input or output format.\\n\\n**Parameters:**\\n* `type_`: A Pydantic model class for which an example JSON object is to be generated.\\n\\n**Returns:** A string that represents a JSON object example, which validates against the provided Pydantic model's JSON schema. If the method is unable to generate a valid example, it returns an empty string.\",\n",
       " \" a valid example, it returns an empty string.\\n\\n#### `_prepare_signature() -> dspy.Signature`\\n\\nPrepares and returns a modified version of the signature associated with the TypedPredictor instance. This method iterates over the signature's fields to add format and parser functions based on their type annotations.\\n\\n**Returns:** A dspy.Signature object that has been enhanced with formatting and parsing specifications for its fields.\\n\",\n",
       " ' with formatting and parsing specifications for its fields.\\n\\n#### `forward(**kwargs) -> dspy.Prediction`\\n\\nExecutes the prediction logic, making use of the `dspy.Predict` component to generate predictions based on the input arguments. This method handles type validation, parsing of output data, and implements retry logic in case the output does not initially follow to the specified output schema.\\n\\n**Parameters:**\\n\\n* `**kwargs',\n",
       " 'Parameters:**\\n\\n* `**kwargs`: Keyword arguments corresponding to the input fields defined in the signature.\\n\\n**Returns:** A dspy.Prediction object containing the prediction results. Each key in this object corresponds to an output field defined in the signature, and its value is the parsed result of the prediction.\\n\\n### Example\\n\\n```python\\nfrom dspy import InputField, OutputField, Signature\\nfrom dspy.functional import',\n",
       " \", Signature\\nfrom dspy.functional import TypedPredictor\\nfrom pydantic import BaseModel\\n\\n# We define a pydantic type that automatically checks if it's argument is valid python code.\\nclass CodeOutput(BaseModel):\\n    code: str\\n    api_reference: str\\n\\nclass CodeSignature(Signature):\\n    function_description: str = InputField()\\n    solution: CodeOutput\",\n",
       " 'Field()\\n    solution: CodeOutput = OutputField()\\n\\ncot_predictor = TypedPredictor(CodeSignature)\\nprediction = cot_predictor(\\n    function_description=\"Write a function that adds two numbers.\"\\n)\\n\\nprint(prediction[\"code\"])\\nprint(prediction[\"api_reference\"])\\n```---\\nsidebar_position: 2\\n---\\n\\n# dsp',\n",
       " 'position: 2\\n---\\n\\n# dspy.TypedChainOfThought\\n\\n### Overview\\n\\n#### `def TypedChainOfThought(signature, max_retries=3) -> dspy.Module`\\n\\nAdds a Chain of Thoughts `dspy.OutputField` to the `dspy.TypedPredictor` module by prepending it to the Signature. Similar to `dspy.TypedPredictor',\n",
       " ' `dspy.TypedPredictor` but automatically adds a \"reasoning\" output field.\\n\\n* **Inputs**:\\n    * `signature`: The `dspy.Signature` specifying the input/output fields\\n    * `max_retries`: Maximum number of retries if outputs fail validation\\n* **Output**: A dspy.Module instance capable of making predictions.\\n\\n### Example\\n\\n',\n",
       " \" of making predictions.\\n\\n### Example\\n\\n```python\\nfrom dspy import InputField, OutputField, Signature\\nfrom dspy.functional import TypedChainOfThought\\nfrom pydantic import BaseModel\\n\\n# We define a pydantic type that automatically checks if it's argument is valid python code.\\nclass CodeOutput(BaseModel):\\n    code: str\\n    api_reference: str\\n\\nclass CodeSignature\",\n",
       " '_reference: str\\n\\nclass CodeSignature(Signature):\\n    function_description: str = InputField()\\n    solution: CodeOutput = OutputField()\\n\\ncot_predictor = TypedChainOfThought(CodeSignature)\\nprediction = cot_predictor(\\n    function_description=\"Write a function that adds two numbers.\"\\n)\\n\\nprint(prediction[\"code\"])\\nprint(',\n",
       " '(prediction[\"code\"])\\nprint(prediction[\"api_reference\"])\\n```---\\nsidebar_position: 3\\n---\\n\\n# dspy.predictor\\n\\n### Overview\\n\\n#### `def predictor(func) -> dspy.Module`\\n\\nThe `@predictor` decorator is used to create a predictor module based on the provided function. It automatically generates a `dspy.TypedPredictor`',\n",
       " 'dspy.TypedPredictor` and from the function\\'s type annotations and docstring.\\n\\n* **Input**: Function with input parameters and return type annotation.\\n* **Output**: A dspy.Module instance capable of making predictions.\\n\\n### Example\\n\\n```python\\nimport dspy\\n\\ncontext = [\"Roses are red.\", \"Violets are blue\"]\\nquestion = \"What color are roses?\"\\n\\n@dsp',\n",
       " 'What color are roses?\"\\n\\n@dspy.predictor\\ndef generate_answer(self, context: list[str], question) -> str:\\n    \"\"\"Answer questions with short factoid answers.\"\"\"\\n    pass\\n\\ngenerate_answer(context=context, question=question)\\n```---\\nsidebar_position: 4\\n---\\n\\n# dspy.cot\\n\\n### Overview\\n\\n#### `def cot',\n",
       " '\\n### Overview\\n\\n#### `def cot(func) -> dspy.Module`\\n\\nThe `@cot` decorator is used to create a Chain of Thoughts module based on the provided function. It automatically generates a `dspy.TypedPredictor` and from the function\\'s type annotations and docstring. Similar to predictor, but adds a \"Reasoning\" output field to capture the model\\'s step-by-step thinking.\\n\\n* **Input',\n",
       " 'by-step thinking.\\n\\n* **Input**: Function with input parameters and return type annotation.\\n* **Output**: A dspy.Module instance capable of making predictions.\\n\\n### Example\\n\\n```python\\nimport dspy\\n\\ncontext = [\"Roses are red.\", \"Violets are blue\"]\\nquestion = \"What color are roses?\"\\n\\n@dspy.cot\\ndef generate_answer(self, context: list[str',\n",
       " '_answer(self, context: list[str], question) -> str:\\n    \"\"\"Answer questions with short factoid answers.\"\"\"\\n    pass\\n\\ngenerate_answer(context=context, question=question)\\n```---\\nsidebar_position: 2\\n---\\n\\n# teleprompt.BootstrapFewShot\\n\\n### Constructor\\n\\nThe constructor initializes the `BootstrapFewShot` class and sets up parameters for bootstra',\n",
       " 'Shot` class and sets up parameters for bootstrapping.\\n\\n```python\\nclass BootstrapFewShot(Teleprompter):\\n    def __init__(self, metric=None, teacher_settings={}, max_bootstrapped_demos=4, max_labeled_demos=16, max_rounds=1):\\n        self.metric = metric\\n        self.',\n",
       " '\\n        self.teacher_settings = teacher_settings\\n\\n        self.max_bootstrapped_demos = max_bootstrapped_demos\\n        self.max_labeled_demos = max_labeled_demos\\n        self.max_rounds = max_rounds\\n```\\n\\n**Parameters:**\\n-',\n",
       " '```\\n\\n**Parameters:**\\n- `metric` (_callable_, _optional_): Metric function to evaluate examples during bootstrapping. Defaults to `None`.\\n- `teacher_settings` (_dict_, _optional_): Settings for teacher predictor. Defaults to empty dictionary.\\n- `max_bootstrapped_demos` (_int_, _optional_): Maximum number of bootstrapped demonstrations per predictor. Defaults to',\n",
       " ' bootstrapped demonstrations per predictor. Defaults to 4.\\n- `max_labeled_demos` (_int_, _optional_): Maximum number of labeled demonstrations per predictor. Defaults to 16.\\n- `max_rounds` (_int_, _optional_): Maximum number of bootstrapping rounds. Defaults to 1.\\n\\n### Method\\n\\n#### `compile(self, student, *, teacher=None, trainset, valset',\n",
       " \" teacher=None, trainset, valset=None)`\\n\\nThis method compiles the BootstrapFewShot instance by performing bootstrapping to refine the student predictor.\\n\\nThis process includes preparing the student and teacher predictors, which involves creating predictor copies, verifying the student predictor is uncompiled, and compiling the teacher predictor with labeled demonstrations via LabeledFewShot if the teacher predictor hasn't been compiled.\\n\\nThe next stage involves preparing predictor mappings by validating that\",\n",
       " ' stage involves preparing predictor mappings by validating that both the student and teacher predictors have the same program structure and the same signatures but are different objects.\\n\\nThe final stage is performing the bootstrapping iterations.\\n\\n**Parameters:**\\n- `student` (_Teleprompter_): Student predictor to be compiled.\\n- `teacher` (_Teleprompter_, _optional_): Teacher predictor used for bootstrapping. Defaults to `None`.\\n- `',\n",
       " '. Defaults to `None`.\\n- `trainset` (_list_): Training dataset used in bootstrapping.\\n- `valset` (_list_, _optional_): Validation dataset used in compilation. Defaults to `None`.\\n\\n**Returns:**\\n- The compiled `student` predictor after bootstrapping with refined demonstrations.\\n\\n### Example\\n\\n```python\\n#Assume defined trainset\\n#Assume defined RAG class\\n',\n",
       " 'et\\n#Assume defined RAG class\\n...\\n\\n#Define teleprompter and include teacher\\nteacher = dspy.OpenAI(model=\\'gpt-3.5-turbo\\', api_key = openai.api_key, api_provider = \"openai\", model_type = \"chat\")\\nteleprompter = BootstrapFewShot(teacher_settings=dict({\\'lm\\': teacher}))\\n\\n# Comp',\n",
       " \"lm': teacher}))\\n\\n# Compile!\\ncompiled_rag = teleprompter.compile(student=RAG(), trainset=trainset)\\n```\\n---\\nsidebar_position: 1\\n---\\n\\n# teleprompt.LabeledFewShot\\n\\n### Constructor\\n\\nThe constructor initializes the `LabeledFewShot` class and sets up its attributes, particularly defining `k` number of samples to be used by the predictor\",\n",
       " '` number of samples to be used by the predictor.\\n\\n```python\\nclass LabeledFewShot(Teleprompter):\\n    def __init__(self, k=16):\\n        self.k = k\\n```\\n\\n**Parameters:**\\n- `k` (_int_): Number of samples to be used for each predictor. Defaults to 16.\\n\\n### Method\\n\\n#### `compile(self',\n",
       " \"### Method\\n\\n#### `compile(self, student, *, trainset)`\\n\\nThis method compiles the `LabeledFewShot` instance by configuring the `student` predictor. It assigns subsets of the `trainset` in each student's predictor's `demos` attribute. If the `trainset` is empty, the method returns the original `student`.\\n\\n**Parameters:**\\n- `student` (_Teleprompter_):\",\n",
       " '- `student` (_Teleprompter_): Student predictor to be compiled.\\n- `trainset` (_list_): Training dataset for compiling with student predictor.\\n\\n**Returns:**\\n- The compiled `student` predictor with assigned training samples for each predictor or the original `student` if the `trainset` is empty.\\n\\n### Example\\n\\n```python\\nimport dspy\\n\\n#Assume defined trainset\\nclass RAG(d',\n",
       " 'ume defined trainset\\nclass RAG(dspy.Module):\\n    def __init__(self, num_passages=3):\\n        super().__init__()\\n\\n        #declare retrieval and predictor modules\\n        self.retrieve = dspy.Retrieve(k=num_passages)\\n        self.gener',\n",
       " '        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\\n    \\n    #flow for answering questions using predictor and retrieval modules\\n    def forward(self, question):\\n        context = self.retrieve(question).passages\\n        prediction = self.generate_answer(context=context, question=question',\n",
       " '_answer(context=context, question=question)\\n        return dspy.Prediction(context=context, answer=prediction.answer)\\n\\n#Define teleprompter\\nteleprompter = LabeledFewShot()\\n\\n# Compile!\\ncompiled_rag = teleprompter.compile(student=RAG(), trainset=trainset)\\n```---\\nsidebar_position: 3\\n---\\n',\n",
       " '\\nsidebar_position: 3\\n---\\n\\n# teleprompt.Ensemble\\n\\n### Constructor\\n\\nThe constructor initializes the `Ensemble` class and sets up its attributes. This teleprompter is designed to create ensembled versions of multiple programs, reducing various outputs from different programs into a single output.\\n\\n```python\\nclass Ensemble(Teleprompter):\\n    def __init__(self, *, reduce_fn=None',\n",
       " '(self, *, reduce_fn=None, size=None, deterministic=False):\\n```\\n\\n**Parameters:**\\n- `reduce_fn` (_callable_, _optional_): Function used to reduce multiple outputs from different programs into a single output. A common choice is `dspy.majority`. Defaults to `None`.\\n- `size` (_int_, _optional_): Number of programs to randomly select for ensembling',\n",
       " ' Number of programs to randomly select for ensembling. If not specified, all programs will be used. Defaults to `None`.\\n- `deterministic` (_bool_, _optional_): Specifies whether ensemble should operate deterministically. Currently, setting this to `True` will raise an error as this feature is pending implementation. Defaults to `False`.\\n\\n### Method\\n\\n#### `compile(self, programs)`\\n\\nThis method compiles an',\n",
       " ' programs)`\\n\\nThis method compiles an ensemble of programs into a single program that when run, can either randomly sample a subset of the given programs to produce outputs or use all of them. The multiple outputs can then be reduced into a single output using the `reduce_fn`.\\n\\n**Parameters:**\\n- `programs` (_list_): List of programs to be ensembled.\\n\\n**Returns:**\\n- `EnsembledProgram` (_Module',\n",
       " '\\n- `EnsembledProgram` (_Module_): An ensembled version of the input programs.\\n\\n### Example\\n\\n```python\\nimport dspy\\nfrom dspy.teleprompt import Ensemble\\n\\n# Assume a list of programs\\nprograms = [program1, program2, program3, ...]\\n\\n# Define Ensemble teleprompter\\nteleprompter = Ensemble(reduce_fn=dspy.',\n",
       " '(reduce_fn=dspy.majority, size=2)\\n\\n# Compile to get the EnsembledProgram\\nensembled_program = teleprompter.compile(programs)\\n```---\\nsidebar_position: 4\\n---\\n\\n# teleprompt.BootstrapFewShotWithRandomSearch\\n\\n### Constructor\\n\\nThe constructor initializes the `BootstrapFewShotWithRandomSearch` class and sets up its attributes.',\n",
       " 'RandomSearch` class and sets up its attributes. It inherits from the `BootstrapFewShot` class and introduces additional attributes for the random search process.\\n\\n```python\\nclass BootstrapFewShotWithRandomSearch(BootstrapFewShot):\\n    def __init__(self, metric, teacher_settings={}, max_bootstrapped_demos=4, max_labeled_demos=16, max_rounds=1, num_',\n",
       " ', max_rounds=1, num_candidate_programs=16, num_threads=6):\\n        self.metric = metric\\n        self.teacher_settings = teacher_settings\\n        self.max_rounds = max_rounds\\n\\n        self.num_threads = num_threads\\n\\n    ',\n",
       " ' = num_threads\\n\\n        self.min_num_samples = 1\\n        self.max_num_samples = max_bootstrapped_demos\\n        self.num_candidate_sets = num_candidate_programs\\n        self.max_num_traces = 1 + int(max_bootstrapped_dem',\n",
       " ' + int(max_bootstrapped_demos / 2.0 * self.num_candidate_sets)\\n\\n        self.max_bootstrapped_demos = self.max_num_traces\\n        self.max_labeled_demos = max_labeled_demos\\n\\n        print(\"Going to sample between\", self.min_num_samples',\n",
       " ' between\", self.min_num_samples, \"and\", self.max_num_samples, \"traces per predictor.\")\\n        print(\"Going to sample\", self.max_num_traces, \"traces in total.\")\\n        print(\"Will attempt to train\", self.num_candidate_sets, \"candidate sets.\")\\n```\\n\\n**Parameters:**\\n- `met',\n",
       " '\\n\\n**Parameters:**\\n- `metric` (_callable_, _optional_): Metric function to evaluate examples during bootstrapping. Defaults to `None`.\\n- `teacher_settings` (_dict_, _optional_): Settings for teacher predictor. Defaults to empty dictionary.\\n- `max_bootstrapped_demos` (_int_, _optional_): Maximum number of bootstrapped demonstrations per predictor. Defaults to 4.',\n",
       " 'pped demonstrations per predictor. Defaults to 4.\\n- `max_labeled_demos` (_int_, _optional_): Maximum number of labeled demonstrations per predictor. Defaults to 16.\\n- `max_rounds` (_int_, _optional_): Maximum number of bootstrapping rounds. Defaults to 1.\\n- `num_candidate_programs` (_int_): Number of candidate programs to generate during random search.\\n- `num',\n",
       " ' to generate during random search.\\n- `num_threads` (_int_): Number of threads used for evaluation during random search.\\n\\n### Method\\n\\nRefer to [teleprompt.BootstrapFewShot](https://dspy-docs.vercel.app/docs/deep-dive/teleprompter/bootstrap-fewshot) documentation.\\n\\n## Example\\n\\n```python\\n#Assume defined trainset\\n#Assume defined R',\n",
       " 'ume defined trainset\\n#Assume defined RAG class\\n...\\n\\n#Define teleprompter and include teacher\\nteacher = dspy.OpenAI(model=\\'gpt-3.5-turbo\\', api_key = openai.api_key, api_provider = \"openai\", model_type = \"chat\")\\nteleprompter = BootstrapFewShotWithRandomSearch(teacher_settings=dict({\\'lm\\': teacher',\n",
       " \"_settings=dict({'lm': teacher}))\\n\\n# Compile!\\ncompiled_rag = teleprompter.compile(student=RAG(), trainset=trainset)\\n```---\\nsidebar_position: 5\\n---\\n\\n# teleprompt.BootstrapFinetune\\n\\n### Constructor\\n\\n### `__init__(self, metric=None, teacher_settings={}, multitask=True)`\\n\\nThe\",\n",
       " '}, multitask=True)`\\n\\nThe constructor initializes a `BootstrapFinetune` instance and sets up its attributes. It defines the teleprompter as a `BootstrapFewShot` instance for the finetuning compilation.\\n\\n```python\\nclass BootstrapFinetune(Teleprompter):\\n    def __init__(self, metric=None, teacher_settings={}, multitask=True):\\n```\\n\\n**Parameters:',\n",
       " 'True):\\n```\\n\\n**Parameters:**\\n- `metric` (_callable_, _optional_): Metric function to evaluate examples during bootstrapping. Defaults to `None`.\\n- `teacher_settings` (_dict_, _optional_): Settings for teacher predictor. Defaults to empty dictionary.\\n- `multitask` (_bool_, _optional_): Enable multitask fine-tuning. Defaults to `True`.\\n',\n",
       " \"tuning. Defaults to `True`.\\n\\n### Method\\n\\n#### `compile(self, student, *, teacher=None, trainset, valset=None, target='t5-large', bsize=12, accumsteps=1, lr=5e-5, epochs=1, bf16=False)`\\n\\nThis method first compiles for bootstrapping with the `BootstrapFewShot` teleprompter. It\",\n",
       " 'BootstrapFewShot` teleprompter. It then prepares fine-tuning data by generating prompt-completion pairs for training and performs finetuning. After compilation, the LMs are set to the finetuned models and the method returns a compiled and fine-tuned predictor.\\n\\n**Parameters:**\\n- `student` (_Predict_): Student predictor to be fine-tuned.\\n- `teacher` (_Predict_, _optional_): Teacher',\n",
       " \" (_Predict_, _optional_): Teacher predictor to help with fine-tuning. Defaults to `None`.\\n- `trainset` (_list_): Training dataset for fine-tuning.\\n- `valset` (_list_, _optional_): Validation dataset for fine-tuning. Defaults to `None`.\\n- `target` (_str_, _optional_): Target model for fine-tuning. Defaults to `'t\",\n",
       " \"-tuning. Defaults to `'t5-large'`.\\n- `bsize` (_int_, _optional_): Batch size for training. Defaults to `12`.\\n- `accumsteps` (_int_, _optional_): Gradient accumulation steps. Defaults to `1`.\\n- `lr` (_float_, _optional_): Learning rate for fine-tuning. Defaults to `5e-5`.\\n- `\",\n",
       " ' to `5e-5`.\\n- `epochs` (_int_, _optional_): Number of training epochs. Defaults to `1`.\\n- `bf16` (_bool_, _optional_): Enable mixed-precision training with BF16. Defaults to `False`.\\n\\n**Returns:**\\n- `compiled2` (_Predict_): A compiled and fine-tuned `Predict` instance.\\n\\n### Example\\n',\n",
       " \"Predict` instance.\\n\\n### Example\\n\\n```python\\n#Assume defined trainset\\n#Assume defined RAG class\\n...\\n\\n#Define teleprompter\\nteleprompter = BootstrapFinetune(teacher_settings=dict({'lm': teacher}))\\n\\n# Compile!\\ncompiled_rag = teleprompter.compile(student=RAG(), trainset=trainset, target='google/flan\",\n",
       " \"trainset, target='google/flan-t5-base')\\n```---\\ntitle: Markdown page example\\n---\\n\\n# Markdown page example\\n\\nYou don't need React to write simple standalone pages.\\n---\\nsidebar_position: 999\\n---\\n\\n\\n# DSPy Cheatsheet\\n\\nThis page will contain snippets for frequent usage patterns.\\n\\n## DSPy DataLoaders\\n\\nImport and initializing a DataLoader Object\",\n",
       " '\\n\\nImport and initializing a DataLoader Object:\\n\\n```python\\nimport dspy\\nfrom dspy.datasets import DataLoader\\n\\ndl = DataLoader()\\n```\\n\\n### Loading from HuggingFace Datasets\\n\\n```python\\ncode_alpaca = dl.from_huggingface(\"HuggingFaceH4/CodeAlpaca_20K\")\\n```\\n\\nYou can access the dataset',\n",
       " \"\\n```\\n\\nYou can access the dataset of the splits by calling key of the corresponding split:\\n\\n```python\\ntrain_dataset = code_alpaca['train']\\ntest_dataset = code_alpaca['test']\\n```\\n\\n### Loading specific splits from HuggingFace\\n\\nYou can also manually specify splits you want to include as a parameters and it'll return a dictionary where keys are splits that you specified:\\n\\n\",\n",
       " ' where keys are splits that you specified:\\n\\n```python\\ncode_alpaca = dl.from_huggingface(\\n    \"HuggingFaceH4/CodeAlpaca_20K\",\\n    split = [\"train\", \"test\"],\\n)\\n\\nprint(f\"Splits in dataset: {code_alpaca.keys()}\")\\n```\\n\\nIf you specify a single split then dataload',\n",
       " 'If you specify a single split then dataloader will return a List of `dspy.Example` instead of dictionary:\\n\\n```python\\ncode_alpaca = dl.from_huggingface(\\n    \"HuggingFaceH4/CodeAlpaca_20K\",\\n    split = \"train\",\\n)\\n\\nprint(f\"Number of examples in split: {len(code_alpaca)}',\n",
       " ': {len(code_alpaca)}\")\\n```\\n\\nYou can slice the split just like you do with HuggingFace Dataset too:\\n\\n```python\\ncode_alpaca_80 = dl.from_huggingface(\\n    \"HuggingFaceH4/CodeAlpaca_20K\",\\n    split = \"train[:80%]\",\\n)\\n\\nprint(f\"Number of',\n",
       " '\\n)\\n\\nprint(f\"Number of examples in split: {len(code_alpaca_80)}\")\\n\\ncode_alpaca_20_80 = dl.from_huggingface(\\n    \"HuggingFaceH4/CodeAlpaca_20K\",\\n    split = \"train[20%:80%]\",\\n)\\n\\nprint(f\"Number of examples in split: {len(code_',\n",
       " ' of examples in split: {len(code_alpaca_20_80)}\")\\n```\\n\\n### Loading specific subset from HuggingFace\\n\\nIf a dataset has a subset you can pass it as an arg like you do with `load_dataset` in HuggingFace:\\n\\n```python\\ngms8k = dl.from_huggingface(\\n    \"gsm8k\",\\n    \"main',\n",
       " 'sm8k\",\\n    \"main\",\\n    input_keys = (\"question\",),\\n)\\n\\nprint(f\"Keys present in the returned dict: {list(gms8k.keys())}\")\\n\\nprint(f\"Number of examples in train set: {len(gms8k[\\'train\\'])}\")\\nprint(f\"Number of examples in test set: {len(gms8k[\\'test\\'])}',\n",
       " '(gms8k[\\'test\\'])}\")\\n```\\n\\n### Loading from CSV\\n\\n```python\\ndolly_100_dataset = dl.from_csv(\"dolly_subset_100_rows.csv\",)\\n```\\n\\nYou can choose only selected columns from the csv by specifying them in the arguments:\\n\\n```python\\ndolly_100_dataset = dl.from_csv(\\n',\n",
       " 'et = dl.from_csv(\\n    \"dolly_subset_100_rows.csv\",\\n    fields=(\"instruction\", \"context\", \"response\"),\\n    input_keys=(\"instruction\", \"context\")\\n)\\n```\\n\\n### Splitting a List of `dspy.Example`\\n\\n```python\\nsplits = dl.train_test_split(dataset, train',\n",
       " \"_test_split(dataset, train_size=0.8) # `dataset` is a List of dspy.Example\\ntrain_dataset = splits['train']\\ntest_dataset = splits['test']\\n```\\n\\n### Sampling from List of `dspy.Example`\\n\\n```python\\nsampled_example = dl.sample(dataset, n=5) # `datas\",\n",
       " 'et, n=5) # `dataset` is a List of dspy.Example\\n```\\n\\n## DSPy Programs\\n\\n### dspy.Signature\\n\\n```python\\nclass BasicQA(dspy.Signature):\\n    \"\"\"Answer questions with short factoid answers.\"\"\"\\n\\n    question = dspy.InputField()\\n    answer = dspy.OutputField(desc=\"',\n",
       " ' = dspy.OutputField(desc=\"often between 1 and 5 words\")\\n```\\n\\n### dspy.ChainOfThought\\n\\n```python\\ngenerate_answer = dspy.ChainOfThought(BasicQA)\\n\\n# Call the predictor on a particular input alongside a hint.\\nquestion=\\'What is the color of the sky?\\'\\npred = generate_answer(question=question)\\n```\\n\\n### dspy',\n",
       " ')\\n```\\n\\n### dspy.ChainOfThoughtwithHint\\n\\n```python\\ngenerate_answer = dspy.ChainOfThoughtWithHint(BasicQA)\\n\\n# Call the predictor on a particular input alongside a hint.\\nquestion=\\'What is the color of the sky?\\'\\nhint = \"It\\'s what you often see during a sunny day.\"\\npred = generate_answer(question=question, hint=h',\n",
       " '_answer(question=question, hint=hint)\\n```\\n\\n### dspy.ProgramOfThought\\n\\n```python\\npot = dspy.ProgramOfThought(BasicQA)\\n\\nquestion = \\'Sarah has 5 apples. She buys 7 more apples from the store. How many apples does Sarah have now?\\'\\nresult = pot(question=question)\\n\\nprint(f\"Question: {question}\")\\nprint(f\"',\n",
       " ': {question}\")\\nprint(f\"Final Predicted Answer (after ProgramOfThought process): {result.answer}\")\\n```\\n\\n### dspy.ReACT\\n\\n```python\\nreact_module = dspy.ReAct(BasicQA)\\n\\nquestion = \\'Sarah has 5 apples. She buys 7 more apples from the store. How many apples does Sarah have now?\\'\\nresult = react_module(question=question)\\n',\n",
       " ' = react_module(question=question)\\n\\nprint(f\"Question: {question}\")\\nprint(f\"Final Predicted Answer (after ReAct process): {result.answer}\")\\n```\\n\\n### dspy.Retreive\\n\\n```python\\ncolbertv2_wiki17_abstracts = dspy.ColBERTv2(url=\\'http://20.102.90.50:2017/wiki17_',\n",
       " \".90.50:2017/wiki17_abstracts')\\ndspy.settings.configure(rm=colbertv2_wiki17_abstracts)\\n\\n#Define Retrieve Module\\nretriever = dspy.Retrieve(k=3)\\n\\nquery='When was the first FIFA World Cup held?'\\n\\n# Call the retriever on a particular query.\\ntopK_passages = retriever(query).pass\",\n",
       " \"_passages = retriever(query).passages\\n\\nfor idx, passage in enumerate(topK_passages):\\n    print(f'{idx+1}]', passage, '\\\\n')\\n```\\n\\n## DSPy Metrics\\n\\n### Function as Metric\\n\\nTo create a custom metric you can create a function that returns either a number or a boolean value:\\n\\n```python\\ndef parse_integer\",\n",
       " \"\\n\\n```python\\ndef parse_integer_answer(answer, only_first_line=True):\\n    try:\\n        if only_first_line:\\n            answer = answer.strip().split('\\\\n')[0]\\n\\n        # find the last token that has a number in it\\n        answer = [\",\n",
       " \"        answer = [token for token in answer.split() if any(c.isdigit() for c in token)][-1]\\n        answer = answer.split('.')[0]\\n        answer = ''.join([c for c in answer if c.isdigit()])\\n        answer = int(answer)\\n\\n    except\",\n",
       " ' = int(answer)\\n\\n    except (ValueError, IndexError):\\n        # print(answer)\\n        answer = 0\\n    \\n    return answer\\n\\n# Metric Function\\ndef gsm8k_metric(gold, pred, trace=None) -> int:\\n    return int(parse_integer_answer(str(gold.answer))) == int',\n",
       " 'answer(str(gold.answer))) == int(parse_integer_answer(str(pred.answer)))\\n```\\n\\n### LLM as Judge\\n\\n```python\\nclass FactJudge(dspy.Signature):\\n    \"\"\"Judge if the answer is factually correct based on the context.\"\"\"\\n\\n    context = dspy.InputField(desc=\"Context for the prediciton\")\\n    question = dspy',\n",
       " '\")\\n    question = dspy.InputField(desc=\"Question to be answered\")\\n    answer = dspy.InputField(desc=\"Answer for the question\")\\n    factually_correct = dspy.OutputField(desc=\"Is the answer factually correct based on the context?\", prefix=\"Facual[Yes/No]:\")\\n\\njudge = dspy.ChainOfThought(FactJudge)\\n\\ndef',\n",
       " 'OfThought(FactJudge)\\n\\ndef factuality_metric(example, pred):\\n    factual = judge(context=example.context, question=example.question, answer=pred.answer)\\n    return int(factual==\"Yes\")\\n```\\n\\n## DSPy Evaluation\\n\\n```python\\nfrom dspy.evaluate import Evaluate\\n\\nevaluate_program = Evaluate(devset=devset, metric',\n",
       " ' Evaluate(devset=devset, metric=your_defined_metric, num_threads=NUM_THREADS, display_progress=True, display_table=num_rows_to_display)\\n\\nevaluate_program(your_dspy_program)\\n```\\n\\n## DSPy Optimizers\\n\\n### dspy.LabeledFewShot \\n```python\\nfrom dspy.teleprompt import Labeled',\n",
       " ' dspy.teleprompt import LabeledFewShot\\n\\nlabeled_fewshot_optimizer = dspy.LabeledFewShot(k=8)\\nyour_dspy_program_compiled = labeled_fewshot_optimizer.compile(student = your_dspy_program, trainset=trainset)\\n```\\n\\n### dspy.BootstrapFewShot \\n```python\\nfrom dspy',\n",
       " ' \\n```python\\nfrom dspy.teleprompt import BootstrapFewShot\\n\\nfewshot_optimizer = BootstrapFewShot(metric=your_defined_metric, max_bootstrapped_demos=4, max_labeled_demos=16, max_rounds=1, max_errors=5)\\n\\nyour_dspy_program_compiled = fewshot_optimizer.compile(student =',\n",
       " 'shot_optimizer.compile(student = your_dspy_program, trainset=trainset)\\n```\\n\\n#### Using another LM for compilation, specifying in teacher_settings\\n```python\\nfrom dspy.teleprompt import BootstrapFewShot\\n\\nfewshot_optimizer = BootstrapFewShot(metric=your_defined_metric, max_bootstrapped_demos=4, max_labeled_dem',\n",
       " 'os=4, max_labeled_demos=16, max_rounds=1, max_errors=5, teacher_settings=dict(lm=gpt4))\\n\\nyour_dspy_program_compiled = fewshot_optimizer.compile(student = your_dspy_program, trainset=trainset)\\n```\\n\\n#### Compiling a compiled program - bootstrapping a bootstraped program\\n',\n",
       " \" - bootstrapping a bootstraped program\\n\\n```python\\nyour_dspy_program_compiledx2 = teleprompter.compile(\\n    your_dspy_program,\\n    teacher=your_dspy_program_compiled,\\n    trainset=trainset,\\n)\\n```\\n\\n#### Saving/loading a compiled program\\n\\n```python\\nsave_path = '\",\n",
       " \"\\n```python\\nsave_path = './v1.json'\\nyour_dspy_program_compiledx2.save(save_path)\\n```\\n\\n```python\\nloaded_program = YourProgramClass()\\nloaded_program.load(path=save_path)\\n```\\n\\n### dspy.BootstrapFewShotWithRandomSearch\\n\\n```python\\nfrom dspy.teleprompt import Bootstrap\",\n",
       " ' dspy.teleprompt import BootstrapFewShotWithRandomSearch\\n\\nfewshot_optimizer = BootstrapFewShotWithRandomSearch(metric=your_defined_metric, max_bootstrapped_demos=2, num_candidate_programs=8, num_threads=NUM_THREADS)\\n\\nyour_dspy_program_compiled = fewshot_optimizer.compile(student = your_d',\n",
       " 'izer.compile(student = your_dspy_program, trainset=trainset, valset=devset)\\n\\n```\\nOther custom configurations are similar to customizing the `dspy.BootstrapFewShot` optimizer. \\n\\n\\n### dspy.Ensemble\\n\\n```python\\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch\\nfrom dspy.teleprompt.ensemble',\n",
       " ' dspy.teleprompt.ensemble import Ensemble\\n\\nfewshot_optimizer = BootstrapFewShotWithRandomSearch(metric=your_defined_metric, max_bootstrapped_demos=2, num_candidate_programs=8, num_threads=NUM_THREADS)\\nyour_dspy_program_compiled = fewshot_optimizer.compile(student = your_dspy_',\n",
       " 'ile(student = your_dspy_program, trainset=trainset, valset=devset)\\n\\nensemble_optimizer = Ensemble(reduce_fn=dspy.majority)\\nprograms = [x[-1] for x in your_dspy_program_compiled.candidate_programs]\\nyour_dspy_program_compiled_ensemble = ensemble_optimizer.compile',\n",
       " 'ensemble = ensemble_optimizer.compile(programs[:3])\\n```\\n\\n### dspy.BootstrapFinetune\\n\\n```python\\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch, BootstrapFinetune\\n\\n#Compile program on current dspy.settings.lm\\nfewshot_optimizer = BootstrapFewShotWithRandomSearch(metric=your_defined_metric',\n",
       " '(metric=your_defined_metric, max_bootstrapped_demos=2, num_threads=NUM_THREADS)\\nyour_dspy_program_compiled = tp.compile(your_dspy_program, trainset=trainset[:some_num], valset=trainset[some_num:])\\n\\n#Configure model to finetune\\nconfig = dict(target',\n",
       " ' to finetune\\nconfig = dict(target=model_to_finetune, epochs=2, bf16=True, bsize=6, accumsteps=2, lr=5e-5)\\n\\n#Compile program on BootstrapFinetune\\nfinetune_optimizer = BootstrapFinetune(metric=your_defined_metric)\\nfinetune_program = finetune_optimizer.compile',\n",
       " ' = finetune_optimizer.compile(your_dspy_program, trainset=some_new_dataset_for_finetuning_model, **config)\\n\\nfinetune_program = your_dspy_program\\n\\n#Load program and activate model\\'s parameters in program before evaluation\\nckpt_path = \"saved_checkpoint_path_from_finetuning\"\\nLM = dspy.HFModel',\n",
       " '\"\\nLM = dspy.HFModel(checkpoint=ckpt_path, model=model_to_finetune)\\n\\nfor p in finetune_program.predictors():\\n    p.lm = LM\\n    p.activated = False\\n```\\n\\n### dspy.SignatureOptimizer\\n\\n```python\\nfrom dspy.teleprompt import SignatureOptimizer\\n\\neval',\n",
       " 'prompt import SignatureOptimizer\\n\\neval_kwargs = dict(num_threads=16, display_progress=True, display_table=0)\\n\\nsignature_optimizer_teleprompter = SignatureOptimizer(prompt_model=model_to_generate_prompts, task_model=model_that_solves_task, metric=your_defined_metric, breadth=num_new_prompts_',\n",
       " ' breadth=num_new_prompts_generated, depth=times_to_generate_prompts, init_temperature=prompt_generation_temperature, verbose=False, log_dir=logging_directory)\\n\\ncompiled_program_optimized_signature = signature_optimizer_teleprompter.compile(your_dspy_program.deepcopy(), devset=trainset, evalset=dev',\n",
       " '=trainset, evalset=devset, eval_kwargs=eval_kwargs)\\n```\\n\\n### dspy.BayesianSignatureOptimizer\\n\\n\\n```python\\nfrom dspy.teleprompt import BayesianSignatureOptimizer\\n\\nteleprompter = BayesianSignatureOptimizer(prompt_model=model_to_generate_prompts, task_model=model_that_s',\n",
       " ', task_model=model_that_solves_task, metric=your_defined_metric, n=num_new_prompts_generated, init_temperature=prompt_generation_temperature)\\n\\nkwargs = dict(num_threads=NUM_THREADS, display_progress=True, display_table=0)\\n\\ncompiled_program_optimized_bayesian_signature = teleprompter.compile',\n",
       " '_signature = teleprompter.compile(your_dspy_program, devset=devset[:DEV_NUM], optuna_trials_num=100, max_bootstrapped_demos=3, max_labeled_demos=5, eval_kwargs=kwargs)\\n```\\n\\n### Signature Optimizer with Types\\n\\n```python\\nfrom dspy.teleprompt.signature_opt_',\n",
       " '.teleprompt.signature_opt_typed import optimize_signature\\nfrom dspy.evaluate.metrics import answer_exact_match\\nfrom dspy.functional import TypedChainOfThought\\n\\ncompiled_program = optimize_signature(\\n    student=TypedChainOfThought(\"question -> answer\"),\\n    evaluator=Evaluate(devset=devset, metric=answer_ex',\n",
       " 'set=devset, metric=answer_exact_match, num_threads=10, display_progress=True),\\n    n_iterations=50,\\n).program\\n```\\n\\n### dspy.KNNFewShot\\n\\n```python\\nfrom dspy.predict import KNN\\nfrom dspy.teleprompt import KNNFewShot\\n\\nknn_optimizer = KNNFewShot(KNN',\n",
       " 'optimizer = KNNFewShot(KNN, k=3, trainset=trainset)\\n\\nyour_dspy_program_compiled = knn_optimizer.compile(student=your_dspy_program, trainset=trainset, valset=devset)\\n```\\n\\n### dspy.BootstrapFewShotWithOptuna\\n\\n```python\\nfrom dspy.teleprompt import',\n",
       " '\\nfrom dspy.teleprompt import BootstrapFewShotWithOptuna\\n\\nfewshot_optuna_optimizer = BootstrapFewShotWithOptuna(metric=your_defined_metric, max_bootstrapped_demos=2, num_candidate_programs=8, num_threads=NUM_THREADS)\\n\\nyour_dspy_program_compiled = fewshot_optuna_optimizer.',\n",
       " ' = fewshot_optuna_optimizer.compile(student=your_dspy_program, trainset=trainset, valset=devset)\\n```\\nOther custom configurations are similar to customizing the `dspy.BootstrapFewShot` optimizer. \\n\\n\\n## DSPy Assertions\\n\\n### Including `dspy.Assert` and `dspy.Suggest` statements\\n```python\\nd',\n",
       " '.Suggest` statements\\n```python\\ndspy.Assert(your_validation_fn(model_outputs), \"your feedback message\", target_module=\"YourDSPyModuleSignature\")\\n\\ndspy.Suggest(your_validation_fn(model_outputs), \"your feedback message\", target_module=\"YourDSPyModuleSignature\")\\n```\\n\\n### Activating DSPy Program with Assertions \\n',\n",
       " ' DSPy Program with Assertions \\n\\n**Note**: To use Assertions properly, you must **activate** a DSPy program that includes `dspy.Assert` or `dspy.Suggest` statements from either of the methods above. \\n\\n```python\\n#1. Using `assert_transform_module:\\nfrom dspy.primitives.assertions import assert_transform_module, backtrack_handler\\n\\n',\n",
       " 'transform_module, backtrack_handler\\n\\nprogram_with_assertions = assert_transform_module(ProgramWithAssertions(), backtrack_handler)\\n\\n#2. Using `activate_assertions()`\\nprogram_with_assertions = ProgramWithAssertions().activate_assertions()\\n```\\n\\n## Compiling with DSPy Programs with Assertions\\n\\n```python\\nprogram_with_assertions = assert_transform_',\n",
       " '_with_assertions = assert_transform_module(ProgramWithAssertions(), backtrack_handler)\\nfewshot_optimizer = BootstrapFewShotWithRandomSearch(metric = your_defined_metric, max_bootstrapped_demos=2, num_candidate_programs=6)\\ncompiled_dspy_program_with_assertions = fewshot_optimizer.compile(student=program_with_assert',\n",
       " 'compile(student=program_with_assertions, teacher = program_with_assertions, trainset=trainset, valset=devset) #student can also be program_without_assertions\\n```\\n---\\nsidebar_position: 998\\n---\\n\\n# FAQs\\n\\n## Is DSPy right for me? DSPy vs. other frameworks\\n\\nThe **DSPy** philosophy and abstraction differ significantly from other libraries',\n",
       " \"y** philosophy and abstraction differ significantly from other libraries and frameworks, so it's usually straightforward to decide when **DSPy** is (or isn't) the right framework for your usecase. If you're a NLP/AI researcher (or a practitioner exploring new pipelines or new tasks), the answer is generally an invariable **yes**. If you're a practitioner doing other things, please read on.\\n\\n**DSPy vs. thin wrappers for prompts (OpenAI\",\n",
       " \" vs. thin wrappers for prompts (OpenAI API, MiniChain, basic templating)** In other words: _Why can't I just write my prompts directly as string templates?_ Well, for extremely simple settings, this _might_ work just fine. (If you're familiar with neural networks, this is like expressing a tiny two-layer NN as a Python for-loop. It kinda works.) However, when you need higher quality (or manageable cost), then you\",\n",
       " ' need higher quality (or manageable cost), then you need to iteratively explore multi-stage decomposition, improved prompting, data bootstrapping, careful finetuning, retrieval augmentation, and/or using smaller (or cheaper, or local) models. The true expressive power of building with foundation models lies in the interactions between these pieces. But every time you change one piece, you likely break (or weaken) multiple other components. **DSPy** cleanly abstracts away (_and',\n",
       " 'SPy** cleanly abstracts away (_and_ powerfully optimizes) the parts of these interactions that are external to your actual system design. It lets you focus on designing the module-level interactions: the _same program_ expressed in 10 or 20 lines of **DSPy** can easily be compiled into multi-stage instructions for `GPT-4`, detailed prompts for `Llama2-13b`, or finetunes for `T5-base`. Oh, and',\n",
       " \" for `T5-base`. Oh, and you wouldn't need to maintain long, brittle, model-specific strings at the core of your project anymore.\\n\\n**DSPy vs. application development libraries like LangChain, LlamaIndex** LangChain and LlamaIndex target high-level application development; they offer _batteries-included_, pre-built application modules that plug in with your data or configuration. If you'd be happy to use a generic, off\",\n",
       " \" you'd be happy to use a generic, off-the-shelf prompt for question answering over PDFs or standard text-to-SQL, you will find a rich ecosystem in these libraries. **DSPy** doesn't internally contain hand-crafted prompts that target specific applications. Instead, **DSPy** introduces a small set of much more powerful and general-purpose modules _that can learn to prompt (or finetune) your LM within your pipeline on your data_. when\",\n",
       " \" your LM within your pipeline on your data_. when you change your data, make tweaks to your program's control flow, or change your target LM, the **DSPy compiler** can map your program into a new set of prompts (or finetunes) that are optimized specifically for this pipeline. Because of this, you may find that **DSPy** obtains the highest quality for your task, with the least effort, provided you're willing to implement (or extend) your own\",\n",
       " \"'re willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\\n\\n**DSPy vs\",\n",
       " \"level libraries).\\n\\n**DSPy vs. generation control libraries like Guidance, LMQL, RELM, Outlines** These are all exciting new libraries for controlling the individual completions of LMs, e.g., if you want to enforce JSON output schema or constrain sampling to a particular regular expression. This is very useful in many settings, but it's generally focused on low-level, structured control of a single LM call. It doesn't help ensure the JSON (or\",\n",
       " \". It doesn't help ensure the JSON (or structured output) you get is going to be correct or useful for your task. In contrast, **DSPy** automatically optimizes the prompts in your programs to align them with various task needs, which may also include producing valid structured outputs. That said, we are considering allowing **Signatures** in **DSPy** to express regex-like constraints that are implemented by these libraries.\\n\\n## Basic Usage\\n\\n**How should I\",\n",
       " '\\n## Basic Usage\\n\\n**How should I use DSPy for my task?** We wrote a [eight-step guide](https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task) on this. In short, using DSPy is an iterative process. You first define your task and the metrics you want to maximize, and prepare a few example inputs — typically without labels (or only with labels for the',\n",
       " ' typically without labels (or only with labels for the final outputs, if your metric requires them). Then, you build your pipeline by selecting built-in layers (`modules`) to use, giving each layer a `signature` (input/output spec), and then calling your modules freely in your Python code. Lastly, you use a DSPy `optimizer` to compile your code into high-quality instructions, automatic few-shot examples, or updated LM weights for your LM.',\n",
       " ' examples, or updated LM weights for your LM.\\n\\n**How do I convert my complex prompt into a DSPy pipeline?** See the same answer above.\\n\\n**What do DSPy optimizers tune?** Or, _what does compiling actually do?_ Each optimizer is different, but they all seek to maximize a metric on your program by updating prompts or LM weights. Current DSPy `optimizers` can inspect your data, simulate traces through your program to',\n",
       " ' inspect your data, simulate traces through your program to generate good/bad examples of each step, propose or refine instructions for each step based on past results, finetune the weights of your LM on self-generated examples, or combine several of these to improve quality or cut cost. We\\'d love to merge new optimizers that explore a richer space: most manual steps you currently go through for prompt engineering, \"synthetic data\" generation, or self-improvement can probably generalized into a',\n",
       " ' or self-improvement can probably generalized into a DSPy optimizer that acts on arbitrary LM programs.\\n\\nOther FAQs. We welcome PRs to add formal answers to each of these here. You will find the answer in existing issues, tutorials, or the papers for all or most of these.\\n\\n- **How do I get multiple outputs?**\\n\\nYou can specify multiple output fields. For the short-form signature, you can list multiple outputs as comma separated values',\n",
       " ', you can list multiple outputs as comma separated values, following the \"->\" indicator (e.g. \"inputs -> output1, output2\"). For the long-form signature, you can include multiple `dspy.OutputField`s.\\n\\n- **How can I work with long responses?**\\n\\nYou can specify the generation of long responses as a `dspy.OutputField`. To ensure comprehensive checks of the content within the long-form generations,',\n",
       " ' of the content within the long-form generations, you can indicate the inclusion of citations per referenced context. Such constraints such as response length or citation inclusion can be stated through Signature descriptions, or concretely enforced through DSPy Assertions. Check out the [LongFormQA notebook](https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/examples/longformqa/longformqa_assertions.',\n",
       " 'formqa/longformqa_assertions.ipynb) to learn more about **Generating long-form length responses to answer questions**.\\n\\n- **How do I define my own metrics? Can metrics return a float?**\\n\\nYou can define metrics as simply Python functions that process model generations and evaluate them based on user-defined requirements. Metrics can compare existent data (e.g. gold labels) to model predictions or they can be used to assess various',\n",
       " ' model predictions or they can be used to assess various components of an output using validation feedback from LMs (e.g. LLMs-as-Judges). Metrics can return `bool`, `int`, and `float` types scores. Check out the official [Metrics docs](https://dspy-docs.vercel.app/docs/building-blocks/metrics) to learn more about defining custom metrics and advanced evaluations using AI feedback and/or DSPy',\n",
       " ' evaluations using AI feedback and/or DSPy programs.\\n\\n- **How expensive or slow is compiling??**\\n\\nTo reflect compiling metrics, we highlight an experiment for reference, compiling the [`SimplifiedBaleen`](https://dspy-docs.vercel.app/docs/tutorials/simplified-baleen) using the [`dspy.BootstrapFewShotWithRandomSearch`](https://dspy-',\n",
       " 'RandomSearch`](https://dspy-docs.vercel.app/docs/deep-dive/teleprompter/bootstrap-fewshot) optimizer on the `gpt-3.5-turbo-1106` model over 7 candidate programs and 10 threads. We report that compiling this program takes around 6 minutes with 3200 API calls, 2.7 million input tokens and 156,000 output tokens, reporting a total cost of $3 USD (at',\n",
       " ' reporting a total cost of $3 USD (at the current pricing of the OpenAI model).\\n\\nCompiling DSPy `optimizers` naturally will incur additional LM calls, but we substantiate this overhead with minimalistic executions with the goal of maximizing performance. This invites avenues to enhance performance of smaller models by compiling DSPy programs with larger models to learn enhanced behavior during compile-time and propagate such behavior to the tested smaller model during inference-time.  \\n\\n\\n## Deploy',\n",
       " \" inference-time.  \\n\\n\\n## Deployment or Reproducibility Concerns\\n\\n- **How do I save a checkpoint of my compiled program?**\\n\\nHere is an example of saving/loading a compiled module:\\n\\n```python\\ncot_compiled = teleprompter.compile(CoT(), trainset=trainset, valset=devset)\\n\\n#Saving\\ncot_compiled.save('compiled_cot_\",\n",
       " \"compiled.save('compiled_cot_gsm8k.json')\\n\\n#Loading:\\ncot = CoT()\\ncot.load('compiled_cot_gsm8k.json')\\n```\\n\\n- **How do I export for deployment?**\\n\\nExporting DSPy programs is simply saving them as highlighted above!\\n\\n- **How do I search my own data?**\\n\\nOpen source libraries such as [RAGautou\",\n",
       " '\\nOpen source libraries such as [RAGautouille](https://github.com/bclavie/ragatouille) enable you to search for your own data through advanced retrieval models like ColBERT with tools to embdeed and index documents. Feel free to integrate such libraries to create searchable datasets while developing your DSPy programs!\\n\\n- **How do I turn off the cache? How do I export the cache?**\\n\\nYou can turn off the',\n",
       " ' cache?**\\n\\nYou can turn off the cache by setting the [`cache_turn_on` flag to `False`](https://github.com/stanfordnlp/dspy/blob/9d8a40c477b9dd6dcdc007647b5b9ddad2b5657a/dsp/modules/cache_utils.py#L10).\\n\\nYour local cache will be saved to the global env',\n",
       " 'Your local cache will be saved to the global env directory `os.environ[\"DSP_NOTEBOOK_CACHEDIR\"]` which you can usually set to `os.path.join(repo_path, \\'cache\\')` and export this cache from here.\\n\\n\\n## Advanced Usage\\n\\n- **How do I parallelize?**\\nYou can parallelize DSPy programs during both compilation and evaluation by specifying multiple thread settings in the respective DSPy `',\n",
       " ' multiple thread settings in the respective DSPy `optimizers` or within the `dspy.Evaluate` utility function.\\n\\n- **How do freeze a module?**\\n\\nModules can be frozen by setting their `._compiled` attribute to be True, indicating the module has gone through optimizer compilation and should not have its parameters adjusted. This is handled internally in optimizers such as `dspy.BootstrapFewShot` where the student program is',\n",
       " \"BootstrapFewShot` where the student program is ensured to be frozen before the teacher propagates the gathered few-shot demonstrations in the bootstrapping process. \\n\\n- **How do I get JSON output?**\\n\\nYou can specify JSON-type descriptions in the `desc` field of the long-form signature `dspy.OutputField` (e.g. `output = dspy.OutputField(desc='key-value pairs')`).\\n\\n\",\n",
       " \"='key-value pairs')`).\\n\\nIf you notice outputs are still not conforming to JSON formatting, try Asserting this constraint! Check out [Assertions](https://dspy-docs.vercel.app/docs/building-blocks/assertions) (or the next question!)\\n\\n- **How do I use DSPy assertions?**\\n\\n    a) **How to Add Assertions to Your Program**:\\n  \",\n",
       " 'ertions to Your Program**:\\n    - **Define Constraints**: Use `dspy.Assert` and/or `dspy.Suggest` to define constraints within your DSPy program. These are based on boolean validation checks for the outcomes you want to enforce, which can simply be Python functions to validate the model outputs.\\n    - **Integrating Assertions**: Keep your Assertion statements following a model generations (',\n",
       " ' your Assertion statements following a model generations (hint: following a module layer)\\n\\n    b) **How to Activate the Assertions**:\\n    1. **Using `assert_transform_module`**:\\n        - Wrap your DSPy module with assertions using the `assert_transform_module` function, along with a `backtrack_handler`. This function transforms your program to include internal assertions backtracking and',\n",
       " ' transforms your program to include internal assertions backtracking and retry logic, which can be customized as well:\\n        `program_with_assertions = assert_transform_module(ProgramWithAssertions(), backtrack_handler)`\\n    2. **Activate Assertions**:\\n        - Directly call `activate_assertions` on your DSPy program with assertions: `program_with',\n",
       " 'SPy program with assertions: `program_with_assertions = ProgramWithAssertions().activate_assertions()`\\n\\n    **Note**: To use Assertions properly, you must **activate** a DSPy program that includes `dspy.Assert` or `dspy.Suggest` statements from either of the methods above. \\n\\n## Errors\\n\\n- **How do I deal with \"context too long\" errors?**\\n',\n",
       " ' with \"context too long\" errors?**\\n\\nIf you\\'re dealing with \"context too long\" errors in DSPy, you\\'re likely using DSPy optimizers to include demonstrations within your prompt, and this is exceeding your current context window. Try reducing these parameters (e.g. `max_bootstrapped_demos` and `max_labeled_demos`). Additionally, you can also reduce the number of retrieved passages/docs/embeddings to',\n",
       " ' of retrieved passages/docs/embeddings to ensure your prompt is fitting within your model context length.\\n\\nA more general fix is simply increasing the number of `max_tokens` specified to the LM request (e.g. `lm = dspy.OpenAI(model = ..., max_tokens = ...`).\\n\\n- **How do I deal with timeouts or backoff errors?**\\n\\nFirstly, please refer to your LM',\n",
       " '**\\n\\nFirstly, please refer to your LM/RM provider to ensure stable status or sufficient rate limits for your use case!\\n\\nAdditionally, try reducing the number of threads you are testing on as the corresponding servers may get overloaded with requests and trigger a backoff + retry mechanism.\\n\\nIf all variables seem stable, you may be experiencing timeouts or backoff errors due to incorrect payload requests sent to the api providers. Please verify your arguments are compatible with the SDK you are interacting',\n",
       " \" your arguments are compatible with the SDK you are interacting with. At times, DSPy may have hard-coded arguments that are not relevant for your compatible, in which case, please free to open a PR alerting this or comment out these default settings for your usage. \\n\\n## Contributing\\n\\n**What if I have a better idea for prompting or synthetic data generation?** Perfect. We encourage you to think if it's best expressed as a module or an optimizer, and\",\n",
       " \" expressed as a module or an optimizer, and we'd love to merge it in DSPy so everyone can use it. DSPy is not a complete project; it's an ongoing effort to create structure (modules and optimizers) in place of hacky prompt and pipeline engineering tricks.\\n\\n**How can I add my favorite LM or vector store?** \\n\\nCheck out these walkthroughs on setting up a [Custom LM client](https://dspy-docs\",\n",
       " ' LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\\n---\\nsidebar_position: 1\\n---\\n\\n# About DSPy\\n\\n**D',\n",
       " '\\n# About DSPy\\n\\n**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate',\n",
       " ' steps to work well together, (4) generate synthetic examples to tune each step, and (5) use these examples to finetune smaller LMs to cut costs. Currently, this is hard and messy: every time you change your pipeline, your LM, or your data, all prompts (or finetuning steps) may need to change.\\n\\nTo make this more systematic and much more powerful, **DSPy** does two things. First, it separates the flow of your',\n",
       " ' things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\\n\\n**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-',\n",
       " '-3.5` or `GPT-4` and local models like `T5-base` or `Llama2-13b` to be much more reliable at tasks, i.e. having higher quality and/or avoiding specific failure patterns. **DSPy** optimizers will \"compile\" the _same_ program into _different_ instructions, few-shot prompts, and/or weight updates (finetunes) for each LM. This is a',\n",
       " \"etunes) for each LM. This is a new paradigm in which LMs and their prompts fade into the background as optimizable pieces of a larger system that can learn from data. **tldr;** less prompting, higher scores, and a more systematic approach to solving hard tasks with LMs.\\n\\n\\n## Analogy to Neural Networks\\n\\nWhen we build neural networks, we don't write manual _for-loops_ over lists of _hand-tuned_ floats.\",\n",
       " ' lists of _hand-tuned_ floats. Instead, you might use a framework like [PyTorch](https://pytorch.org/) to compose layers (e.g., `Convolution` or `Dropout`) and then use optimizers (e.g., SGD or Adam) to learn the parameters of the network.\\n\\nDitto! **DSPy** gives you the right general-purpose modules (e.g., `ChainOfTh',\n",
       " ' modules (e.g., `ChainOfThought`, `ReAct`, etc.), which replace string-based prompting tricks. To replace prompt hacking and one-off synthetic data generators, **DSPy** also gives you general optimizers (`BootstrapFewShotWithRandomSearch` or `BayesianSignatureOptimizer`), which are algorithms that update parameters in your program. Whenever you modify your code, your data, your assertions, or your metric, you can _comp',\n",
       " ' assertions, or your metric, you can _compile_ your program again and **DSPy** will create new effective prompts that fit your changes.\\n---\\nsidebar_position: 99999\\n---\\n\\n# Additional Resources\\n\\n## Tutorials\\n\\n| **Level** |  **Tutorial** |  **Run in Colab** |  **Description** |\\n| --- | -------------  |  -------------  |  -------------  | \\n|',\n",
       " '  |  -------------  | \\n| Beginner |  [**Getting Started**](https://github.com/stanfordnlp/dspy/blob/main/intro.ipynb) | [<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/github/stanfordnlp/d',\n",
       " 'com/github/stanfordnlp/dspy/blob/main/intro.ipynb)  |  Introduces the basic building blocks in DSPy. Tackles the task of complex question answering with HotPotQA. |\\n| Beginner | [**Minimal Working Example**](https://dspy-docs.vercel.app/docs/quick-start/minimal-example) | N/A | Builds and',\n",
       " 'example) | N/A | Builds and optimizes a very simple chain-of-thought program in DSPy for math question answering. Very short. |\\n| Beginner | [**Compiling for Tricky Tasks**](https://github.com/stanfordnlp/dspy/blob/main/examples/nli/scone/scone.ipynb) | N/A | Teaches LMs to reason about logical statements',\n",
       " ' | Teaches LMs to reason about logical statements and negation. Uses GPT-4 to bootstrap few-shot CoT demonstrations for GPT-3.5. Establishes a state-of-the-art result on [ScoNe](https://arxiv.org/abs/2305.19426). Contributed by [Chris Potts](https://twitter.com/ChrisGPotts/status/1740033519446057077).',\n",
       " '/1740033519446057077). |\\n| Beginner | [**Local Models & Custom Datasets**](https://github.com/stanfordnlp/dspy/blob/main/skycamp2023.ipynb) | [<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com',\n",
       " 'https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/skycamp2023.ipynb) | Illustrates two different things together: how to use local models (Llama-2-13B in particular) and how to use your own data examples for training and development.\\n| Intermediate | [**The DSPy Paper**](https://arxiv.org/abs/2310.',\n",
       " 'xiv.org/abs/2310.03714) | N/A | Sections 3, 5, 6, and 7 of the DSPy paper can be consumed as a tutorial. They include explained code snippets, results, and discussions of the abstractions and API.\\n| Intermediate | [**DSPy Assertions**](https://arxiv.org/abs/2312.13382) | [<img align=\"center\" src=\"https://',\n",
       " '<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/examples/longformqa/longformqa_assertions.ipynb) | Introduces example of applying DSPy Assertions while generating long-form responses to questions with citations.',\n",
       " ' generating long-form responses to questions with citations. Presents comparative evaluation in both zero-shot and compiled settings.\\n| Intermediate | [**Finetuning for Complex Programs**](https://twitter.com/lateinteraction/status/1712135660797317577) | [<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google',\n",
       " ' />](https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/examples/qa/hotpot/multihop_finetune.ipynb) | Teaches a local T5 model (770M) to do exceptionally well on HotPotQA. Uses only 200 labeled answers. Uses no hand-written prompts, no calls to OpenAI, and no labels for retrieval or reasoning.\\n|',\n",
       " ' and no labels for retrieval or reasoning.\\n| Advanced | [**Information Extraction**](https://twitter.com/KarelDoostrlnck/status/1724991014207930696) | [<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/drive/1CpsOiLiLYKeGr',\n",
       " '/1CpsOiLiLYKeGrhmq579_FmtGsD5uZ3Qe) | Tackles extracting information from long articles (biomedical research papers). Combines in-context learning and retrieval to set SOTA on BioDEX. Contributed by [Karel D’Oosterlinck](https://twitter.com/KarelDoostrlnck/status/1724991014207930696).  |\\n\\n\\n',\n",
       " '1014207930696).  |\\n\\n\\n## Resources\\n\\n- [DSPy talk at ScaleByTheBay Nov 2023](https://www.youtube.com/watch?v=Dt3H2ninoeY).\\n- [DSPy webinar with MLOps Learners](https://www.youtube.com/watch?v=im7bCLW2aM4), a bit longer with Q&A.\\n- Hands-on Overview',\n",
       " ' Q&A.\\n- Hands-on Overviews of DSPy by the community: [DSPy Explained! by Connor Shorten](https://www.youtube.com/watch?v=41EfOY0Ldkc), [DSPy explained by code_your_own_ai](https://www.youtube.com/watch?v=ycfnKPxBMck)\\n- Interviews: [Weaviate Podcast in-person',\n",
       " 's: [Weaviate Podcast in-person](https://www.youtube.com/watch?v=CDung1LnLbY), and you can find 6-7 other remote podcasts on YouTube from a few different perspectives/audiences.\\n- **Tracing in DSPy** with Arize Phoenix: [Tutorial for tracing your prompts and the steps of your DSPy programs](https://colab.research.google.com/github/Arize',\n",
       " 'research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/tracing/dspy_tracing_tutorial.ipynb)---\\nsidebar_position: 99998\\n---\\n\\n# Community Examples\\n\\nThe DSPy team believes complexity has to be justified. We take this seriously: we never release a complex tutorial (above) or example (below) _unless we can demonstrate empirically that this',\n",
       " \") _unless we can demonstrate empirically that this complexity has generally led to improved quality or cost._ This kind of rule is rarely enforced by other frameworks or docs, but you can count on it in DSPy examples.\\n\\nThere's a bunch of examples in the `examples/` directory and in the top-level directory. We welcome contributions!\\n\\nYou can find other examples tweeted by [@lateinteraction](https://twitter.com/lateinteraction) on Twitter\",\n",
       " 'twitter.com/lateinteraction) on Twitter/X.\\n\\n**Some other examples (not exhaustive, feel free to add more via PR):**\\n\\n- Applying DSPy Assertions\\n  - [Long-form Answer Generation with Citations, by Arnav Singhvi](https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/examples/longformqa/longformqa_',\n",
       " 'amples/longformqa/longformqa_assertions.ipynb)\\n  - [Generating Answer Choices for Quiz Questions, by Arnav Singhvi](https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/examples/quiz/quiz_assertions.ipynb)\\n  - [Generating Tweets for QA, by Arnav Singhvi](https://',\n",
       " 'A, by Arnav Singhvi](https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/examples/tweets/tweets_assertions.ipynb)\\n- [Compiling LCEL runnables from LangChain in DSPy](https://github.com/stanfordnlp/dspy/blob/main/examples/tweets/compiling',\n",
       " '/examples/tweets/compiling_langchain.ipynb)\\n- [AI feedback, or writing LM-based metrics in DSPy](https://github.com/stanfordnlp/dspy/blob/main/examples/tweets/tweet_metric.py)\\n- [DSPy Optimizers Benchmark on a bunch of different tasks, by Michael Ryan](https://github.com/stanfordnl',\n",
       " '](https://github.com/stanfordnlp/dspy/tree/main/testing/tasks)\\n- [Indian Languages NLI with gains due to compiling by Saiful Haq](https://github.com/saifulhaq95/DSPy-Indic/blob/main/indicxlni.ipynb)\\n- [Sophisticated Extreme Multi-Class Classification, IReRa, by Karel D’',\n",
       " ' IReRa, by Karel D’Oosterlinck](https://github.com/KarelDO/xmc.dspy)\\n- [DSPy on BIG-Bench Hard Example, by Chris Levy](https://drchrislevy.github.io/posts/dspy/dspy.html)\\n- [Using Ollama with DSPy for Mistral (quantized) by @jrknox1977](https',\n",
       " ') by @jrknox1977](https://gist.github.com/jrknox1977/78c17e492b5a75ee5bbaf9673aee4641)\\n- [Using DSPy, \"The Unreasonable Effectiveness of Eccentric Automatic Prompts\" (paper) by VMware\\'s Rick Battle & Teja Gollapudi, and interview at TheRegister](https://www.theregister.com/2024/02',\n",
       " \".theregister.com/2024/02/22/prompt_engineering_ai_models/)\\n\\nThere are also recent cool examples at [Weaviate's DSPy cookbook](https://github.com/weaviate/recipes/tree/main/integrations/dspy) by Connor Shorten. [See tutorial on YouTube](https://www.youtube.com/watch?v=CEuUG4Umfxs).---\\nside\",\n",
       " 'uUG4Umfxs).---\\nsidebar_position: 2\\n---\\n\\n# [02] Multi-Hop Question Answering\\n\\nA single search query is often not enough for complex QA tasks. For instance, an example within `HotPotQA` includes a question about the birth city of the writer of \"Right Back At It Again\". A search query often identifies the author correctly as \"Jeremy McKinnon\", but lacks the capability to compose the intended answer in',\n",
       " ' but lacks the capability to compose the intended answer in determining when he was born.\\n\\nThe standard approach for this challenge in retrieval-augmented NLP literature is to build multi-hop search systems, like GoldEn (Qi et al., 2019) and Baleen (Khattab et al., 2021). These systems read the retrieved results and then generate additional queries to gather additional information when necessary before arriving to a final answer. Using DSPy, we can easily simulate such systems',\n",
       " \" DSPy, we can easily simulate such systems in a few lines of code.\\n\\n## Configuring LM and RM\\n\\nWe'll start by setting up the language model (LM) and retrieval model (RM), which **DSPy** supports through multiple [LM](https://dspy-docs.vercel.app/docs/category/language-model-clients) and [RM](https://dspy-docs.vercel.app/docs/\",\n",
       " \"-docs.vercel.app/docs/category/retrieval-model-clients) APIs and [local models hosting](https://dspy-docs.vercel.app/docs/category/local-language-model-clients).\\n\\nIn this notebook, we'll work with GPT-3.5 (`gpt-3.5-turbo`) and the `ColBERTv2` retriever (a free server hosting a\",\n",
       " '2` retriever (a free server hosting a Wikipedia 2017 \"abstracts\" search index containing the first paragraph of each article from this [2017 dump](https://hotpotqa.github.io/wiki-readme.html)). We configure the LM and RM within DSPy, allowing DSPy to internally call the respective module when needed for generation or retrieval. \\n\\n```python\\nimport dspy\\n\\nturbo = dspy.OpenAI(',\n",
       " \"turbo = dspy.OpenAI(model='gpt-3.5-turbo')\\ncolbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\\n\\ndspy.settings.configure(lm=turbo, rm=colbertv2_wiki17_abstracts)\\n```\",\n",
       " 'wiki17_abstracts)\\n```\\n\\n## Loading the Dataset\\n\\nFor this tutorial, we make use of the mentioned `HotPotQA` dataset, a collection of complex question-answer pairs typically answered in a multi-hop fashion. We can load this dataset provided by DSPy through the `HotPotQA` class:\\n\\n```python\\nfrom dspy.datasets import HotPotQA\\n\\n# Load the dataset',\n",
       " \" HotPotQA\\n\\n# Load the dataset.\\ndataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)\\n\\n# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\\ntrainset = [x.with_inputs('question') for x in dataset.train\",\n",
       " \"s('question') for x in dataset.train]\\ndevset = [x.with_inputs('question') for x in dataset.dev]\\n\\nlen(trainset), len(devset)\\n```\\n**Output:**\\n```text\\n(20, 50)\\n```\\n\\n## Building Signature\\n\\nNow that we have the data loaded let's start defining the signatures for sub-tasks of out Baleen pipeline.\\n\\nWe\",\n",
       " 'asks of out Baleen pipeline.\\n\\nWe\\'ll start by creating the `GenerateAnswer` signature that\\'ll take `context` and `question` as input and give `answer` as output.\\n\\n```python\\nclass GenerateAnswer(dspy.Signature):\\n    \"\"\"Answer questions with short factoid answers.\"\"\"\\n\\n    context = dspy.InputField(desc=\"may contain relevant facts\")\\n    question = d',\n",
       " ' relevant facts\")\\n    question = dspy.InputField()\\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\\n```\\n\\nUnlike usual QA pipelines, we have an intermediate question-generation step in Baleen for which we\\'ll need to define a new Signature for the \"hop\" behavior: inputting some context and a question to generate a search query to find missing information. \\n\\n```python',\n",
       " ' find missing information. \\n\\n```python\\nclass GenerateSearchQuery(dspy.Signature):\\n    \"\"\"Write a simple search query that will help answer a complex question.\"\"\"\\n\\n    context = dspy.InputField(desc=\"may contain relevant facts\")\\n    question = dspy.InputField()\\n    query = dspy.OutputField()\\n```\\n\\n:::info\\nWe could',\n",
       " \"```\\n\\n:::info\\nWe could have written `context = GenerateAnswer.signature.context` to avoid duplicating the description of the context field.\\n:::\\n\\nNow that we have the necessary signatures in place, we can start building the Baleen pipeline!\\n\\n## Building the Pipeline\\n\\nSo, let's define the program itself `SimplifiedBaleen`. There are many possible ways to implement this, but we'll keep this version down to\",\n",
       " \" this, but we'll keep this version down to the key elements.\\n\\n```python\\nfrom dsp.utils import deduplicate\\n\\nclass SimplifiedBaleen(dspy.Module):\\n    def __init__(self, passages_per_hop=3, max_hops=2):\\n        super().__init__()\\n\\n        self.generate_query = [\",\n",
       " '   self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\\n        self.',\n",
       " '\\n        self.max_hops = max_hops\\n    \\n    def forward(self, question):\\n        context = []\\n        \\n        for hop in range(self.max_hops):\\n            query = self.generate_query[hop](context=context, question',\n",
       " '_query[hop](context=context, question=question).query\\n            passages = self.retrieve(query).passages\\n            context = deduplicate(context + passages)\\n\\n        pred = self.generate_answer(context=context, question=question)\\n        return dspy.Prediction',\n",
       " '    return dspy.Prediction(context=context, answer=pred.answer)\\n```\\n\\nAs we can see, the `__init__` method defines a few key sub-modules:\\n\\n- **generate_query**: For each hop, we will have one `dspy.ChainOfThought` predictor with the `GenerateSearchQuery` signature.\\n- **retrieve**: This module will conduct the search using the',\n",
       " \"**: This module will conduct the search using the generated queries over our defined ColBERT RM search index via the `dspy.Retrieve` module.\\n- **generate_answer**: This `dspy.Predict` module will be used with the `GenerateAnswer` signature to produce the final answer.\\n\\nThe `forward` method uses these sub-modules in simple control flow.\\n\\n1. First, we'll loop up to `self.\",\n",
       " \" First, we'll loop up to `self.max_hops` times.\\n2. In each iteration, we'll generate a search query using the predictor at `self.generate_query[hop]`.\\n3. We'll retrieve the top-k passages using that query.\\n4. We'll add the (deduplicated) passages to our `context` accumulator.\\n5. After the loop, we'll use `self.generate_answer` to\",\n",
       " \" use `self.generate_answer` to produce an answer.\\n6. We'll return a prediction with the retrieved `context` and predicted `answer`.\\n\\n## Executing the Pipeline\\n\\nLet's execute this program in its zero-shot (uncompiled) setting. \\n\\nThis doesn't necessarily imply the performance will be bad but rather that we're bottlenecked directly by the reliability of the underlying LM to understand our sub-tasks from minimal instructions. Often,\",\n",
       " ' sub-tasks from minimal instructions. Often, this is perfectly fine when using the most expensive/powerful models (e.g., GPT-4) on the easiest and most standard tasks (e.g., answering simple questions about popular entities).\\n\\n```python\\n# Ask any question you like to this simple RAG program.\\nmy_question = \"How many storeys are in the castle that David Gregory inherited?\"\\n\\n# Get the prediction. This contains `pred',\n",
       " '\\n# Get the prediction. This contains `pred.context` and `pred.answer`.\\nuncompiled_baleen = SimplifiedBaleen()  # uncompiled (i.e., zero-shot) program\\npred = uncompiled_baleen(my_question)\\n\\n# Print the contexts and the answer.\\nprint(f\"Question: {my_question}\")\\nprint(f\"Predicted Answer: {pred.answer}\")',\n",
       " 'Predicted Answer: {pred.answer}\")\\nprint(f\"Retrieved Contexts (truncated): {[c[:200] + \\'...\\' for c in pred.context]}\")\\n```\\n**Output:**\\n```text\\nQuestion: How many storeys are in the castle that David Gregory inherited?\\nPredicted Answer: five\\nRetrieved Contexts (truncated): [\\'David Gregory (physician) | David Gregory (20 December',\n",
       " ' (physician) | David Gregory (20 December 1625 – 1720) was a Scottish physician and inventor. His surname is sometimes spelt as Gregorie, the original Scottish spelling. He inherited Kinn...\\', \\'The Boleyn Inheritance | The Boleyn Inheritance is a novel by British author Philippa Gregory which was first published in 2006. It is a direct sequel to her previous novel \"The Other Boleyn Girl,\" an...\\', \\'Gregory of Gaeta |',\n",
       " \" an...', 'Gregory of Gaeta | Gregory was the Duke of Gaeta from 963 until his death. He was the second son of Docibilis II of Gaeta and his wife Orania. He succeeded his brother John II, who had left only daugh...', 'Kinnairdy Castle | Kinnairdy Castle is a tower house, having five storeys and a garret, two miles south of Aberchirder, Aberdeenshire, Scotland.\",\n",
       " 'irder, Aberdeenshire, Scotland. The alternative name is Old Kinnairdy....\\', \\'Kinnaird Head | Kinnaird Head (Scottish Gaelic: \"An Ceann Àrd\" , \"high headland\") is a headland projecting into the North Sea, within the town of Fraserburgh, Aberdeenshire on the east coast of Scotla...\\', \\'Kinnaird Castle, Brechin | Kinnaird Castle is a 15th-century castle in Angus',\n",
       " \" Castle is a 15th-century castle in Angus, Scotland. The castle has been home to the Carnegie family, the Earl of Southesk, for more than 600 years....']\\n```\\n\\nWe can inspect the last **three** calls to the LM (i.e., generating the first hop's query, generating the second hop's query, and generating the answer) using:\\n\\n```python\\nturbo.inspect_history(n=3)\\n\",\n",
       " \"inspect_history(n=3)\\n```\\n\\n## Optimizing the Pipeline\\n\\nHowever, a zero-shot approach quickly falls short for more specialized tasks, novel domains/settings, and more efficient (or open) models. \\n\\nTo address this, **DSPy** offers compilation. Let's compile our multi-hop (`SimplifiedBaleen`) program. \\n\\nLet's first define our validation logic for compilation: \\n\\n\",\n",
       " ' define our validation logic for compilation: \\n\\n- The predicted answer matches the gold answer.\\n- The retrieved context contains the gold answer.\\n- None of the generated queries is rambling (i.e., none exceeds 100 characters in length).\\n- None of the generated queries is roughly repeated (i.e., none is within 0.8 or higher F1 score of earlier queries).\\n\\n```python\\ndef validate_context_and_answer_and_hops(',\n",
       " \"context_and_answer_and_hops(example, pred, trace=None):\\n    if not dspy.evaluate.answer_exact_match(example, pred): return False\\n    if not dspy.evaluate.answer_passage_match(example, pred): return False\\n\\n    hops = [example.question] + [outputs.query for *_, outputs in trace if 'query' in outputs]\\n\\n\",\n",
       " \" in trace if 'query' in outputs]\\n\\n    if max([len(h) for h in hops]) > 100: return False\\n    if any(dspy.evaluate.answer_exact_match_str(hops[idx], hops[:idx], frac=0.8) for idx in range(2, len(hops))): return False\\n\\n    return True\\n```\\n\\nWe'll use one\",\n",
       " \" True\\n```\\n\\nWe'll use one of the most basic teleprompters in **DSPy**, namely, `BootstrapFewShot` to optimize the predictors in pipeline with few-shot examples.\\n\\n```python\\nfrom dspy.teleprompt import BootstrapFewShot\\n\\nteleprompter = BootstrapFewShot(metric=validate_context_and_answer_and_hops)\\ncompiled_baleen =\",\n",
       " \"hops)\\ncompiled_baleen = teleprompter.compile(SimplifiedBaleen(), teacher=SimplifiedBaleen(passages_per_hop=2), trainset=trainset)\\n```\\n\\n## Evaluating the Pipeline\\n\\nLet's now define our evaluation function and compare the performance of the uncompiled and compiled Baleen pipelines. While this devset does not serve as a completely reliable benchmark, it is instructive to\",\n",
       " \" a completely reliable benchmark, it is instructive to use for this tutorial. \\n\\n```python\\nfrom dspy.evaluate.evaluate import Evaluate\\n\\n# Set up the `evaluate_on_hotpotqa` function. We'll use this many times below.\\nevaluate_on_hotpotqa = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)\\n\\nuncompiled\",\n",
       " '_table=5)\\n\\nuncompiled_baleen_retrieval_score = evaluate_on_hotpotqa(uncompiled_baleen, metric=gold_passages_retrieved, display=False)\\n\\ncompiled_baleen_retrieval_score = evaluate_on_hotpotqa(compiled_baleen, metric=gold_passages_retrieved)\\n\\nprint(f\"## Retri',\n",
       " ')\\n\\nprint(f\"## Retrieval Score for uncompiled Baleen: {uncompiled_baleen_retrieval_score}\")\\nprint(f\"## Retrieval Score for compiled Baleen: {compiled_baleen_retrieval_score}\")\\n```\\n**Output:**\\n```text\\n## Retrieval Score for uncompiled Baleen: 36.0\\n## Retrieval Score for compiled Bale',\n",
       " '0\\n## Retrieval Score for compiled Baleen: 60.0\\n```\\n\\nExcellent! There might be something to this compiled, multi-hop program then.\\n\\nEarlier, we said simple programs are not very effective at finding all evidence required for answering each question. Is this resolved by the adding some greater prompting techniques in the forward function of `SimplifiedBaleen`? Does compiling programs improve performance?\\n\\nWhile in our tutorial we demonstrate our findings, the',\n",
       " \"While in our tutorial we demonstrate our findings, the answer for these questions will not always be obvious. However, DSPy makes it extremely easy to try out the many diverse approaches with minimal effort. \\n\\nNow that you've seen a example of how to build a simple yet powerful pipeline, it's time for you to build one yourself!\\n---\\nsidebar_position: 1\\n---\\n\\n# [01] RAG: Retrieval-Augmented Generation\\n\\nRet\",\n",
       " ' Retrieval-Augmented Generation\\n\\nRetrieval-augmented generation (RAG) is an approach that allows LLMs to tap into a large corpus of knowledge from sources and query its knowledge store to find relevant passages/content and produce a well-refined response.\\n\\nRAG ensures LLMs can dynamically utilize real-time knowledge even if not originally trained on the subject and give thoughtful answers. However, with this nuance comes greater complexities in setting up refined RAG pipelines. To',\n",
       " \" complexities in setting up refined RAG pipelines. To reduce these intricacies, we turn to **DSPy**, which offers a seamless approach to setting up prompting pipelines!\\n\\n## Configuring LM and RM\\n\\nWe'll start by setting up the language model (LM) and retrieval model (RM), which **DSPy** supports through multiple [LM](https://dspy-docs.vercel.app/docs/category/language-model-clients) and\",\n",
       " \"category/language-model-clients) and [RM](https://dspy-docs.vercel.app/docs/category/retrieval-model-clients) APIs and [local models hosting](https://dspy-docs.vercel.app/docs/category/local-language-model-clients).\\n\\nIn this notebook, we'll work with GPT-3.5 (`gpt-3.5-turbo`\",\n",
       " 'gpt-3.5-turbo`) and the `ColBERTv2` retriever (a free server hosting a Wikipedia 2017 \"abstracts\" search index containing the first paragraph of each article from this [2017 dump](https://hotpotqa.github.io/wiki-readme.html)). We configure the LM and RM within DSPy, allowing DSPy to internally call the respective module when needed for generation or retrieval. \\n\\n```',\n",
       " \" for generation or retrieval. \\n\\n```python\\nimport dspy\\n\\nturbo = dspy.OpenAI(model='gpt-3.5-turbo')\\ncolbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\\n\\ndspy.settings.configure(lm=turbo\",\n",
       " 'settings.configure(lm=turbo, rm=colbertv2_wiki17_abstracts)\\n```\\n\\n\\n## Loading the Dataset\\n\\nFor this tutorial, we make use of the `HotPotQA` dataset, a collection of complex question-answer pairs typically answered in a multi-hop fashion. We can load this dataset provided by DSPy through the `HotPotQA` class:\\n\\n```python\\nfrom d',\n",
       " \" class:\\n\\n```python\\nfrom dspy.datasets import HotPotQA\\n\\n# Load the dataset.\\ndataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)\\n\\n# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\\ntrainset\",\n",
       " \" labels and/or metadata.\\ntrainset = [x.with_inputs('question') for x in dataset.train]\\ndevset = [x.with_inputs('question') for x in dataset.dev]\\n\\nlen(trainset), len(devset)\\n```\\n**Output:**\\n```text\\n(20, 50)\\n```\\n\\n## Building Signatures\\n\\nNow that we have the data loaded, let\",\n",
       " \"\\nNow that we have the data loaded, let's start defining the [signatures](https://dspy-docs.vercel.app/docs/building-blocks/signatures) for the sub-tasks of our pipeline.\\n\\nWe can identify our simple input `question` and output `answer`, but since we are building out a RAG pipeline, we wish to utilize some contextual information from our ColBERT corpus. So let's define our signature: `context\",\n",
       " '. So let\\'s define our signature: `context, question --> answer`.\\n\\n```python\\nclass GenerateAnswer(dspy.Signature):\\n    \"\"\"Answer questions with short factoid answers.\"\"\"\\n\\n    context = dspy.InputField(desc=\"may contain relevant facts\")\\n    question = dspy.InputField()\\n    answer = dspy.OutputField(desc=\"often between 1 and 5',\n",
       " 'OutputField(desc=\"often between 1 and 5 words\")\\n```\\n\\nWe include small descriptions for the `context` and `answer` fields to define more robust guidelines on what the model will receive and should generate. \\n\\n## Building the Pipeline\\n\\nWe will build our RAG pipeline as a [DSPy module](https://dspy-docs.vercel.app/docs/building-blocks/modules) which will require two methods:\\n\\n*',\n",
       " \") which will require two methods:\\n\\n* The `__init__` method will simply declare the sub-modules it needs: `dspy.Retrieve` and `dspy.ChainOfThought`. The latter is defined to implement our `GenerateAnswer` signature.\\n* The `forward` method will describe the control flow of answering the question using the modules we have: Given a question, we'll search for the top-3 relevant passages and then feed them\",\n",
       " ' the top-3 relevant passages and then feed them as context for answer generation.\\n\\n\\n```python\\nclass RAG(dspy.Module):\\n    def __init__(self, num_passages=3):\\n        super().__init__()\\n\\n        self.retrieve = dspy.Retrieve(k=num_passages)\\n        self',\n",
       " ')\\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\\n    \\n    def forward(self, question):\\n        context = self.retrieve(question).passages\\n        prediction = self.generate_answer(context=context, question=question)\\n        return dsp',\n",
       " \"        return dspy.Prediction(context=context, answer=prediction.answer)\\n```\\n\\n## Optimizing the Pipeline\\n\\n##### Compiling the RAG program\\n\\nHaving defined this program, let's now **compile** it. [Compiling a program](https://dspy-docs.vercel.app/docs/building-blocks/optimizers) will update the parameters stored in each module. In\",\n",
       " \" will update the parameters stored in each module. In our setting, this is primarily in the form of collecting and selecting good demonstrations for inclusion within the prompt(s).\\n\\nCompiling depends on three things:\\n\\n1. **A training set.** We'll just use our 20 question–answer examples from `trainset` above.\\n1. **A metric for validation.** We'll define a simple `validate_context_and_answer` that checks that the predicted\",\n",
       " '_and_answer` that checks that the predicted answer is correct and that the retrieved context actually contains the answer.\\n1. **A specific teleprompter.** The **DSPy** compiler includes a number of **teleprompters** that can optimize your programs.\\n\\n```python\\nfrom dspy.teleprompt import BootstrapFewShot\\n\\n# Validation logic: check that the predicted answer is correct.\\n# Also check that the retrieved context does',\n",
       " '.\\n# Also check that the retrieved context does actually contain that answer.\\ndef validate_context_and_answer(example, pred, trace=None):\\n    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\\n    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\\n    return answer_EM and answer_PM\\n\\n# Set up',\n",
       " 'EM and answer_PM\\n\\n# Set up a basic teleprompter, which will compile our RAG program.\\nteleprompter = BootstrapFewShot(metric=validate_context_and_answer)\\n\\n# Compile!\\ncompiled_rag = teleprompter.compile(RAG(), trainset=trainset)\\n```\\n\\n:::info\\n**Teleprompters:** Teleprompters are powerful optimizers that can',\n",
       " ' Teleprompters are powerful optimizers that can take any program and learn to bootstrap and select effective prompts for its modules. Hence the name which means \"prompting at a distance\".\\n\\nDifferent teleprompters offer various tradeoffs in terms of how much they optimize cost versus quality, etc. We will used a simple default `BootstrapFewShot` in the example above.\\n\\n_If you\\'re into analogies, you could think of this as your training data,',\n",
       " \" you could think of this as your training data, your loss function, and your optimizer in a standard DNN supervised learning setup. Whereas SGD is a basic optimizer, there are more sophisticated (and more expensive!) ones like Adam or RMSProp._\\n:::\\n\\n## Executing the Pipeline\\n\\nNow that we've compiled our RAG program, let's try it out.\\n\\n```python\\n# Ask any question you like to this simple RAG program.\",\n",
       " ' question you like to this simple RAG program.\\nmy_question = \"What castle did David Gregory inherit?\"\\n\\n# Get the prediction. This contains `pred.context` and `pred.answer`.\\npred = compiled_rag(my_question)\\n\\n# Print the contexts and the answer.\\nprint(f\"Question: {my_question}\")\\nprint(f\"Predicted Answer: {pred.answer}\")\\nprint(f\"Retrieved Context',\n",
       " '}\")\\nprint(f\"Retrieved Contexts (truncated): {[c[:200] + \\'...\\' for c in pred.context]}\")\\n```\\n\\nExcellent. How about we inspect the last prompt for the LM?\\n\\n```python\\nturbo.inspect_history(n=1)\\n```\\n**Output:**\\n```text\\nAnswer questions with short factoid answers.\\n\\n---\\n\\nQuestion: At',\n",
       " ' answers.\\n\\n---\\n\\nQuestion: At My Window was released by which American singer-songwriter?\\nAnswer: John Townes Van Zandt\\n\\nQuestion: \"Everything Has Changed\" is a song from an album released under which record label ?\\nAnswer: Big Machine Records\\n...(truncated)\\n```\\n\\nEven though we haven\\'t written any of this detailed demonstrations, we see that DSPy was able to bootstrap this 3,000 token prompt',\n",
       " \" able to bootstrap this 3,000 token prompt for 3-shot retrieval-augmented generation with hard negative passages and uses Chain-of-Thought reasoning within an extremely simply-written program.\\n\\nThis illustrates the power of composition and learning. Of course, this was just generated by a particular teleprompter, which may or may not be perfect in each setting. As you'll see in DSPy, there is a large but systematic space of options you have to optimize and\",\n",
       " \" but systematic space of options you have to optimize and validate with respect to your program's quality and cost.\\n\\nYou can also easily inspect the learned objects themselves.\\n\\n```python\\nfor name, parameter in compiled_rag.named_predictors():\\n    print(name)\\n    print(parameter.demos[0])\\n    print()\\n```\\n\\n## Evaluating the Pipeline\\n\\nWe can now evaluate our `\",\n",
       " \" the Pipeline\\n\\nWe can now evaluate our `compiled_rag` program on the dev set. Of course, this tiny set is _not_ meant to be a reliable benchmark, but it'll be instructive to use it for illustration.\\n\\nLet's evaluate the accuracy (exact match) of the predicted answer.\\n\\n```python\\nfrom dspy.evaluate.evaluate import Evaluate\\n\\n# Set up the `evaluate_on_hotpotqa` function\",\n",
       " \" `evaluate_on_hotpotqa` function. We'll use this many times below.\\nevaluate_on_hotpotqa = Evaluate(devset=devset, num_threads=1, display_progress=False, display_table=5)\\n\\n# Evaluate the `compiled_rag` program with the `answer_exact_match` metric.\\nmetric = dspy.evaluate.answer_exact_match\\nevaluate_\",\n",
       " '.answer_exact_match\\nevaluate_on_hotpotqa(compiled_rag, metric=metric)\\n```\\n**Output:**\\n```text\\nAverage Metric: 22 / 50  (44.0): 100%|██████████| 50/50 [00:00<00:00, 116.45it/s]\\nAverage Metric: 22 / 50  (44.0%)\\n\\n44.0\\n```\\n',\n",
       " '%)\\n\\n44.0\\n```\\n\\n## Evaluating the Retrieval\\n\\nIt may also be instructive to look at the accuracy of retrieval. While there are multiple ways to do this, we can simply check whether the retrieved passages contain the answer.\\n\\nWe can make use of our dev set which includes the gold titles that should be retrieved.\\n\\n```python\\ndef gold_passages_retrieved(example, pred, trace=None):\\n ',\n",
       " \"example, pred, trace=None):\\n    gold_titles = set(map(dspy.evaluate.normalize_text, example['gold_titles']))\\n    found_titles = set(map(dspy.evaluate.normalize_text, [c.split(' | ')[0] for c in pred.context]))\\n\\n    return gold_titles.issubset(found_titles\",\n",
       " 'itles.issubset(found_titles)\\n\\ncompiled_rag_retrieval_score = evaluate_on_hotpotqa(compiled_rag, metric=gold_passages_retrieved)\\n```\\n**Output:**\\n```text\\nAverage Metric: 13 / 50  (26.0): 100%|██████████| 50/50 [00:00<00:00, 671.76it/s]Average',\n",
       " \", 671.76it/s]Average Metric: 13 / 50  (26.0%)\\n```\\n\\nAlthough this simple `compiled_rag` program is able to answer a decent fraction of the questions correctly (on this tiny set, over 40%), the quality of retrieval is much lower.\\n\\nThis potentially suggests that the LM is often relying on the knowledge it memorized during training to answer questions. To address this weak retrieval, let's explore a second program\",\n",
       " \" this weak retrieval, let's explore a second program that involves more advanced search behavior.\\n---\\nsidebar_position: 2\\n---\\n\\n# Signatures\\n\\nWhen we assign tasks to LMs in DSPy, we specify the behavior we need as a Signature.\\n\\n**A signature is a declarative specification of input/output behavior of a DSPy module.** Signatures allow you to tell the LM _what_ it needs to do, rather than specify\",\n",
       " \"what_ it needs to do, rather than specify _how_ we should ask the LM to do it.\\n\\n\\nYou're probably familiar with function signatures, which specify the input and output arguments and their types. DSPy signatures are similar, but the differences are that:\\n\\n- While typical function signatures just _describe_ things, DSPy Signatures _define and control the behavior_ of modules.\\n\\n- The field names matter in DSPy Signatures. You\",\n",
       " ' names matter in DSPy Signatures. You express semantic roles in plain English: a `question` is different from an `answer`, a `sql_query` is different from `python_code`.\\n\\n\\n## Why should I use a DSPy Signature?\\n\\n**tl;dr** For modular and clean code, in which LM calls can be optimized into high-quality prompts (or automatic finetunes).\\n\\n**Long Answer:** Most people coerce LMs',\n",
       " 'Long Answer:** Most people coerce LMs to do tasks by hacking long, brittle prompts. Or by collecting/generating data for fine-tuning.\\n\\nWriting signatures is far more modular, adaptive, and reproducible than hacking at prompts or finetunes. The DSPy compiler will figure out how to build a highly-optimized prompt for your LM (or finetune your small LM) for your signature, on your data, and within your pipeline. In many',\n",
       " ' your data, and within your pipeline. In many cases, we found that compiling leads to better prompts than humans write. Not because DSPy optimizers are more creative than humans, but simply because they can try more things and tune the metrics directly.\\n\\n\\n## **Inline** DSPy Signatures\\n\\nSignatures can be defined as a short string, with argument names that define semantic roles for inputs/outputs.\\n\\n1. Question Answering: `\"question',\n",
       " '1. Question Answering: `\"question -> answer\"`\\n\\n2. Sentiment Classification: `\"sentence -> sentiment\"`\\n\\n3. Summarization: `\"document -> summary\"`\\n\\nYour signatures can also have multiple input/output fields.\\n\\n4. Retrieval-Augmented Question Answering: `\"context, question -> answer\"`\\n\\n5. Multiple-Choice Question Answering with Reasoning: `\"question',\n",
       " ' Answering with Reasoning: `\"question, choices -> reasoning, selection\"`\\n\\n\\n**Tip:** For fields, any valid variable names work! Field names should be semantically meaningful, but start simple and don\\'t prematurely optimize keywords! Leave that kind of hacking to the DSPy compiler. For example, for summarization, it\\'s probably fine to say `\"document -> summary\"`, `\"text -> gist\"`, or `\"long_context -> tldr',\n",
       " ' or `\"long_context -> tldr\"`.\\n\\n\\n### Example A: Sentiment Classification\\n\\n```python\\nsentence = \"it\\'s a charming and often affecting journey.\"  # example from the SST-2 dataset.\\n\\nclassify = dspy.Predict(\\'sentence -> sentiment\\')\\nclassify(sentence=sentence).sentiment\\n```\\n**Output:**\\n```text\\n\\'Positive\\'\\n```',\n",
       " '`text\\n\\'Positive\\'\\n```\\n\\n\\n### Example B: Summarization\\n\\n```python\\n# Example from the XSum dataset.\\ndocument = \"\"\"The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for',\n",
       " ' and then Colchester United. He scored twice for the U\\'s but was unable to save them from relegation. The length of Lee\\'s contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page.\"\"\"\\n\\nsummarize = dspy.ChainOfThought(\\'document -> summary\\')\\nresponse = summarize(document=document)\\n\\nprint(response.summary)\\n```\\n**Output:**\\n```text\\nThe',\n",
       " '**Output:**\\n```text\\nThe 21-year-old Lee made seven appearances and scored one goal for West Ham last season. He had loan spells in League One with Blackpool and Colchester United, scoring twice for the latter. He has now signed a contract with Barnsley, but the length of the contract has not been revealed.\\n```\\n\\n\\nMany DSPy modules (except `dspy.Predict`) return auxiliary information by expanding your signature',\n",
       " 'redict`) return auxiliary information by expanding your signature under the hood.\\n\\nFor example, `dspy.ChainOfThought` also adds a `rationale` field that includes the LM\\'s reasoning before it generates the output `summary`.\\n\\n```python\\nprint(\"Rationale:\", response.rationale)\\n```\\n**Output:**\\n```text\\nRationale: produce the summary. We need to highlight the key points about Lee\\'s',\n",
       " \" We need to highlight the key points about Lee's performance for West Ham, his loan spells in League One, and his new contract with Barnsley. We also need to mention that his contract length has not been disclosed.\\n```\\n\\n## **Class-based** DSPy Signatures\\n\\nFor some advanced tasks, you need more verbose signatures. This is typically to:\\n\\n1. Clarify something about the nature of the task (expressed below as a `\",\n",
       " ' of the task (expressed below as a `docstring`).\\n\\n2. Supply hints on the nature of an input field, expressed as a `desc` keyword argument for `dspy.InputField`.\\n\\n3. Supply constraints on an output field, expressed as a `desc` keyword argument for `dspy.OutputField`.\\n\\n\\n### Example C: Classification\\n\\nNotice how the docstring contains (minimal) instructions, which in this case are necessary',\n",
       " 'imal) instructions, which in this case are necessary to have a fully-defined task.\\n\\nSome optimizers in DSPy, like `SignatureOptimizer`, can take this simple docstring and then generate more effective variants if needed.\\n\\n```python\\nclass Emotion(dspy.Signature):\\n    \"\"\"Classify emotion among sadness, joy, love, anger, fear, surprise.\"\"\"\\n    \\n    sentence',\n",
       " '\\n    \\n    sentence = dspy.InputField()\\n    sentiment = dspy.OutputField()\\n\\nsentence = \"i started feeling a little vulnerable when the giant spotlight started blinding me\"  # from dair-ai/emotion\\n\\nclassify = dspy.Predict(Emotion)\\nclassify(sentence=sentence)\\n```\\n**Output:**\\n```text\\nPred',\n",
       " \"**Output:**\\n```text\\nPrediction(\\n    sentiment='Fear'\\n)\\n```\\n\\n**Tip:** There's nothing wrong with specifying your requests to the LM more clearly. Class-based Signatures help you with that. However, don't prematurely tune the keywords of the your signature by hand. The DSPy optimizers will likely do a better job (and will transfer better across LMs).\\n\\n\\n### Example D: A metric that\",\n",
       " ').\\n\\n\\n### Example D: A metric that evaluates faithfulness to citations\\n\\n```python\\nclass CheckCitationFaithfulness(dspy.Signature):\\n    \"\"\"Verify that the text is based on the provided context.\"\"\"\\n\\n    context = dspy.InputField(desc=\"facts here are assumed to be true\")\\n    text = dspy.InputField()\\n    faithfulness = dspy.',\n",
       " '    faithfulness = dspy.OutputField(desc=\"True/False indicating if text is faithful to context\")\\n\\ncontext = \"The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U\\'s but was unable',\n",
       " ' He scored twice for the U\\'s but was unable to save them from relegation. The length of Lee\\'s contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page.\"\\n\\ntext = \"Lee scored 3 goals for Colchester United.\"\\n\\nfaithfulness = dspy.ChainOfThought(CheckCitationFaithfulness)\\nfaithfulness(context=context, text=text)\\n```\\n**Output:**\\n```',\n",
       " '```\\n**Output:**\\n```text\\nPrediction(\\n    rationale=\"produce the faithfulness. We know that Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U\\'s but was unable to save them from relegation. However, there is no mention of him scoring three goals for Colchester United.\",\\n    faithfulness=\\'False\\'\\n)\\n```\\n\\n\\n## Using',\n",
       " \"'\\n)\\n```\\n\\n\\n## Using signatures to build modules & compiling them\\n\\nWhile signatures are convenient for prototyping with structured inputs/outputs, that's not the main reason to use them!\\n\\nYou should compose multiple signatures into bigger [DSPy modules](https://dspy-docs.vercel.app/docs/building-blocks/modules) and [compile these modules into optimized prompts](https://dspy-docs.vercel.\",\n",
       " '://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\\n---\\nsidebar_position: 3\\n---\\n\\n# Modules\\n\\nA **DSPy module** is a building block for programs that use LMs.\\n\\n- Each built-in module abstracts a',\n",
       " '\\n- Each built-in module abstracts a **prompting technique** (like chain of thought or ReAct). Crucially, they are generalized to handle any [DSPy Signature].\\n\\n- A DSPy module has **learnable parameters** (i.e., the little pieces comprising the prompt and the LM weights) and can be invoked (called) to process inputs and return outputs.\\n\\n- Multiple modules can be composed into bigger modules (programs). D',\n",
       " \" be composed into bigger modules (programs). DSPy modules are inspired directly by NN modules in PyTorch, but applied to LM programs.\\n\\n\\n## How do I use a built-in module, like `dspy.Predict` or `dspy.ChainOfThought`?\\n\\nLet's start with the most fundamental module, `dspy.Predict`. Internally, all other DSPy modules are just built using `dsp\",\n",
       " \"SPy modules are just built using `dspy.Predict`.\\n\\nWe'll assume you are already at least a little familiar with [DSPy signatures], which are declarative specs for defining the behavior of any module we use in DSPy.\\n\\nTo use a module, we first **declare** it by giving it a signature. Then we **call** the module with the input arguments, and extract the output fields!\\n\\n```python\\nsent\",\n",
       " ' output fields!\\n\\n```python\\nsentence = \"it\\'s a charming and often affecting journey.\"  # example from the SST-2 dataset.\\n\\n# 1) Declare with a signature.\\nclassify = dspy.Predict(\\'sentence -> sentiment\\')\\n\\n# 2) Call with input argument(s). \\nresponse = classify(sentence=sentence)\\n\\n# 3) Access the output.\\nprint(response.sentiment',\n",
       " \" the output.\\nprint(response.sentiment)\\n```\\n**Output:**\\n```text\\nPositive\\n```\\n\\nWhen we declare a module, we can pass configuration keys to it.\\n\\nBelow, we'll pass `n=5` to request five completions. We can also pass `temperature` or `max_len`, etc.\\n\\nLet's use `dspy.ChainOfThought`. In many cases, simply swapping\",\n",
       " 'OfThought`. In many cases, simply swapping `dspy.ChainOfThought` in place of `dspy.Predict` improves quality.\\n\\n```python\\nquestion = \"What\\'s something great about the ColBERT retrieval model?\"\\n\\n# 1) Declare with a signature, and pass some config.\\nclassify = dspy.ChainOfThought(\\'question -> answer\\', n=5)\\n\\n# 2) Call with input',\n",
       " \"5)\\n\\n# 2) Call with input argument.\\nresponse = classify(question=question)\\n\\n# 3) Access the outputs.\\nresponse.completions.answer\\n```\\n**Output:**\\n```text\\n['One great thing about the ColBERT retrieval model is its superior efficiency and effectiveness compared to other models.',\\n 'Its ability to efficiently retrieve relevant information from large document collections.',\\n 'One great thing about the ColBERT retrieval\",\n",
       " \" 'One great thing about the ColBERT retrieval model is its superior performance compared to other models and its efficient use of pre-trained language models.',\\n 'One great thing about the ColBERT retrieval model is its superior efficiency and accuracy compared to other models.',\\n 'One great thing about the ColBERT retrieval model is its ability to incorporate user feedback and support complex queries.']\\n```\\n\\nLet's discuss the output object here.\\n\\nThe `dspy\",\n",
       " ' object here.\\n\\nThe `dspy.ChainOfThought` module will generally inject a `rationale` before the output field(s) of your signature.\\n\\nLet\\'s inspect the (first) rationale and answer!\\n\\n```python\\nprint(f\"Rationale: {response.rationale}\")\\nprint(f\"Answer: {response.answer}\")\\n```\\n**Output:**\\n```text\\nRationale:',\n",
       " '**\\n```text\\nRationale: produce the answer. We can consider the fact that ColBERT has shown to outperform other state-of-the-art retrieval models in terms of efficiency and effectiveness. It uses contextualized embeddings and performs document retrieval in a way that is both accurate and scalable.\\nAnswer: One great thing about the ColBERT retrieval model is its superior efficiency and effectiveness compared to other models.\\n```\\n\\nThis is accessible whether we',\n",
       " '\\n```\\n\\nThis is accessible whether we request one or many completions.\\n\\nWe can also access the different completions as a list of `Prediction`s or as several lists, one for each field.\\n\\n```python\\nresponse.completions[3].rationale == response.completions.rationale[3]\\n```\\n**Output:**\\n```text\\nTrue\\n```\\n\\n\\n## What other DSPy modules',\n",
       " '`\\n\\n\\n## What other DSPy modules are there? How can I use them?\\n\\nThe others are very similar. They mainly change the internal behavior with which your signature is implemented!\\n\\n1. **`dspy.Predict`**: Basic predictor. Does not modify the signature. Handles the key forms of learning (i.e., storing the instructions and demonstrations and updates to the LM).\\n\\n2. **`dspy.ChainOfTh',\n",
       " \". **`dspy.ChainOfThought`**: Teaches the LM to think step-by-step before committing to the signature's response.\\n\\n3. **`dspy.ProgramOfThought`**: Teaches the LM to output code, whose execution results will dictate the response.\\n\\n4. **`dspy.ReAct`**: An agent that can use tools to implement the given signature.\\n\\n5. **`d\",\n",
       " \" given signature.\\n\\n5. **`dspy.MultiChainComparison`**: Can compare multiple outputs from `ChainOfThought` to produce a final prediction.\\n\\n\\nWe also have some function-style modules:\\n\\n6. **`dspy.majority`**: Can do basic voting to return the most popular response from a set of predictions.\\n\\n\\nCheck out further examples in [each module's respective guide](https://dspy-docs.\",\n",
       " \" guide](https://dspy-docs.vercel.app/api/category/modules).\\n\\n\\n## How do I compose multiple modules into a bigger program?\\n\\nDSPy is just Python code that uses modules in any control flow you like. (There's some magic internally at `compile` time to trace your LM calls.)\\n\\nWhat this means is that, you can just call the modules freely. No weird abstractions for chaining calls.\\n\\nThis\",\n",
       " \" abstractions for chaining calls.\\n\\nThis is basically PyTorch's design approach for define-by-run / dynamic computation graphs. Refer to the intro tutorials for examples.\\n# DSPy Assertions \\n\\n## Introduction\\n\\nLanguage models (LMs) have transformed how we interact with machine learning, offering vast capabilities in natural language understanding and generation. However, ensuring these models adhere to domain-specific constraints remains a challenge. Despite the growth of techniques like fine-\",\n",
       " \" challenge. Despite the growth of techniques like fine-tuning or “prompt engineering”, these approaches are extremely tedious and rely on heavy, manual hand-waving to guide the LMs in adhering to specific constraints. Even DSPy's modularity of programming prompting pipelines lacks mechanisms to effectively and automatically enforce these constraints. \\n\\nTo address this, we introduce DSPy Assertions, a feature within the DSPy framework designed to automate the enforcement of\",\n",
       " ' DSPy framework designed to automate the enforcement of computational constraints on LMs. DSPy Assertions empower developers to guide LMs towards desired outcomes with minimal manual intervention, enhancing the reliability, predictability, and correctness of LM outputs.\\n\\n### dspy.Assert and dspy.Suggest API    \\n\\nWe introduce two primary constructs within DSPy Assertions:\\n\\n- **`dspy.Assert`**:\\n ',\n",
       " 'spy.Assert`**:\\n  - **Parameters**: \\n    - `constraint (bool)`: Outcome of Python-defined boolean validation check.\\n    - `msg (Optional[str])`: User-defined error message providing feedback or correction guidance.\\n    - `backtrack (Optional[module])`: Specifies target module for retry attempts upon constraint failure. The default backtracking module is the last',\n",
       " \" failure. The default backtracking module is the last module before the assertion.\\n  - **Behavior**: Initiates retry  upon failure, dynamically adjusting the pipeline's execution. If failures persist, it halts execution and raises a `dspy.AssertionError`.\\n\\n- **`dspy.Suggest`**:\\n  - **Parameters**: Similar to `dspy.Assert`.\\n  - **Behavior**: Encourages\",\n",
       " '  - **Behavior**: Encourages self-refinement through retries without enforcing hard stops. Logs failures after maximum backtracking attempts and continues execution.\\n\\n- **dspy.Assert vs. Python Assertions**: Unlike conventional Python `assert` statements that terminate the program upon failure, `dspy.Assert` conducts a sophisticated retry mechanism, allowing the pipeline to adjust. \\n\\nSpecifically, when a constraint is not met:\\n',\n",
       " \"Specifically, when a constraint is not met:\\n\\n- Backtracking Mechanism: An under-the-hood backtracking is initiated, offering the model a chance to self-refine and proceed, which is done through\\n- Dynamic Signature Modification: internally modifying your DSPy program’s Signature by adding the following fields:\\n    - Past Output: your model's past output that did not pass the validation_fn\\n    - Instruction: your user-\",\n",
       " \"\\n    - Instruction: your user-defined feedback message on what went wrong and what possibly to fix\\n\\nIf the error continues past the `max_backtracking_attempts`, then `dspy.Assert` will halt the pipeline execution, altering you with an `dspy.AssertionError`. This ensures your program doesn't continue executing with “bad” LM behavior and immediately highlights sample failure outputs for user assessment. \\n\\n- **\",\n",
       " ' outputs for user assessment. \\n\\n- **dspy.Suggest vs. dspy.Assert**: `dspy.Suggest` on the other hand offers a softer approach. It maintains the same retry backtracking as `dspy.Assert` but instead serves as a gentle nudger. If the model outputs cannot pass the model constraints after the `max_backtracking_attempts`, `dspy.Suggest` will log the persistent failure',\n",
       " 'spy.Suggest` will log the persistent failure and continue execution of the program on the rest of the data. This ensures the LM pipeline works in a \"best-effort\" manner without halting execution. \\n\\n- **`dspy.Suggest`** are best utilized as \"helpers\" during the evaluation phase, offering guidance and potential corrections without halting the pipeline.\\n- **`dspy.Assert`** are recommended during the development stage as \"checkers',\n",
       " ' are recommended during the development stage as \"checkers\" to ensure the LM behaves as expected, providing a robust mechanism for identifying and addressing errors early in the development cycle.\\n\\n\\n## Use Case: Including Assertions in DSPy Programs\\n\\nWe start with using an example of a multi-hop QA SimplifiedBaleen pipeline as defined in the intro walkthrough. \\n\\n```python\\nclass SimplifiedBaleen(dspy.Module):\\n ',\n",
       " 'en(dspy.Module):\\n    def __init__(self, passages_per_hop=2, max_hops=2):\\n        super().__init__()\\n\\n        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\\n        self.retrieve = d',\n",
       " '     self.retrieve = dspy.Retrieve(k=passages_per_hop)\\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\\n        self.max_hops = max_hops\\n\\n    def forward(self, question):\\n        context = []\\n       ',\n",
       " ' = []\\n        prev_queries = [question]\\n\\n        for hop in range(self.max_hops):\\n            query = self.generate_query[hop](context=context, question=question).query\\n            prev_queries.append(query)\\n         ',\n",
       " '\\n            passages = self.retrieve(query).passages\\n            context = deduplicate(context + passages)\\n        \\n        pred = self.generate_answer(context=context, question=question)\\n        pred = dspy.Prediction(context=context',\n",
       " ' dspy.Prediction(context=context, answer=pred.answer)\\n        return pred\\n\\nbaleen = SimplifiedBaleen()\\n\\nbaleen(question = \"Which award did Gary Zukav\\'s first book receive?\")\\n```\\n\\nTo include DSPy Assertions, we simply define our validation functions and declare our assertions following the respective model generation. \\n\\nFor this use case, suppose',\n",
       " '. \\n\\nFor this use case, suppose we want to impose the following constraints:\\n    1. Length - each query should be less than 100 characters\\n    2. Uniqueness - each generated query should differ from previously-generated queries. \\n    \\nWe can define these validation checks as boolean functions:\\n\\n```python\\n#simplistic boolean check for query length\\nlen(query) <= 100\\n\\n#Python function for valid',\n",
       " ') <= 100\\n\\n#Python function for validating distinct queries\\ndef validate_query_distinction_local(previous_queries, query):\\n    \"\"\"check if query is distinct from previous queries\"\"\"\\n    if previous_queries == []:\\n        return True\\n    if dspy.evaluate.answer_exact_match_str(query, previous_queries, frac=0.',\n",
       " ' previous_queries, frac=0.8):\\n        return False\\n    return True\\n```\\n\\nWe can declare these validation checks through `dspy.Suggest` statements (as we want to test the program in a best-effort demonstration). We want to keep these after the query generation `query = self.generate_query[hop](context=context, question=question).query`.\\n\\n```python',\n",
       " '=question).query`.\\n\\n```python\\ndspy.Suggest(\\n    len(query) <= 100,\\n    \"Query should be short and less than 100 characters\",\\n)\\n\\ndspy.Suggest(\\n    validate_query_distinction_local(prev_queries, query),\\n    \"Query should be distinct from: \"\\n    + \"; \".join(f\"{i+',\n",
       " ' \"; \".join(f\"{i+1}) {q}\" for i, q in enumerate(prev_queries)),\\n)\\n```\\n\\nIt is recommended to define a program with assertions separately than your original program if you are doing comparative evaluation for the effect of assertions. If not, feel free to set Assertions away!\\n\\nLet\\'s take a look at how the SimplifiedBaleen program will look with Assertions included:\\n\\n``',\n",
       " ' look with Assertions included:\\n\\n```python\\nclass SimplifiedBaleenAssertions(dspy.Module):\\n    def __init__(self, passages_per_hop=2, max_hops=2):\\n        super().__init__()\\n        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _',\n",
       " 'Thought(GenerateSearchQuery) for _ in range(max_hops)]\\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\\n        self.max_hops = max_hops\\n\\n    def forward(self,',\n",
       " 'hops\\n\\n    def forward(self, question):\\n        context = []\\n        prev_queries = [question]\\n\\n        for hop in range(self.max_hops):\\n            query = self.generate_query[hop](context=context, question=question).query\\n\\n         ',\n",
       " '\\n\\n            dspy.Suggest(\\n                len(query) <= 100,\\n                \"Query should be short and less than 100 characters\",\\n            )\\n\\n            dspy.Suggest(\\n',\n",
       " '    dspy.Suggest(\\n                validate_query_distinction_local(prev_queries, query),\\n                \"Query should be distinct from: \"\\n                + \"; \".join(f\"{i+1}) {q}\" for i,',\n",
       " 'i+1}) {q}\" for i, q in enumerate(prev_queries)),\\n            )\\n\\n            prev_queries.append(query)\\n            passages = self.retrieve(query).passages\\n            context = deduplicate(context + passages)',\n",
       " ' = deduplicate(context + passages)\\n        \\n        if all_queries_distinct(prev_queries):\\n            self.passed_suggestions += 1\\n\\n        pred = self.generate_answer(context=context, question=question)\\n        pred = dspy',\n",
       " '      pred = dspy.Prediction(context=context, answer=pred.answer)\\n        return pred\\n```\\n\\nNow calling programs with DSPy Assertions requires one last step, and that is transforming the program to wrap it with internal assertions backtracking and Retry logic. \\n\\n```python\\nfrom dspy.primitives.assertions import assert_transform_module, backtrack_',\n",
       " ' import assert_transform_module, backtrack_handler\\n\\nbaleen_with_assertions = assert_transform_module(SimplifiedBaleenAssertions(), backtrack_handler)\\n\\n# backtrack_handler is parameterized over a few settings for the backtracking mechanism\\n# To change the number of max retry attempts, you can do\\nbaleen_with_assertions_retry_once = assert_transform_module(Simpl',\n",
       " 'once = assert_transform_module(SimplifiedBaleenAssertions(), \\n    functools.partial(backtrack_handler, max_backtracks=1))\\n```\\n\\nAlternatively, you can also directly call `activate_assertions` on the program with `dspy.Assert/Suggest` statements using the default backtracking mechanism (`max_backtracks=2`):\\n\\n```python\\nbaleen_',\n",
       " \"\\n\\n```python\\nbaleen_with_assertions = SimplifiedBaleenAssertions().activate_assertions()\\n```\\n\\nNow let's take a look at the internal LM backtracking by inspecting the history of the LM query generations. Here we see that when a query fails to pass the validation check of being less than 100 characters, its internal `GenerateSearchQuery` signature is dynamically modified during the backtracking+Retry process to include the past\",\n",
       " ' backtracking+Retry process to include the past query and the corresponding user-defined instruction: `\"Query should be short and less than 100 characters\"`.\\n\\n\\n```text\\nWrite a simple search query that will help answer a complex question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: may contain relevant facts\\n\\nQuestion: ${question}\\n\\nReasoning: Let\\'s think step by step in order to ${produce the query}. We ...\\n\\n',\n",
       " ' ${produce the query}. We ...\\n\\nQuery: ${query}\\n\\n---\\n\\nContext:\\n[1] «Kerry Condon | Kerry Condon (born 4 January 1983) is [...]»\\n[2] «Corona Riccardo | Corona Riccardo (c. 1878October 15, 1917) was [...]»\\n\\nQuestion: Who acted in the shot film The Shore and is also the youngest actress ever to play Ophelia in a Royal Shakespeare',\n",
       " ' ever to play Ophelia in a Royal Shakespeare Company production of \"Hamlet.\" ?\\n\\nReasoning: Let\\'s think step by step in order to find the answer to this question. First, we need to identify the actress who played Ophelia in a Royal Shakespeare Company production of \"Hamlet.\" Then, we need to find out if this actress also acted in the short film \"The Shore.\"\\n\\nQuery: \"actress who played Ophelia in Royal Shakespeare Company production',\n",
       " ' who played Ophelia in Royal Shakespeare Company production of Hamlet\" + \"actress in short film The Shore\"\\n\\n\\n\\nWrite a simple search query that will help answer a complex question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: may contain relevant facts\\n\\nQuestion: ${question}\\n\\nPast Query: past output with errors\\n\\nInstructions: Some instructions you must satisfy\\n\\nQuery: ${query}\\n\\n---\\n\\nContext:\\n[',\n",
       " '}\\n\\n---\\n\\nContext:\\n[1] «Kerry Condon | Kerry Condon (born 4 January 1983) is an Irish television and film actress, best known for her role as Octavia of the Julii in the HBO/BBC series \"Rome,\" as Stacey Ehrmantraut in AMC\\'s \"Better Call Saul\" and as the voice of F.R.I.D.A.Y. in various films in the Marvel Cinematic Universe',\n",
       " '. in various films in the Marvel Cinematic Universe. She is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of \"Hamlet.\"»\\n[2] «Corona Riccardo | Corona Riccardo (c. 1878October 15, 1917) was an Italian born American actress who had a brief Broadway stage career before leaving to become a wife and mother. Born in Naples she came to acting in 1894 playing a Mexican girl in a play at',\n",
       " ' 1894 playing a Mexican girl in a play at the Empire Theatre. Wilson Barrett engaged her for a role in his play \"The Sign of the Cross\" which he took on tour of the United States. Riccardo played the role of Ancaria and later played Berenice in the same play. Robert B. Mantell in 1898 who struck by her beauty also cast her in two Shakespeare plays, \"Romeo and Juliet\" and \"Othello\". Author Lewis Strang writing',\n",
       " ' \"Othello\". Author Lewis Strang writing in 1899 said Riccardo was the most promising actress in America at the time. Towards the end of 1898 Mantell chose her for another Shakespeare part, Ophelia im Hamlet. Afterwards she was due to join Augustin Daly\\'s Theatre Company but Daly died in 1899. In 1899 she gained her biggest fame by playing Iras in the first stage production of Ben-Hur.»\\n\\nQuestion: Who acted in the shot film The Shore',\n",
       " 'Question: Who acted in the shot film The Shore and is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of \"Hamlet.\" ?\\n\\nPast Query: \"actress who played Ophelia in Royal Shakespeare Company production of Hamlet\" + \"actress in short film The Shore\"\\n\\nInstructions: Query should be short and less than 100 characters\\n\\nQuery: \"actress Ophelia RSC Hamlet\" + \"actress The',\n",
       " ' RSC Hamlet\" + \"actress The Shore\"\\n\\n```\\n\\n\\n## Assertion-Driven Optimizations\\n\\nDSPy Assertions work with optimizations that DSPy offers, particularly with `BootstrapFewShotWithRandomSearch`, including the following settings:\\n\\n- Compilation with Assertions\\n    This includes assertion-driven example bootstrapping and counterexample bootstrapping during compilation. The teacher model for bootstrapping',\n",
       " ' during compilation. The teacher model for bootstrapping few-shot demonstrations can make use of DSPy Assertions to offer robust bootstrapped examples for the student model to learn from during inference. In this setting, the student model does not perform assertion aware optimizations (backtracking and retry) during inference.\\n- Compilation + Inference with Assertions\\n    -This includes assertion-driven optimizations in both compilation and inference. Now the teacher model offers assertion-driven',\n",
       " ' inference. Now the teacher model offers assertion-driven examples but the student can further optimize with assertions of its own during inference time. \\n```python\\nteleprompter = BootstrapFewShotWithRandomSearch(\\n    metric=validate_context_and_answer_and_hops,\\n    max_bootstrapped_demos=max_bootstrapped_demos,\\n    num_candidate_programs=6,\\n',\n",
       " '_candidate_programs=6,\\n)\\n\\n#Compilation with Assertions\\ncompiled_with_assertions_baleen = teleprompter.compile(student = baleen, teacher = baleen_with_assertions, trainset = trainset, valset = devset)\\n\\n#Compilation + Inference with Assertions\\ncompiled_baleen_with_assertions = teleprompter.comp',\n",
       " 'with_assertions = teleprompter.compile(student=baleen_with_assertions, teacher = baleen_with_assertions, trainset=trainset, valset=devset)\\n\\n```# Typed Predictors\\n\\nIn DSPy Signatures, we have `InputField` and `OutputField` that define the nature of inputs and outputs of the field. However, the inputs and output to these fields are always',\n",
       " ', the inputs and output to these fields are always `str`-typed, which requires input and output string processing.\\n\\nPydantic `BaseModel` is a great way to enforce type constraints on the fields, but it is not directly compatible with the `dspy.Signature`. Typed Predictors resolves this as a way to enforce the type constraints on the inputs and outputs of the fields in a `dspy.Signature`.\\n\\n## Executing Typ',\n",
       " \".Signature`.\\n\\n## Executing Typed Predictors\\n\\nUsing Typed Predictors is not too different than any other module with the minor additions of type hints to signature attributes and using a special Predictor module instead of `dspy.Predict`. Let's take a look at a simple example to understand this.\\n\\n### Defining Input and Output Models\\n\\nLet's take a simple task as an example i.e. given the `context` and `query\",\n",
       " 'e. given the `context` and `query`, the LLM should return an `answer` and `confidence_score`. Let\\'s define our `Input` and `Output` models via pydantic.\\n\\n```python\\nfrom pydantic import BaseModel, Field\\n\\nclass Input(BaseModel):\\n    context: str = Field(description=\"The context for the question\")\\n    query: str = Field(description=\"The question to be',\n",
       " ' str = Field(description=\"The question to be answered\")\\n\\nclass Output(BaseModel):\\n    answer: str = Field(description=\"The answer for the question\")\\n    confidence: float = Field(ge=0, le=1, description=\"The confidence score for the answer\")\\n```\\n\\nAs you can see, we can describe the attributes by defining a simple Signature that takes in the input and returns the output.\\n\\n### Creating Typ',\n",
       " ' and returns the output.\\n\\n### Creating Typed Predictor\\n\\nA Typed Predictor needs a Typed Signature, which extends a `dspy.Signature` with the addition of specifying \"field type\".\\n\\n```python\\nclass QASignature(dspy.Signature):\\n    \"\"\"Answer the question based on the context and query provided, and on the scale of 10 tell how confident you are about the answer.\"\"\"\\n\\n  ',\n",
       " ' you are about the answer.\"\"\"\\n\\n    input: Input = dspy.InputField()\\n    output: Output = dspy.OutputField()\\n```\\n\\nNow that we have the `QASignature`, let\\'s define a Typed Predictor that executes this Signature while conforming to the type constraints.\\n\\n```python\\npredictor = dspy.TypedPredictor(QASignature)\\n``',\n",
       " 'redictor(QASignature)\\n```\\n\\nSimilar to other modules, we pass the `QASignature` to `dspy.TypedPredictor` which enforces the typed constraints.\\n\\nAnd similarly to `dspy.Predict`, we can also use a \"string signature\", which we type as:\\n```python\\npredictor = dspy.TypedPredictor(\"input:Input -> output:Output\")',\n",
       " 'or(\"input:Input -> output:Output\")\\n```\\n\\n### I/O in Typed Predictors\\n\\nNow let\\'s test out the Typed Predictor by providing some sample input to the predictor and verifying the output type. We can create an `Input` instance and pass it to the predictor to get a dictionary of the output. \\n\\n```python\\ndoc_query_pair = Input(\\n    context=\"The quick brown fox jumps over the',\n",
       " '  context=\"The quick brown fox jumps over the lazy dog\",\\n    query=\"What does the fox jumps over?\",\\n)\\n\\nprediction = predictor(input=doc_query_pair)\\n```\\n\\nLet\\'s see the output and its type.\\n\\n```python\\nanswer = prediction.answer\\nconfidence_score = prediction.confidence\\n\\nprint(f\"Prediction: {prediction}\\\\n\\\\n\")\\nprint(f\"Answer:',\n",
       " '\\\\n\")\\nprint(f\"Answer: {answer}, Answer Type: {type(answer)}\")\\nprint(f\"Confidence Score: {confidence_score}, Confidence Score Type: {type(confidence_score)}\")\\n```\\n\\n## Typed Chain of Thoughts with `dspy.TypedChainOfThought`\\n\\nExtending the analogous comparison of `TypedPredictor` to `dspy.Predict`, we create `',\n",
       " 'dspy.Predict`, we create `TypedChainOfThought`, the typed counterpart of `dspy.ChainOfThought`:\\n\\n```python\\ncot_predictor = dspy.TypedChainOfThought(QASignature)\\n\\ndoc_query_pair = Input(\\n    context=\"The quick brown fox jumps over the lazy dog\",\\n    query=\"What does the fox jumps over?\",\\n',\n",
       " ' query=\"What does the fox jumps over?\",\\n)\\n\\nprediction = cot_predictor(input=doc_query_pair)\\n```\\n\\n## Typed Predictors as Decorators\\n\\nWhile the `dspy.TypedPredictor` and `dspy.TypedChainOfThought` provide a convenient way to use typed predictors, you can also use them as decorators to enforce type constraints on the inputs and outputs',\n",
       " 'ators to enforce type constraints on the inputs and outputs of the function. This relies on the internal definitions of the Signature class and its function arguments, outputs, and docstrings.\\n\\n```python\\n@dspy.predictor\\ndef answer(doc_query_pair: Input) -> Output:\\n    \"\"\"Answer the question based on the context and query provided, and on the scale of 0-1 tell how confident you are about the answer.\"\"\"\\n ',\n",
       " ' confident you are about the answer.\"\"\"\\n    pass\\n\\n@dspy.cot\\ndef answer(doc_query_pair: Input) -> Output:\\n    \"\"\"Answer the question based on the context and query provided, and on the scale of 0-1 tell how confident you are about the answer.\"\"\"\\n    pass\\n\\nprediction = answer(doc_query_pair=doc_query_pair)\\n```\\n\\n##',\n",
       " \"query_pair)\\n```\\n\\n## Composing Functional Typed Predictors in `dspy.Module`\\n\\nIf you're creating DSPy pipelines via `dspy.Module`, then you can simply use Functional Typed Predictors by creating these class methods and using them as decorators. Here is an example of using functional typed predictors to create a `SimplifiedBaleen` pipeline:\\n\\n```python\\nclass SimplifiedBaleen\",\n",
       " '```python\\nclass SimplifiedBaleen(FunctionalModule):\\n    def __init__(self, passages_per_hop=3, max_hops=1):\\n        super().__init__()\\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\\n        self.max_hops = max_hops',\n",
       " '  self.max_hops = max_hops\\n\\n    @cot\\n    def generate_query(self, context: list[str], question) -> str:\\n        \"\"\"Write a simple search query that will help answer a complex question.\"\"\"\\n        pass\\n\\n    @cot\\n    def generate_answer(self, context: list[str], question) -> str:\\n ',\n",
       " '[str], question) -> str:\\n        \"\"\"Answer questions with short factoid answers.\"\"\"\\n        pass\\n\\n    def forward(self, question):\\n        context = []\\n\\n        for _ in range(self.max_hops):\\n            query = self.generate_query(context=context',\n",
       " ' self.generate_query(context=context, question=question)\\n            passages = self.retrieve(query).passages\\n            context = deduplicate(context + passages)\\n\\n        answer = self.generate_answer(context=context, question=question)\\n        return dspy.Pred',\n",
       " '     return dspy.Prediction(context=context, answer=answer)\\n```\\n\\n## Optimizing Typed Predictors\\n\\nTyped predictors can be optimized on the Signature instructions through the `optimize_signature` optimizer. Here is an example of this optimization on the `QASignature`:\\n\\n```python\\nimport dspy\\nfrom dspy.evaluate import Evaluate\\nfrom dspy.evaluate',\n",
       " \" import Evaluate\\nfrom dspy.evaluate.metrics import answer_exact_match\\nfrom dspy.teleprompt.signature_opt_typed import optimize_signature\\n\\nturbo = dspy.OpenAI(model='gpt-3.5-turbo', max_tokens=4000)\\ngpt4 = dspy.OpenAI(model='gpt-4', max_tokens=4000\",\n",
       " \"-4', max_tokens=4000)\\ndspy.settings.configure(lm=turbo)\\n\\nevaluator = Evaluate(devset=devset, metric=answer_exact_match, num_threads=10, display_progress=True)\\n\\nresult = optimize_signature(\\n    student=dspy.TypedPredictor(QASignature),\\n    evalu\",\n",
       " 'ASignature),\\n    evaluator=evaluator,\\n    initial_prompts=6,\\n    n_iterations=100,\\n    max_examples=30,\\n    verbose=True,\\n    prompt_model=gpt4,\\n)\\n```\\n---\\nsidebar_position: 5\\n---\\n\\n# Metrics\\n\\nDSPy is a machine',\n",
       " ' Metrics\\n\\nDSPy is a machine learning framework, so you must think about your **automatic metrics** for evaluation (to track your progress) and optimization (so DSPy can make your programs more effective).\\n\\n\\n## What is a metric and how do I define a metric for my task?\\n\\nA metric is just a function that will take examples from your data and take the output of your system, and return a score that quantifies how good the output is. What',\n",
       " ' that quantifies how good the output is. What makes outputs from your system good or bad? \\n\\nFor simple tasks, this could be just \"accuracy\" or \"exact match\" or \"F1 score\". This may be the case for simple classification or short-form QA tasks.\\n\\nHowever, for most applications, your system will output long-form outputs. There, your metric should probably be a smaller DSPy program that checks multiple properties of the output (',\n",
       " 'y program that checks multiple properties of the output (quite possibly using AI feedback from LMs).\\n\\nGetting this right on the first try is unlikely, but you should start with something simple and iterate. \\n\\n\\n## Simple metrics\\n\\nA DSPy metric is just a function in Python that takes `example` (e.g., from your training or dev set) and the output `pred` from your DSPy program, and outputs a `float` (or `',\n",
       " \", and outputs a `float` (or `int` or `bool`) score.\\n\\nYour metric should also accept an optional third argument called `trace`. You can ignore this for a moment, but it will enable some powerful tricks if you want to use your metric for optimization.\\n\\nHere's a simple example of a metric that's comparing `example.answer` and `pred.answer`. This particular metric will return a `bool`.\\n\\n```python\\ndef validate\",\n",
       " 'bool`.\\n\\n```python\\ndef validate_answer(example, pred, trace=None):\\n    return example.answer.lower() == pred.answer.lower()\\n```\\n\\nSome people find these utilities (built-in) convenient:\\n\\n- `dspy.evaluate.metrics.answer_exact_match`\\n- `dspy.evaluate.metrics.answer_passage_match`\\n\\nYour metrics could',\n",
       " \"passage_match`\\n\\nYour metrics could be more complex, e.g. check for multiple properties. The metric below will return a `float` if `trace is None` (i.e., if it's used for evaluation or optimization), and will return a `bool` otherwise (i.e., if it's used to bootstrap demonstrations).\\n\\n```python\\ndef validate_context_and_answer(example, pred, trace=None):\\n  \",\n",
       " \", pred, trace=None):\\n    # check the gold label and the predicted answer are the same\\n    answer_match = example.answer.lower() == pred.answer.lower()\\n\\n    # check the predicted answer comes from one of the retrieved contexts\\n    context_match = any((pred.answer.lower() in c) for c in pred.context)\\n\\n    if trace is None: # if we're doing\",\n",
       " \" if trace is None: # if we're doing evaluation or optimization\\n        return (answer_match + context_match) / 2.0\\n    else: # if we're doing bootstrapping, i.e. self-generating good demonstrations of each step\\n        return answer_match and context_match\\n```\\n\\nDefining a good metric is an iterative process, so doing some initial evaluations\",\n",
       " ' an iterative process, so doing some initial evaluations and looking at your data and your outputs are key.\\n\\n\\n## Evaluation\\n\\nOnce you have a metric, you can run evaluations in a simple Python loop.\\n\\n```python\\nscores = []\\nfor x in devset:\\n    pred = program(**x.inputs())\\n    score = metric(x, pred)\\n    scores.append(score)\\n```\\n',\n",
       " ' scores.append(score)\\n```\\n\\nIf you need some utilities, you can also use the built-in `Evaluate` utility. It can help with things like parallel evaluation (multiple threads) or showing you a sample of inputs/outputs and the metric scores.\\n\\n```python\\nfrom dspy.evaluate import Evaluate\\n\\n# Set up the evaluator, which can be re-used in your code.\\nevaluator = Eval',\n",
       " ' in your code.\\nevaluator = Evaluate(devset=YOUR_DEVSET, num_threads=1, display_progress=True, display_table=5)\\n\\n# Launch evaluation.\\nevaluator(YOUR_PROGRAM, metric=YOUR_METRIC)\\n```\\n\\n\\n## Intermediate: Using AI feedback for your metric\\n\\nFor most applications, your system will output long-form outputs, so your metric should',\n",
       " ' output long-form outputs, so your metric should check multiple dimensions of the output using AI feedback from LMs.\\n\\nThis simple signature could come in handy.\\n\\n```python\\n# Define the signature for automatic assessments.\\nclass Assess(dspy.Signature):\\n    \"\"\"Assess the quality of a tweet along the specified dimension.\"\"\"\\n\\n    assessed_text = dspy.InputField()\\n    assessment_question',\n",
       " 'InputField()\\n    assessment_question = dspy.InputField()\\n    assessment_answer = dspy.OutputField(desc=\"Yes or No\")\\n```\\n\\nFor example, below is a simple metric that uses GPT-4-turbo to check if a generated tweet (1) answers a given question correctly and (2) whether it\\'s also engaging. We also check that (3) `len(tweet) <= 280',\n",
       " '3) `len(tweet) <= 280` characters.\\n\\n```python\\ngpt4T = dspy.OpenAI(model=\\'gpt-4-1106-preview\\', max_tokens=1000, model_type=\\'chat\\')\\n\\ndef metric(gold, pred, trace=None):\\n    question, answer, tweet = gold.question, gold.answer, pred.output\\n\\n    engaging = \"Does',\n",
       " '.output\\n\\n    engaging = \"Does the assessed text make for a self-contained, engaging tweet?\"\\n    correct = f\"The text should answer `{question}` with `{answer}`. Does the assessed text contain this answer?\"\\n    \\n    with dspy.context(lm=gpt4T):\\n        correct =  dspy.Predict(Assess)(ass',\n",
       " \"spy.Predict(Assess)(assessed_text=tweet, assessment_question=correct)\\n        engaging = dspy.Predict(Assess)(assessed_text=tweet, assessment_question=engaging)\\n\\n    correct, engaging = [m.assessment_answer.lower() == 'yes' for m in [correct, engaging]]\\n    score = (correct + engaging) if\",\n",
       " '   score = (correct + engaging) if correct and (len(tweet) <= 280) else 0\\n\\n    if trace is not None: return score >= 2\\n    return score / 2.0\\n```\\n\\nWhen compiling, `trace is not None`, and we want to be strict about judging things, so we will only return `True` if `score >= 2`. Otherwise, we return a score out of 1.0 (i.e',\n",
       " \" score out of 1.0 (i.e., `score / 2.0`).\\n\\n\\n## Advanced: Using a DSPy program as your metric\\n\\nIf your metric is itself a DSPy program, one of the most powerful ways to iterate is to compile (optimize) your metric itself. That's usually easy because the output of the metric is usually a simple value (e.g., a score out of 5) so the metric's metric is easy to\",\n",
       " \" 5) so the metric's metric is easy to define and optimize by collecting a few examples.\\n\\n\\n\\n### Advanced: Accessing the `trace`\\n\\nWhen your metric is used during evaluation runs, DSPy will not try to track the steps of your program.\\n\\nBut during compiling (optimization), DSPy will trace your LM calls. The trace will contain inputs/outputs to each DSPy predictor and you can leverage that to validate intermediate steps for optimization.\",\n",
       " \" can leverage that to validate intermediate steps for optimization.\\n\\n\\n```python\\ndef validate_hops(example, pred, trace=None):\\n    hops = [example.question] + [outputs.query for *_, outputs in trace if 'query' in outputs]\\n\\n    if max([len(h) for h in hops]) > 100: return False\\n    if any(dspy.evaluate.answer_exact_match\",\n",
       " 'y.evaluate.answer_exact_match_str(hops[idx], hops[:idx], frac=0.8) for idx in range(2, len(hops))): return False\\n\\n    return True\\n```\\n---\\nsidebar_position: 6\\n---\\n\\n# Optimizers (formerly Teleprompters)\\n\\nA **DSPy optimizer** is an algorithm that can tune the parameters of a DSP',\n",
       " ' algorithm that can tune the parameters of a DSPy program (i.e., the prompts and/or the LM weights) to maximize the metrics you specify, like accuracy.\\n\\nThere are many built-in optimizers in DSPy, which apply vastly different strategies. A typical DSPy optimizer takes three things:\\n\\n- Your **DSPy program**. This may be a single module (e.g., `dspy.Predict`) or',\n",
       " ' `dspy.Predict`) or a complex multi-module program.\\n\\n- Your **metric**. This is a function that evaluates the output of your program, and assigns it a score (higher is better).\\n\\n- A few **training inputs**. This may be very small (i.e., only 5 or 10 examples) and incomplete (only inputs to your program, without any labels).\\n\\nIf you happen to have a lot of data, D',\n",
       " ' you happen to have a lot of data, DSPy can leverage that. But you can start small and get strong results.\\n\\n**Note:** Formerly called **DSPy Teleprompters**. We are making an official name update, which will be reflected throughout the library and documentation.\\n\\n\\n## **What** does a DSPy Optimizer tune? **How** does it tune them?\\n\\nTraditional deep neural networks (DNNs) can be optimized',\n",
       " ' neural networks (DNNs) can be optimized with gradient descent, given a loss function and some training data.\\n\\nDSPy programs consist of multiple calls to LMs, stacked together as [DSPy modules]. Each DSPy module has internal parameters of three kinds: (1) the LM weights, (2) the instructions, and (3) demonstrations of the input/output behavior.\\n\\nGiven a metric, DSPy can optimize all of these three with multi',\n",
       " \"SPy can optimize all of these three with multi-stage optimization algorithms. These can combine gradient descent (for LM weights) and discrete LM-driven optimization, i.e. for crafting/updating instructions and for creating/validating demonstrations. DSPy Demonstrations are like few-shot examples, but they're far more powerful. They can be created from scratch, given your program, and their creation and selection can be optimized in many effective ways.\\n\\nIn many cases,\",\n",
       " ' many effective ways.\\n\\nIn many cases, we found that compiling leads to better prompts than humans write. Not because DSPy optimizers are more creative than humans, but simply because they can try more things, much more systematically, and tune the metrics directly.\\n\\n\\n## What DSPy Optimizers are currently available?\\n\\nAll of these can be accessed via `from dspy.teleprompt import *`.\\n\\n#### Automatic Few-Shot Learning\\n\\n1.',\n",
       " '#### Automatic Few-Shot Learning\\n\\n1. **`LabeledFewShot`**: Simply constructs few-shot examples from provided labeled Q/A pairs.\\n\\n2. **`BootstrapFewShot`**: Uses your program to self-generate complete demonstrations for every stage of your program. Will simply use the generated demonstrations (if they pass the metric) without any further optimization. Advanced: Supports using a teacher program (a different DSPy program that has compatible structure)',\n",
       " ' different DSPy program that has compatible structure) and a teacher LM, for harder tasks.\\n\\n3. **`BootstrapFewShotWithRandomSearch`**: Applies `BootstrapFewShot` several times with random search over generated demonstrations, and selects the best program.\\n\\n4. **`BootstrapFewShotWithOptuna`**: Applies `BootstrapFewShot` through Optuna hyperparameter optimization across demonstration sets, running trials to maximize evaluation metrics.',\n",
       " ' demonstration sets, running trials to maximize evaluation metrics. \\n\\n\\n#### Automatic Instruction Optimization\\n\\n4. **`SignatureOptimizer`**: Generates and refines new instructions for each step, and optimizes them with coordinate ascent.\\n\\n5. **`BayesianSignatureOptimizer`**: Generates instructions and few-shot examples in each step. The instruction generation is data-aware and demonstration-aware. Uses Bayesian Optimization to effectively search over',\n",
       " '. Uses Bayesian Optimization to effectively search over the space of generation instructions/demonstrations across your modules.\\n\\n\\n#### Automatic Finetuning\\n\\n6. **`BootstrapFinetune`**: Distills a prompt-based DSPy program into weight updates (for smaller LMs). The output is a DSPy program that has the same steps, but where each step is conducted by a finetuned model instead of a prompted LM.\\n\\n\\n#### Program Transform',\n",
       " ' of a prompted LM.\\n\\n\\n#### Program Transformations\\n\\n7. **`KNNFewShot`**. Selects demonstrations through k-Nearest Neighbors algorithm integrating `BootstrapFewShot` for bootstrapping/selection process. \\n\\n8. **`Ensemble`**: Ensembles a set of DSPy programs and either uses the full set or randomly samples a subset into a single program.\\n\\n\\n## Which optimizer should I use?\\n\\n',\n",
       " \"## Which optimizer should I use?\\n\\nAs a rule of thumb, if you don't know where to start, use `BootstrapFewShotWithRandomSearch`.\\n\\nHere's the general guidance on getting started:\\n\\n* If you have very little data, e.g. 10 examples of your task, use `BootstrapFewShot`.\\n\\n* If you have slightly more data, e.g. 50 examples of your task, use `BootstrapFewShotWith\",\n",
       " ' your task, use `BootstrapFewShotWithRandomSearch`.\\n\\n* If you have more data than that, e.g. 300 examples or more, use `BayesianSignatureOptimizer`.\\n\\n* If you have been able to use one of these with a large LM (e.g., 7B parameters or above) and need a very efficient program, compile that down to a small LM with `BootstrapFinetune`.\\n\\n\\n## How do I use',\n",
       " 'etune`.\\n\\n\\n## How do I use an optimizer?\\n\\nThey all share this general interface, with some differences in the keyword arguments (hyperparameters).\\n\\nLet\\'s see this with the most common one, `BootstrapFewShotWithRandomSearch`.\\n\\n```python\\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch\\n\\n# Set up the optimizer: we want to \"bootstrap\" (i.e., self',\n",
       " ' \"bootstrap\" (i.e., self-generate) 8-shot examples of your program\\'s steps.\\n# The optimizer will repeat this 10 times (plus some initial attempts) before selecting its best attempt on the devset.\\nconfig = dict(max_bootstrapped_demos=3, max_labeled_demos=3, num_candidate_programs=10, num_threads=4)\\n\\nteleprompter = Boot',\n",
       " \"=4)\\n\\nteleprompter = BootstrapFewShotWithRandomSearch(metric=YOUR_METRIC_HERE, **config)\\noptimized_program = teleprompter.compile(YOUR_PROGRAM_HERE, trainset=YOUR_TRAINSET_HERE)\\n```\\n\\n## Saving and loading optimizer output\\n\\nAfter running a program through an optimizer, it's useful to also save it.\",\n",
       " \"izer, it's useful to also save it. At a later point, a program can be loaded from a file and used for inference. For this, the `load` and `save` methods can be used.\\n\\n### Saving a program\\n\\n```python\\noptimized_program.save(YOUR_SAVE_PATH)\\n```\\n\\nThe resulting file is in plain-text JSON format. It contains all the parameters and steps in the source program. You\",\n",
       " ' the parameters and steps in the source program. You can always read it and see what the optimizer generated.\\n\\n### Loading a program\\n\\nTo load a program from a file, you can instantiate an object from that class and then call the load method on it.\\n\\n```python\\nloaded_program = YOUR_PROGRAM_CLASS()\\nloaded_program.load(path=YOUR_SAVE_PATH)\\n```---\\nsidebar_position',\n",
       " \")\\n```---\\nsidebar_position: 2\\n---\\n\\n# Language Models\\n\\nThe most powerful features in DSPy revolve around algorithmically optimizing the prompts (or weights) of LMs, especially when you're building programs that use the LMs within a pipeline.\\n\\nLet's first make sure you can set up your language model. DSPy support clients for many remote and local LMs.\\n\\n## Setting up the LM client.\\n\\nYou\",\n",
       " \"## Setting up the LM client.\\n\\nYou can just call the constructor that connects to the LM. Then, use `dspy.configure` to declare this as the dexfault LM.\\n\\nFor example, to use OpenAI language models, you can do it as follows.\\n\\n```python\\ngpt3_turbo = dspy.OpenAI(model='gpt-3.5-turbo-1106', max_tok\",\n",
       " 'turbo-1106\\', max_tokens=300)\\ndspy.configure(lm=gpt3_turbo)\\n```\\n\\n## Directly calling the LM.\\n\\nYou can simply call the LM with a string to give it a raw prompt, i.e. a string.\\n\\n```python\\ngpt3_turbo(\"hello! this is a raw prompt to GPT-3.5\")\\n```',\n",
       " ' GPT-3.5\")\\n```\\n\\n**Output:**\\n```text\\n[\\'Hello! How can I assist you today?\\']\\n```\\n\\nThis is almost never the recommended way to interact with LMs in DSPy, but it is allowed.\\n\\n## Using the LM with DSPy signatures.\\n\\nYou can also use the LM via DSPy [`signature` (input/output spec)](https://dspy',\n",
       " '/output spec)](https://dspy-docs.vercel.app/docs/building-blocks/signatures) and [`modules`](https://dspy-docs.vercel.app/docs/building-blocks/modules), which we discuss in more depth in the remaining guides.\\n\\n```python\\n# Define a module (ChainOfThought) and assign it a signature (return an answer, given a question).\\nqa = d',\n",
       " ' answer, given a question).\\nqa = dspy.ChainOfThought(\\'question -> answer\\')\\n\\n# Run with the default LM configured with `dspy.configure` above.\\nresponse = qa(question=\"How many floors are in the castle David Gregory inherited?\")\\nprint(response.answer)\\n```\\n**Output:**\\n```text\\nThe castle David Gregory inherited has 7 floors.\\n```\\n\\n## Using multiple L',\n",
       " '.\\n```\\n\\n## Using multiple LMs at once.\\n\\nThe default LM above is GPT-3.5, `gpt3_turbo`. What if I want to run a piece of code with, say, GPT-4 or LLama-2?\\n\\nInstead of changing the default LM, you can just change it inside a block of code.\\n\\n**Tip:** Using `dspy.configure` and `dspy',\n",
       " 'y.configure` and `dspy.context` is thread-safe!\\n\\n```python\\n# Run with the default LM configured above, i.e. GPT-3.5\\nresponse = qa(question=\"How many floors are in the castle David Gregory inherited?\")\\nprint(\\'GPT-3.5:\\', response.answer)\\n\\ngpt4_turbo = dspy.OpenAI(model=\\'gpt-4',\n",
       " '.OpenAI(model=\\'gpt-4-1106-preview\\', max_tokens=300)\\n\\n# Run with GPT-4 instead\\nwith dspy.context(lm=gpt4_turbo):\\n    response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\\n    print(\\'GPT-4-turbo:\\', response.answer)\\n```\\n**',\n",
       " \"', response.answer)\\n```\\n**Output:**\\n```text\\nGPT-3.5: The castle David Gregory inherited has 7 floors.\\nGPT-4-turbo: The number of floors in the castle David Gregory inherited cannot be determined with the information provided.\\n```\\n\\n## Tips and Tricks.\\n\\nIn DSPy, all LM calls are cached. If you repeat the same call, you will\\nget the same outputs.\",\n",
       " ' call, you will\\nget the same outputs. (If you change the inputs or configurations, you\\nwill get new outputs.)\\n\\nTo generate 5 outputs, you can use `n=5` in the module constructor, or\\npass `config=dict(n=5)` when invoking the module.\\n\\n```python\\nqa = dspy.ChainOfThought(\\'question -> answer\\', n=5)\\n\\nresponse = qa(question=\"How many',\n",
       " '\\nresponse = qa(question=\"How many floors are in the castle David Gregory inherited?\")\\nresponse.completions.answer\\n```\\n**Output:**\\n```text\\n[\"The specific number of floors in David Gregory\\'s inherited castle is not provided here, so further research would be needed to determine the answer.\",\\n    \\'The castle David Gregory inherited has 4 floors.\\',\\n    \\'The castle David Gregory inherited has 5 floors.\\',\\n',\n",
       " \" castle David Gregory inherited has 5 floors.',\\n    'David Gregory inherited 10 floors in the castle.',\\n    'The castle David Gregory inherited has 5 floors.']\\n```\\n\\nIf you just call `qa(...)` in a loop with the same input, it will always\\nreturn the same value! That\\\\'s by design.\\n\\nTo loop and generate one output at a time with the same input, bypass\\nthe cache by making sure each\",\n",
       " ' input, bypass\\nthe cache by making sure each request is (slightly) unique, as below.\\n\\n```python\\nfor idx in range(5):\\n    response = qa(question=\"How many floors are in the castle David Gregory inherited?\", config=dict(temperature=0.7+0.0001*idx))\\n    print(f\\'{idx+1}.\\', response.answer)\\n```\\n**',\n",
       " \", response.answer)\\n```\\n**Output:**\\n```text\\n1. The specific number of floors in David Gregory's inherited castle is not provided here, so further research would be needed to determine the answer.\\n2. It is not possible to determine the exact number of floors in the castle David Gregory inherited without specific information about the castle's layout and history.\\n3. The castle David Gregory inherited has 5 floors.\\n4. We need more information to determine the\",\n",
       " '\\n4. We need more information to determine the number of floors in the castle David Gregory inherited.\\n5. The castle David Gregory inherited has a total of 6 floors.\\n```\\n\\n## Remote LMs.\\n\\nThese models are managed services. You just need to sign up and obtain an API key. Calling any of the remote LMs below assumes authentication and mirrors the following format for setting up the LM:\\n\\n```python\\nlm = dspy.{',\n",
       " 'python\\nlm = dspy.{provider_listed_below}(model=\"your model\", model_request_kwargs=\"...\")\\n```\\n\\n1.  `dspy.OpenAI` for GPT-3.5 and GPT-4.\\n\\n2.  `dspy.Cohere`\\n\\n3.  `dspy.Anyscale` for hosted Llama2 models.\\n\\n4. `',\n",
       " ' Llama2 models.\\n\\n4. `dspy.Together` for hosted various open source models.\\n\\n\\n### Local LMs.\\n\\nYou need to host these models on your own GPU(s). Below, we include pointers for how to do that.\\n\\n1.  `dspy.HFClientTGI`: for HuggingFace models through the Text Generation Inference (TGI) system. [Tutorial: How do I install and launch the',\n",
       " 'Tutorial: How do I install and launch the TGI server?](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/local_models/HFClientTGI)\\n\\n```python\\ntgi_llama2 = dspy.HFClientTGI(model=\"meta-llama/Llama-2-7b-hf\", port=8080, url=\"http',\n",
       " 'f\", port=8080, url=\"http://localhost\")\\n```\\n\\n2.  `dspy.HFClientVLLM`: for HuggingFace models through vLLM. [Tutorial: How do I install and launch the vLLM server?](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/local_models/HFClientVLLM)\\n\\n``',\n",
       " '/HFClientVLLM)\\n\\n```python\\nvllm_llama2 = dspy.HFClientVLLM(model=\"meta-llama/Llama-2-7b-hf\", port=8080, url=\"http://localhost\")\\n```\\n\\n3.  `dspy.HFModel` (experimental) [Tutorial: How do I initialize models using HFModel](https://dspy-',\n",
       " \" using HFModel](https://dspy-docs.vercel.app/api/local_language_model_clients/HFModel)\\n\\n```python\\nllama = dspy.HFModel(model = 'meta-llama/Llama-2-7b-hf')\\n```\\n\\n4.  `dspy.Ollama` (experimental) for open source models through [Ollama](https://\",\n",
       " ' source models through [Ollama](https://ollama.com). [Tutorial: How do I install and use Ollama on a local computer?](https://dspy-docs.vercel.app/api/local_language_model_clients/Ollama)\\\\n\",\\n\\n```python\\nmistral_ollama = dspy.OllamaLocal(model=\\'mistral\\')\\n```\\n\\n5.  `d',\n",
       " \"\\n```\\n\\n5.  `dspy.ChatModuleClient` (experimental): [How do I install and use MLC?](https://dspy-docs.vercel.app/api/local_language_model_clients/MLC)\\n\\n```python\\nmodel = 'dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1'\\n\",\n",
       " \"f-q4f16_1'\\nmodel_path = 'dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so'\\n\\nllama = dspy.ChatModuleClient(model=model, model_path=model_path)\\n```\\n---\\nsidebar_position: 5\\n---\\n\\n# Data\\n\\nDSPy is a machine learning\",\n",
       " ' Data\\n\\nDSPy is a machine learning framework, so working in it involves training sets, development sets, and test sets.\\n\\nFor each example in your data, we distinguish typically between three types of values: the inputs, the intermediate labels, and the final label. You can use DSPy effectively without any intermediate or final labels, but you will need at least a few example inputs.\\n\\n## How much data do I need and how do I collect data for my task',\n",
       " ' need and how do I collect data for my task?\\n\\nConcretely, you can use DSPy optimizers usefully with as few as 10 example inputs, but having 50-100 examples (or even better, 300-500 examples) goes a long way.\\n\\nHow can you get examples like these? If your task is extremely unusual, please invest in preparing ~10 examples by hand. Often times, depending on your metric below, you just need inputs and not labels,',\n",
       " \" below, you just need inputs and not labels, so it's not that hard.\\n\\nHowever, chances are that your task is not actually that unique. You can almost always find somewhat adjacent datasets on, say, HuggingFace datasets or other forms of data that you can leverage here.\\n\\nIf there's data whose licenses are permissive enough, we suggest you use them. Otherwise, you can also start using/deploying/demoing your system and collect some initial data that\",\n",
       " 'demoing your system and collect some initial data that way.\\n\\n## DSPy `Example` objects\\n\\nThe core data type for data in DSPy is `Example`. You will use **Examples** to represent items in your training set and test set. \\n\\nDSPy **Examples** are similar to Python `dict`s but have a few useful utilities. Your DSPy modules will return values of the type `Prediction`, which is a special sub-',\n",
       " ' `Prediction`, which is a special sub-class of `Example`.\\n\\nWhen you use DSPy, you will do a lot of evaluation and optimization runs. Your individual datapoints will be of type `Example`:\\n\\n```python\\nqa_pair = dspy.Example(question=\"This is a question?\", answer=\"This is an answer.\")\\n\\nprint(qa_pair)\\nprint(qa_pair.question)\\nprint(qa',\n",
       " \"qa_pair.question)\\nprint(qa_pair.answer)\\n```\\n**Output:**\\n```text\\nExample({'question': 'This is a question?', 'answer': 'This is an answer.'}) (input_keys=None)\\nThis is a question?\\nThis is an answer.\\n```\\n\\nExamples can have any field keys and any value types, though usually values are strings.\\n\\n```text\\nobject = Example\",\n",
       " '.\\n\\n```text\\nobject = Example(field1=value1, field2=value2, field3=value3, ...)\\n```\\n\\nYou can now express your training set for example as:\\n\\n```python\\ntrainset = [dspy.Example(report=\"LONG REPORT 1\", summary=\"short summary 1\"), ...]\\n```\\n\\n\\n### Specifying Input Keys\\n\\nIn traditional ML, there are separated \"input',\n",
       " '\\nIn traditional ML, there are separated \"inputs\" and \"labels\".\\n\\nIn DSPy, the `Example` objects have a `with_inputs()` method, which can mark specific fields as inputs. (The rest are just metadata or labels.)\\n\\n```python\\n# Single Input.\\nprint(qa_pair.with_inputs(\"question\"))\\n\\n# Multiple Inputs; be careful about marking your labels as inputs unless you mean it',\n",
       " ' about marking your labels as inputs unless you mean it.\\nprint(qa_pair.with_inputs(\"question\", \"answer\"))\\n```\\n\\nValues can be accessed using the `.`(dot) operator. You can access the value of key `name` in defined object `Example(name=\"John Doe\", job=\"sleep\")` through `object.name`. \\n\\nTo access or exclude certain keys, use `inputs()` and `labels()',\n",
       " ' `inputs()` and `labels()` methods to return new Example objects containing only input or non-input keys, respectively.\\n\\n```python\\narticle_summary = dspy.Example(article= \"This is an article.\", summary= \"This is a summary.\").with_inputs(\"article\")\\n\\ninput_key_only = article_summary.inputs()\\nnon_input_key_only = article_summary.labels()\\n',\n",
       " 'only = article_summary.labels()\\n\\nprint(\"Example object with Input fields only:\", input_key_only)\\nprint(\"Example object with Non-Input fields only:\", non_input_key_only)\\n```\\n\\n**Output**\\n```\\nExample object with Input fields only: Example({\\'article\\': \\'This is an article.\\'}) (input_keys=None)\\nExample object with Non-Input fields only: Example({\\'summary',\n",
       " \" Non-Input fields only: Example({'summary': 'This is a summary.'}) (input_keys=None)\\n```\\n---\\nsidebar_position: 1\\n---\\n\\n# Using DSPy in 8 Steps\\n\\nUsing DSPy well for solving a new task is just doing good machine learning with LMs.\\n\\nWhat this means is that it's an iterative process. You make some initial choices, which will be sub-optimal, and\",\n",
       " ', which will be sub-optimal, and then you refine them incrementally. \\n\\nAs we discuss below, you will define your task and the metrics you want to maximize, and prepare a few example inputs — typically without labels (or only with labels for the final outputs, if your metric requires them). Then, you build your pipeline by selecting built-in layers [(`modules`)](https://dspy-docs.vercel.app/docs/building-blocks',\n",
       " 'vercel.app/docs/building-blocks/modules) to use, giving each layer a [`signature` (input/output spec)](https://dspy-docs.vercel.app/docs/building-blocks/signatures), and then calling your modules freely in your Python code. Lastly, you use a DSPy [`optimizer`](https://dspy-docs.vercel.app/docs/building-blocks/optimizers',\n",
       " \"app/docs/building-blocks/optimizers) to compile your code into high-quality instructions, automatic few-shot examples, or updated LM weights for your LM.\\n\\n\\n## 1) Define your task.\\n\\nYou cannot use DSPy well if you haven't defined the problem you're trying to solve.\\n\\n**Expected Input/Output Behavior:** Are you trying to build a chatbot over your data? A code assistant? A system for extracting information from\",\n",
       " \" A code assistant? A system for extracting information from papers? Or perhaps a translation system? Or a system for highlighting snippets from search results? Or a system to summarize information on a topic, with citations?\\n\\nIt's often useful to come up with just 3-4 examples of the inputs and outputs of your program (e.g., questions and their answers, or topics and their summaries).\\n\\nIf you need help thinking about your task, we recently created a [Discord server\",\n",
       " \" task, we recently created a [Discord server](https://discord.gg/VzS6RHHK6F) for the community.\\n\\n**Quality and Cost Specs:** You probably don't have infinite budget. Your final system can't be too expensive to run, and it should probably respond to users quickly enough.\\n\\nTake this as an opportunity to guess what kind of language model you'd like to use. Maybe GPT-3.5? Or\",\n",
       " '. Maybe GPT-3.5? Or a small open model, like Mistral-7B or Llama2-13B-chat? Or Mixtral? Or maybe you really need GPT-4-turbo? Or perhaps your resources are very constrained, and you want your final LM to be T5-base.\\n\\n## 2) Define your pipeline.\\n\\nWhat should your DSPy program do? Can it just be a simple chain-of',\n",
       " '? Can it just be a simple chain-of-thought step? Or do you need the LM to use retrieval? Or maybe other tools, like a calculator or a calendar API?\\n\\nIs there a typical workflow for solving your problem in multiple well-defined steps? Or do you want a fully open-ended LM (or open-ended tool use with agents) for your task?\\n\\nThink about this space but always start simple. Almost every task should probably start with just a single',\n",
       " ' Almost every task should probably start with just a single [dspy.ChainofThought](https://dspy-docs.vercel.app/api/modules/ChainOfThought) module, and then add complexity incrementally as you go.\\n\\nThen write your (initial) DSPy program. Again: start simple, and let the next few steps guide any complexity you will add.\\n\\n## 3) Explore a few examples.\\n\\nBy this point',\n",
       " \" Explore a few examples.\\n\\nBy this point, you probably have a few examples of the task you're trying to solve.\\n\\nRun them through your pipeline. Consider using a large and powerful LM at this point, or a couple of different LMs, just to understand what's possible. (DSPy will make swapping these LMs pretty easy - [LM Guide](https://dspy-docs.vercel.app/docs/building-blocks/language_models).\",\n",
       " \"docs/building-blocks/language_models).)\\n\\nAt this point, you're still using your pipeline zero-shot, so it will be far from perfect. DSPy will help you optimize the instructions, few-shot examples, and even weights of your LM calls below, but understanding where things go wrong in zero-shot usage will go a long way.\\n\\nRecord the interesting (both easy and hard) examples you try: even if you don't have labels, simply\",\n",
       " \": even if you don't have labels, simply tracking the inputs you tried will be useful for DSPy optimizers below.\\n\\n## 4) Define your data.\\n\\nNow it's time to more formally declare your training and validation data for DSPy evaluation and optimization - [Data Guide](https://dspy-docs.vercel.app/docs/building-blocks/data).\\n\\nYou can use DSPy optimizers usefully with as few as\",\n",
       " \"SPy optimizers usefully with as few as 10 examples, but having 50-100 examples (or even better, 300-500 examples) goes a long way.\\n\\nHow can you get examples like these? If your task is extremely unusual, please invest in preparing ~10 examples by hand. Often times, depending on your metric below, you just need inputs and not labels, so it's not that hard.\\n\\nHowever, chances are that your task is not actually that unique.\",\n",
       " \" are that your task is not actually that unique. You can almost always find somewhat adjacent datasets on, say, HuggingFace datasets or other forms of data that you can leverage here.\\n\\nIf there's data whose licenses are permissive enough, we suggest you use them. Otherwise, you can also start using/deploying/demoing your system and collect some initial data that way.\\n\\n\\n## 5) Define your metric.\\n\\nWhat makes outputs from your system good or\",\n",
       " \"\\n\\nWhat makes outputs from your system good or bad? Invest in defining metrics and improving them over time incrementally. It's really hard to consistently improve what you aren't able to define.\\n\\nA metric is just a function that will take examples from your data and take the output of your system, and return a score that quantifies how good the output is - [Metric Guide](https://dspy-docs.vercel.app/docs/building-blocks/met\",\n",
       " '.app/docs/building-blocks/metrics).\\n\\nFor simple tasks, this could be just \"accuracy\" or \"exact match\" or \"F1 score\". This may be the case for simple classification or short-form QA tasks.\\n\\nHowever, for most applications, your system will output long-form outputs. There, your metric should probably be a smaller DSPy program that checks multiple properties of the output (quite possibly using AI feedback from LMs',\n",
       " \" output (quite possibly using AI feedback from LMs).\\n\\nGetting this right on the first try is unlikely, but you should start with something simple and iterate. (If your metric is itself a DSPy program, notice that one of the most powerful ways to iterate is to compile (optimize) your metric itself. That's usually easy because the output of the metric is usually a simple value (e.g., a score out of 5) so the metric's metric is\",\n",
       " ' out of 5) so the metric\\'s metric is easy to define and optimize by collecting a few examples.)\\n\\n\\n## 6) Collect preliminary \"zero-shot\" evaluations.\\n\\nNow that you have some data and a metric, run evaluation on your pipeline before any optimizer runs.\\n\\nLook at the outputs and the metric scores. This will probably allow you to spot any major issues, and it will define a baseline for your next step.\\n\\n## 7) Compile with a',\n",
       " '.\\n\\n## 7) Compile with a DSPy optimizer.\\n\\nGiven some data and a metric, we can now optimize the program you built - [Optimizer Guide](https://dspy-docs.vercel.app/docs/building-blocks/optimizers).\\n\\nDSPy includes many optimizers that do different things. Remember: DSPy optimizers will create examples of each step, craft instructions, and/or update LM weights.',\n",
       " \" craft instructions, and/or update LM weights. In general, you don't need to have labels for your pipeline steps, but your data examples need to have input values and whatever labels your metric requires (e.g., no labels if your metric is reference-free, but final output labels otherwise in most cases).\\n\\nHere's the general guidance on getting started:\\n\\n* If you have very little data, e.g. 10 examples of your task, use [`Bootstrap\",\n",
       " ' examples of your task, use [`BootstrapFewShot`](https://dspy-docs.vercel.app/docs/deep-dive/teleprompter/bootstrap-fewshot)\\n\\n* If you have slightly more data, e.g. 50 examples of your task, use [`BootstrapFewShotWithRandomSearch`](https://dspy-docs.vercel.app/docs/deep-dive/teleprompter/',\n",
       " '/deep-dive/teleprompter/bootstrap-fewshot).\\n\\n* If you have more data than that, e.g. 300 examples or more, use [`BayesianSignatureOptimizer`](https://dspy-docs.vercel.app/docs/deep-dive/teleprompter/signature-optimizer).\\n\\n* If you have been able to use one of these with a large LM (e.g.,',\n",
       " \" these with a large LM (e.g., 7B parameters or above) and need a very efficient program, compile that down to a small LM with `BootstrapFinetune`.\\n\\n## 8) Iterate.\\n\\nAt this point, you are either very happy with everything (we've seen quite a few people get it right on first try with DSPy) or, more likely, you've made a lot of progress but you don't like something about the final program\",\n",
       " \" but you don't like something about the final program or the metric.\\n\\nAt this point, go back to step 1 and revisit the major questions. Did you define your task well? Do you need to collect (or find online) more data for your problem? Do you want to update your metric? And do you want to use a more sophisticated optimizer? Do you need to consider advanced features like [DSPy Assertions](https://dspy-docs.vercel\",\n",
       " 'https://dspy-docs.vercel.app/docs/building-blocks/assertions)? Or, perhaps most importantly, do you want to add some more complexity or steps in your DSPy program itself? Do you want to use multiple optimizers in a sequence?\\n\\nIterative development is key. DSPy gives you the pieces to do that incrementally: iterating on your data, your program structure, your assertions, your metric, and your optimization steps.',\n",
       " ' assertions, your metric, and your optimization steps.\\n\\nOptimizing complex LM programs is an entirely new paradigm that only exists in DSPy at the time of writing, so naturally the norms around what to do are still emerging. If you need help, we recently created a [Discord server](https://discord.gg/VzS6RHHK6F) for the community.# Understanding Typed Predictors\\n\\n## Why use a Typed Predictor?\\n',\n",
       " '## Why use a Typed Predictor?\\n\\n## How to use a Typed Predictor?\\n\\n## Prompt of Typed Predictors\\n\\n## How Typed Predictors work?# Functional Typed Predictors\\n\\n## Typed Predictors as Decorators\\n\\n## Functional Typed Predictors in `dspy.Module`\\n\\n## How Functional Typed Predictors work?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.add(ids=[str(uuid.uuid4()) for _ in range(len(split_docs))], documents=split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_model = ChromadbRM(\"docs\", \"./chroma\", embedding_function=default_ef, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.settings.configure(lm=llm, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"?\\ni'm doing well, thanks for asking. I hope you're doing okay too!\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"hi how are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resp = requests.get('https://raw.githubusercontent.com/weaviate-tutorials/quickstart/main/data/jeopardy_tiny.json')\n",
    "# data = json.loads(resp.text)  \n",
    "\n",
    "# question_objs = list()\n",
    "# for i, d in enumerate(data):\n",
    "#     question_objs.append({\n",
    "#         \"answer\": d[\"Answer\"],\n",
    "#         \"question\": d[\"Question\"],\n",
    "#         \"category\": d[\"Category\"],\n",
    "\n",
    "# questions = weaviate_client.collections.get(\"Question\")\n",
    "# questions.data.insert_many(question_objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionToBlogOutline(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Your task is to write a blog post that will help answer the given question. \n",
    "    Please use the contexts to evaluate the structure of the blog post.\n",
    "    \"\"\"\n",
    "\n",
    "    question = dspy.InputField()\n",
    "    context = dspy.InputField()\n",
    "    blog_outline = dspy.OutputField(desc=\"A comma separated list of topics. IMPORTANT!! ONLY SEPARATE LIST ITEMS WITH A COMMA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "691"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(collection.get(include=['embeddings'])['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. Introduction to DSPy and its purpose\\n2. Understanding data requirements for training, development, and testing sets\\n3. Overview of DSPy modules and how to use them\\n4. Tips for optimizing models using DSPy's automatically-optimizing programming model\\n5. Best practices for implementing short programs with DSPy\\n6. Resources for further learning and support\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_question = \"What is DSPy used for?\"\n",
    "ex_context = dspy.Retrieve(k=5)(ex_question).passages\n",
    "ex_context = \"\".join(ex_context)\n",
    "dspy.ChainOfThought(QuestionToBlogOutline)(question=ex_question, context=ex_context).blog_outline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'re willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\\n\\n**DSPy vs to the API References for DSPy! This is where you'll find easy-to-understand information about all the parts of DSPy that you can use in your projects. We've got guides on different tools and helpers that DSPy has, like modules and optimizers. Everything is sorted so you can quickly find what you need. If you're making something and need to quickly get started with DSPy to do certain tasks, this place will show you how to set it Data\\n\\nDSPy is a machine learning framework, so working in it involves training sets, development sets, and test sets.\\n\\nFor each example in your data, we distinguish typically between three types of values: the inputs, the intermediate labels, and the final label. You can use DSPy effectively without any intermediate or final labels, but you will need at least a few example inputs.\\n\\n## How much data do I need and how do I collect data for my tasky** philosophy and abstraction differ significantly from other libraries and frameworks, so it's usually straightforward to decide when **DSPy** is (or isn't) the right framework for your usecase. If you're a NLP/AI researcher (or a practitioner exploring new pipelines or new tasks), the answer is generally an invariable **yes**. If you're a practitioner doing other things, please read on.\\n\\n**DSPy vs. thin wrappers for prompts (OpenAI`\\n\\n\\n## What other DSPy modules are there? How can I use them?\\n\\nThe others are very similar. They mainly change the internal behavior with which your signature is implemented!\\n\\n1. **`dspy.Predict`**: Basic predictor. Does not modify the signature. Handles the key forms of learning (i.e., storing the instructions and demonstrations and updates to the LM).\\n\\n2. **`dspy.ChainOfTh\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. Introduction to DSPy and its purpose\\n2. Understanding data requirements for training, development, and testing sets\\n3. Overview of DSPy modules and how to use them\\n4. Tips for optimizing models using DSPy's automatically-optimizing programming model\\n5. Best practices for implementing short programs with DSPy\\n6. Resources for further learning and support\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cot = dspy.ChainOfThought(QuestionToBlogOutline)\n",
    "cot(question=ex_question, context=ex_context).blog_outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChainOfThought(QuestionToBlogOutline(question, context -> blog_outline\n",
       "    instructions='\\n    Your task is to write a blog post that will help answer the given question. \\n    Please use the contexts to evaluate the structure of the blog post.\\n    '\n",
       "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
       "    context = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Context:', 'desc': '${context}'})\n",
       "    blog_outline = Field(annotation=str required=True json_schema_extra={'desc': 'A comma separated list of topics. IMPORTANT!! ONLY SEPARATE LIST ITEMS WITH A COMMA', '__dspy_field_type': 'output', 'prefix': 'Blog Outline:'})\n",
       "))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'long_text': \"'re willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\\n\\n**DSPy vs\"},\n",
       " {'long_text': ' Data\\n\\nDSPy is a machine learning framework, so working in it involves training sets, development sets, and test sets.\\n\\nFor each example in your data, we distinguish typically between three types of values: the inputs, the intermediate labels, and the final label. You can use DSPy effectively without any intermediate or final labels, but you will need at least a few example inputs.\\n\\n## How much data do I need and how do I collect data for my task'},\n",
       " {'long_text': \"y** philosophy and abstraction differ significantly from other libraries and frameworks, so it's usually straightforward to decide when **DSPy** is (or isn't) the right framework for your usecase. If you're a NLP/AI researcher (or a practitioner exploring new pipelines or new tasks), the answer is generally an invariable **yes**. If you're a practitioner doing other things, please read on.\\n\\n**DSPy vs. thin wrappers for prompts (OpenAI\"},\n",
       " {'long_text': \" to the API References for DSPy! This is where you'll find easy-to-understand information about all the parts of DSPy that you can use in your projects. We've got guides on different tools and helpers that DSPy has, like modules and optimizers. Everything is sorted so you can quickly find what you need. If you're making something and need to quickly get started with DSPy to do certain tasks, this place will show you how to set it\"},\n",
       " {'long_text': 'xiv.org/abs/2310.03714) | N/A | Sections 3, 5, 6, and 7 of the DSPy paper can be consumed as a tutorial. They include explained code snippets, results, and discussions of the abstractions and API.\\n| Intermediate | [**DSPy Assertions**](https://arxiv.org/abs/2312.13382) | [<img align=\"center\" src=\"https://'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_model(\"What is dspy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task is to write a blog post that will help answer the given question. \n",
      "    Please use the contexts to evaluate the structure of the blog post.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Context: ${context}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the blog_outline}. We ...\n",
      "\n",
      "Blog Outline: A comma separated list of topics. IMPORTANT!! ONLY SEPARATE LIST ITEMS WITH A COMMA\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is DSPy used for?\n",
      "\n",
      "Context:\n",
      "'re willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs to the API References for DSPy! This is where you'll find easy-to-understand information about all the parts of DSPy that you can use in your projects. We've got guides on different tools and helpers that DSPy has, like modules and optimizers. Everything is sorted so you can quickly find what you need. If you're making something and need to quickly get started with DSPy to do certain tasks, this place will show you how to set it Data\n",
      "\n",
      "DSPy is a machine learning framework, so working in it involves training sets, development sets, and test sets.\n",
      "\n",
      "For each example in your data, we distinguish typically between three types of values: the inputs, the intermediate labels, and the final label. You can use DSPy effectively without any intermediate or final labels, but you will need at least a few example inputs.\n",
      "\n",
      "## How much data do I need and how do I collect data for my tasky** philosophy and abstraction differ significantly from other libraries and frameworks, so it's usually straightforward to decide when **DSPy** is (or isn't) the right framework for your usecase. If you're a NLP/AI researcher (or a practitioner exploring new pipelines or new tasks), the answer is generally an invariable **yes**. If you're a practitioner doing other things, please read on.\n",
      "\n",
      "**DSPy vs. thin wrappers for prompts (OpenAI`\n",
      "\n",
      "\n",
      "## What other DSPy modules are there? How can I use them?\n",
      "\n",
      "The others are very similar. They mainly change the internal behavior with which your signature is implemented!\n",
      "\n",
      "1. **`dspy.Predict`**: Basic predictor. Does not modify the signature. Handles the key forms of learning (i.e., storing the instructions and demonstrations and updates to the LM).\n",
      "\n",
      "2. **`dspy.ChainOfTh\n",
      "\n",
      "Reasoning: Let's think step by step in order to use DSPy for machine learning tasks. First, we need to understand what types of data are required for training, development, and testing sets. We distinguish between inputs, intermediate labels, and final labels. The amount of data needed may vary depending on the task, but it's generally straightforward to decide whether DSPy is the right framework for NLP/AI research or practitioners exploring new pipelines or tasks. For other use cases, we recommend reading further.\n",
      "\n",
      "Blog Outline:\u001b[32m 1. Introduction to DSPy and its purpose\n",
      "2. Understanding data requirements for training, development, and testing sets\n",
      "3. Overview of DSPy modules and how to use them\n",
      "4. Tips for optimizing models using DSPy's automatically-optimizing programming model\n",
      "5. Best practices for implementing short programs with DSPy\n",
      "6. Resources for further learning and support\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicToParagraph(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
    "    \"\"\"\n",
    "\n",
    "    topic = dspy.InputField(desc=\"A topic to write a paragraph about based on the information given.\")\n",
    "    context = dspy.InputField(desc=\"contains relevant information about the topic.\")\n",
    "    paragraph = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"DSPy is a lightweight and automatically-optimizing programming model that provides a lower-level alternative to higher-level libraries like HuggingFace Transformers in neural networks. Its API references are easily accessible, making it easy to find information about its parts for use in projects. Sections 3, 5, 6, and 7 of the DSPy paper can be consumed as a tutorial, providing explained code snippets, results, and discussions of abstractions and API. DSPy optimizers can be used with as few as 10 example inputs, but having more examples goes a long way. Other modules in DSPy, such as `Predict` and `ChainOfThought`, mainly change the internal behavior of signature implementation. The `Predict` module does not modify the signature and handles learning forms like storing instructions, demonstrations, and updates to the LM, while the `ChainOfThought` module adds a chain-of-thought reasoning process to the model's output.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_topic = \"An Overview of DSPy's features.\"\n",
    "ex_topic_context = dspy.Retrieve(k=5)(ex_topic).passages\n",
    "ex_topic_context = \"\".join(ex_topic_context)\n",
    "dspy.ChainOfThought(TopicToParagraph)(topic=ex_topic, context=ex_topic_context).paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: An Overview of DSPy's features.\n",
      "\n",
      "Context:\n",
      "'re willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs to the API References for DSPy! This is where you'll find easy-to-understand information about all the parts of DSPy that you can use in your projects. We've got guides on different tools and helpers that DSPy has, like modules and optimizers. Everything is sorted so you can quickly find what you need. If you're making something and need to quickly get started with DSPy to do certain tasks, this place will show you how to set itxiv.org/abs/2310.03714) | N/A | Sections 3, 5, 6, and 7 of the DSPy paper can be consumed as a tutorial. They include explained code snippets, results, and discussions of the abstractions and API.\n",
      "| Intermediate | [**DSPy Assertions**](https://arxiv.org/abs/2312.13382) | [<img align=\"center\" src=\"https:// need and how do I collect data for my task?\n",
      "\n",
      "Concretely, you can use DSPy optimizers usefully with as few as 10 example inputs, but having 50-100 examples (or even better, 300-500 examples) goes a long way.\n",
      "\n",
      "How can you get examples like these? If your task is extremely unusual, please invest in preparing ~10 examples by hand. Often times, depending on your metric below, you just need inputs and not labels,`\n",
      "\n",
      "\n",
      "## What other DSPy modules are there? How can I use them?\n",
      "\n",
      "The others are very similar. They mainly change the internal behavior with which your signature is implemented!\n",
      "\n",
      "1. **`dspy.Predict`**: Basic predictor. Does not modify the signature. Handles the key forms of learning (i.e., storing the instructions and demonstrations and updates to the LM).\n",
      "\n",
      "2. **`dspy.ChainOfTh\n",
      "\n",
      "Reasoning: Let's think step by step in order to provide an overview of DSPy's features based on the given context. Firstly, we need to understand that DSPy is a lightweight and automatically-optimizing programming model for when we require it, not a library of predefined prompts and integrations. It can be compared to PyTorch in neural networks, while higher-level libraries like HuggingFace Transformers represent the latter. Secondly, we can refer to the API references for DSPy to easily find information about its parts that we can use in our projects. Thirdly, we can consume sections 3, 5, 6, and 7 of the DSPy paper as a tutorial, which includes explained code snippets, results, and discussions of abstractions and API. Fourthly, we can use DSPy optimizers with as few as 10 example inputs, but having more examples goes a long way. Lastly, we can get examples by hand if our task is extremely unusual, or we may not even need labels for some metrics. Other modules in DSPy are similar and mainly change the internal behavior of the signature implementation. The `Predict` module does not modify the signature and handles learning forms like storing instructions, demonstrations, and updates to the LM. The `ChainOfThought` module adds a chain-of-thought reasoning process to the model's output.\n",
      "\n",
      "Paragraph:\u001b[32m DSPy is a lightweight and automatically-optimizing programming model that provides a lower-level alternative to higher-level libraries like HuggingFace Transformers in neural networks. Its API references are easily accessible, making it easy to find information about its parts for use in projects. Sections 3, 5, 6, and 7 of the DSPy paper can be consumed as a tutorial, providing explained code snippets, results, and discussions of abstractions and API. DSPy optimizers can be used with as few as 10 example inputs, but having more examples goes a long way. Other modules in DSPy, such as `Predict` and `ChainOfThought`, mainly change the internal behavior of signature implementation. The `Predict` module does not modify the signature and handles learning forms like storing instructions, demonstrations, and updates to the LM, while the `ChainOfThought` module adds a chain-of-thought reasoning process to the model's output.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProofRead(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Proofread a blog post and output a more well written version of the original post.\n",
    "    \"\"\"\n",
    "\n",
    "    blog_post = dspy.InputField()\n",
    "    proofread_blog_post = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleGenerator(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Write a title for a blog post given a description of the topics the blog covers as input.\n",
    "    \"\"\"\n",
    "\n",
    "    blog_outline = dspy.InputField()\n",
    "    title = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining for iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlogPostWriter(dspy.Module):\n",
    "    def __init__(self): \n",
    "        self.question_to_blog_outline = dspy.ChainOfThought(QuestionToBlogOutline)\n",
    "        self.topic_to_paragraph = dspy.ChainOfThought(TopicToParagraph)\n",
    "        self.proof_reader = dspy.ChainOfThought(ProofRead)\n",
    "        self.title_generator = dspy.ChainOfThought(TitleGenerator)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = dspy.Retrieve(k=5)(question).passages\n",
    "        context = \"\".join(context)\n",
    "        raw_blog_outline = self.question_to_blog_outline(question=question, context=context).blog_outline\n",
    "        blog_outline = re.split(\",|\\n\", raw_blog_outline)\n",
    "        blog = \"\"\n",
    "        for topic in blog_outline:\n",
    "            topic_contexts = dspy.Retrieve(k=5)(topic).passages\n",
    "            topic_contexts = \"\".join(topic_contexts)\n",
    "            blog += self.topic_to_paragraph(topic=topic, context=context).paragraph\n",
    "            blog += \"\\n \\n\"\n",
    "        blog = self.proof_reader(blog_post=blog).proofread_blog_post\n",
    "        title = self.title_generator(blog_outline=raw_blog_outline).title\n",
    "        final_blog = f\"{title} \\n \\n {blog}\"\n",
    "        return dspy.Prediction(blog=final_blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mastering Speech Processing with DSPy: From Flow Separation to LM-driven Algorithms \n",
      " \n",
      " Yes, please proofread the blog post and suggest any improvements in terms of clarity, style, and grammar.\n"
     ]
    }
   ],
   "source": [
    "dspy_topic = \"What is DSPy fopr LLMs?\"\n",
    "dspy_blog = BlogPostWriter()(dspy_topic)\n",
    "print(dspy_blog.blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task is to write a blog post that will help answer the given question. \n",
      "    Please use the contexts to evaluate the structure of the blog post.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Context: ${context}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the blog_outline}. We ...\n",
      "\n",
      "Blog Outline: A comma separated list of topics. IMPORTANT!! ONLY SEPARATE LIST ITEMS WITH A COMMA\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is DSPy fopr LLMs?\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m understand what DSPy is and how it works. First, we need to understand that DSPy separates the flow of a program from its parameters, which are the LM prompts and weights used in each step. This separation allows for more flexibility and control over these parameters, as they can be tuned using new `optimizers` introduced by DSPy. These optimizers are driven by LMs and aim to maximize a specific `metric`. By doing so, DSPy can teach powerful models like GPT-3.5 or GPT-\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task is to write a blog post that will help answer the given question. \n",
      "    Please use the contexts to evaluate the structure of the blog post.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Context: ${context}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the blog_outline}. We ...\n",
      "\n",
      "Blog Outline: A comma separated list of topics. IMPORTANT!! ONLY SEPARATE LIST ITEMS WITH A COMMA\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is DSPy fopr LLMs?\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to understand what DSPy is and how it works. First, we need to understand that DSPy separates the flow of a program from its parameters, which are the LM prompts and weights used in each step. This separation allows for more flexibility and control over these parameters, as they can be tuned using new `optimizers` introduced by DSPy. These optimizers are driven by LMs and aim to maximize a specific `metric`. By doing so, DSPy can teach powerful models like GPT-3.5 or GPT-\n",
      "\n",
      "Blog Outline:\u001b[32m 1. Introduction to DSPy\n",
      "2. Separation of flow and parameters in DSPy\n",
      "3. Introducing new optimizers in DSPy\n",
      "4. Maximizing metrics with DSPy's LM-driven algorithms\n",
      "5. Examples of using DSPy for complex systems and pipelines\n",
      "6. Comparison between DSPy and higher-level libraries like HuggingFace Transformers\n",
      "7. Conclusion and future developments in DSPy.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: 1. Introduction to DSPy\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m introduce DSPy. Firstly, we need to explain what DSPy is and how it differs from other libraries for working with LMs. We can compare it to PyTorch versus HuggingFace Transformers as a way of illustrating the difference between a framework for automatically optimizing LM prompts and weights and higher-level libraries that provide predefined prompts and integrations. Secondly, we need to explain how DSPy separates the flow of a program from its parameters and introduces new optimizers that can tune those parameters based on a desired metric. Finally, we can mention some examples of powerful models that DSPy can teach, such as GPT-3.5 or GPT-\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: 1. Introduction to DSPy\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to introduce DSPy. Firstly, we need to explain what DSPy is and how it differs from other libraries for working with LMs. We can compare it to PyTorch versus HuggingFace Transformers as a way of illustrating the difference between a framework for automatically optimizing LM prompts and weights and higher-level libraries that provide predefined prompts and integrations. Secondly, we need to explain how DSPy separates the flow of a program from its parameters and introduces new optimizers that can tune those parameters based on a desired metric. Finally, we can mention some examples of powerful models that DSPy can teach, such as GPT-3.5 or GPT-\n",
      "\n",
      "Paragraph:\u001b[32m DSPy is a framework for algorithmically optimizing LM prompts and weights, especially when LMs are used one or more times within a pipeline. Unlike higher-level libraries that provide predefined prompts and integrations, DSPy separates the flow of a program from its parameters and introduces new optimizers that can tune those parameters based on a desired metric. This lightweight programming model allows for automatically optimizing LM prompts and weights without requiring extensive implementation or extension efforts. With DSPy, powerful models like GPT-3.5 and GPT- can be taught routinely.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: 2. Separation of flow and parameters in DSPy\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m explain what DSPy is and how it works. First, we need to understand that DSPy separates the flow of a program from its parameters, which are the LM prompts and weights used in each step. This separation allows for more flexibility and control over these parameters, as they can be tuned using new optimizers introduced by DSPy. These optimizers use LM-driven algorithms to maximize a specific metric, making it possible to teach powerful models like GPT-3.5 or GPT-\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: 2. Separation of flow and parameters in DSPy\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to explain what DSPy is and how it works. First, we need to understand that DSPy separates the flow of a program from its parameters, which are the LM prompts and weights used in each step. This separation allows for more flexibility and control over these parameters, as they can be tuned using new optimizers introduced by DSPy. These optimizers use LM-driven algorithms to maximize a specific metric, making it possible to teach powerful models like GPT-3.5 or GPT-\n",
      "\n",
      "Paragraph:\u001b[32m DSPy is a framework for algorithmically optimizing LM prompts and weights in programs that use LMs, especially when they are used multiple times within a pipeline. It separates the flow of a program from its parameters, which are the LM prompts and weights used in each step, allowing for more flexibility and control over these parameters. DSPy introduces new optimizers, which are LM-driven algorithms that can tune the prompts and/or weights of LM calls given a specific metric to maximize. This makes it possible to teach powerful models like GPT-3.5 or GPT-\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: 3. Introducing new optimizers in DSPy\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m introduce new optimizers in DSPy. Firstly, we need to understand the concept of DSPy and how it differs from LM clients and RM clients. DSPy separates the flow of a program from its parameters, which are the prompts and weights for each step. This allows us to introduce new optimizers that can tune these parameters using LM-driven algorithms. These optimizers aim to maximize a specific metric. By introducing new optimizers, we can improve the performance of our programs that use LMs, especially when they are used multiple times within a pipeline. DSPy is designed for algorithmically optimizing LM prompts and weights, making it a lightweight but automatically-optimizing programming model. It's different from higher-level libraries like HuggingFace Transformers, which provide predefined prompts and integrations. By using DSPy, we can avoid the need to implement our own short programs or tweak steps to work well together. Instead, we can rely on LM-driven algorithms to optimize our programs automatically. Overall, DSPy is a powerful framework for algorithmically optimizing LMs in complex systems, especially when they are used multiple times within a pipeline. It's designed for users who need a lightweight but automatically-optimizing programming model, rather than a library of predefined prompts and integrations.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: 3. Introducing new optimizers in DSPy\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to introduce new optimizers in DSPy. Firstly, we need to understand the concept of DSPy and how it differs from LM clients and RM clients. DSPy separates the flow of a program from its parameters, which are the prompts and weights for each step. This allows us to introduce new optimizers that can tune these parameters using LM-driven algorithms. These optimizers aim to maximize a specific metric. By introducing new optimizers, we can improve the performance of our programs that use LMs, especially when they are used multiple times within a pipeline. DSPy is designed for algorithmically optimizing LM prompts and weights, making it a lightweight but automatically-optimizing programming model. It's different from higher-level libraries like HuggingFace Transformers, which provide predefined prompts and integrations. By using DSPy, we can avoid the need to implement our own short programs or tweak steps to work well together. Instead, we can rely on LM-driven algorithms to optimize our programs automatically. Overall, DSPy is a powerful framework for algorithmically optimizing LMs in complex systems, especially when they are used multiple times within a pipeline. It's designed for users who need a lightweight but automatically-optimizing programming model, rather than a library of predefined prompts and integrations.\n",
      "\n",
      "Paragraph:\u001b[32m DSPy is a framework that separates the flow of a program from its parameters, which are the prompts and weights for each step. It introduces new optimizers that can tune these parameters using LM-driven algorithms to maximize specific metrics. By introducing new optimizers, DSPy improves the performance of programs that use LMs, especially when they are used multiple times within a pipeline. Unlike higher-level libraries like HuggingFace Transformers, which provide predefined prompts and integrations, DSPy is designed for users who need a lightweight but automatically-optimizing programming model. It's particularly powerful for algorithmically optimizing LMs in complex systems where multiple LM calls are required.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: 4. Maximizing metrics with DSPy's LM-driven algorithms\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m explain how DSPy optimizes metrics with its LM-driven algorithms. Firstly, DSPy separates the flow of a program from its parameters, which are the prompts and weights used for each step involving an LM call. This separation allows for greater flexibility and modularity in building complex systems using LMs. Secondly, DSPy introduces new optimizers that utilize LM-driven algorithms to tune the prompts and/or weights of LM calls based on a desired metric to be maximized. By leveraging the power of advanced models like GPT-3.5 or GPT-3, DSPy is capable of routinely teaching these models to optimize metrics in various applications.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: 4. Maximizing metrics with DSPy's LM-driven algorithms\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to explain how DSPy optimizes metrics with its LM-driven algorithms. Firstly, DSPy separates the flow of a program from its parameters, which are the prompts and weights used for each step involving an LM call. This separation allows for greater flexibility and modularity in building complex systems using LMs. Secondly, DSPy introduces new optimizers that utilize LM-driven algorithms to tune the prompts and/or weights of LM calls based on a desired metric to be maximized. By leveraging the power of advanced models like GPT-3.5 or GPT-3, DSPy is capable of routinely teaching these models to optimize metrics in various applications.\n",
      "\n",
      "Paragraph:\u001b[32m DSPy, a framework for algorithmically optimizing LM prompts and weights, offers a unique programming model that separates the flow of a program from its parameters, which are the prompts and weights used for each step involving an LM call. This separation allows for greater flexibility and modularity in building complex systems using LMs. DSPy introduces new optimizers that utilize LM-driven algorithms to tune the prompts and/or weights of LM calls based on a desired metric to be maximized, making it possible to routinely teach powerful models like GPT-3.5 or GPT-3 to optimize metrics in various applications.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: 5. Examples of using DSPy for complex systems and pipelines\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m explain what DSPy is and how it works. First, we need to understand that DSPy separates the flow of a program from its parameters, which are the prompts and weights used by LMs at each step. This separation allows for more flexibility and control over these parameters, as they can be automatically tuned using new optimizers introduced by DSPy. These optimizers use LM-driven algorithms to maximize a desired metric, such as accuracy or efficiency. By leveraging the power of LMs like GPT-3.5 or GPT-\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: 5. Examples of using DSPy for complex systems and pipelines\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to explain what DSPy is and how it works. First, we need to understand that DSPy separates the flow of a program from its parameters, which are the prompts and weights used by LMs at each step. This separation allows for more flexibility and control over these parameters, as they can be automatically tuned using new optimizers introduced by DSPy. These optimizers use LM-driven algorithms to maximize a desired metric, such as accuracy or efficiency. By leveraging the power of LMs like GPT-3.5 or GPT-\n",
      "\n",
      "Paragraph:\u001b[32m DSPy is a framework for algorithmically optimizing LM prompts and weights, especially in complex systems and pipelines where multiple LMs are used. It separates the flow of a program from its parameters, allowing for more flexibility and control over these parameters through new optimizers that use LM-driven algorithms to maximize desired metrics. DSPy can routinely teach powerful models like GPT-3.5 or GPT-\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: 6. Comparison between DSPy and higher-level libraries like HuggingFace Transformers\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m explain what DSPy is and how it differs from other libraries for working with LMs. First, we need to understand that DSPy separates the flow of a program (modules) from its parameters (LM prompts and weights). This separation allows us to optimize each step independently using DSPy's new `optimizers`, which are algorithms driven by LMs that can tune both prompts and weights based on a desired metric. By doing so, DSPy enables the automatic optimization of complex systems built with LMs, making it easier for users to implement their own short programs without having to manually tweak each step. Compared to higher-level libraries like HuggingFace Transformers, which provide predefined prompts and integrations, DSPy offers a lightweight programming model that is automatically optimizing. This difference is similar to the distinction between PyTorch and HuggingFace Transformers in neural networks. Overall, DSPy's unique approach allows it to routinely teach powerful models like GPT-3.5 or GPT-J with its automatic optimization capabilities.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: 6. Comparison between DSPy and higher-level libraries like HuggingFace Transformers\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to explain what DSPy is and how it differs from other libraries for working with LMs. First, we need to understand that DSPy separates the flow of a program (modules) from its parameters (LM prompts and weights). This separation allows us to optimize each step independently using DSPy's new `optimizers`, which are algorithms driven by LMs that can tune both prompts and weights based on a desired metric. By doing so, DSPy enables the automatic optimization of complex systems built with LMs, making it easier for users to implement their own short programs without having to manually tweak each step. Compared to higher-level libraries like HuggingFace Transformers, which provide predefined prompts and integrations, DSPy offers a lightweight programming model that is automatically optimizing. This difference is similar to the distinction between PyTorch and HuggingFace Transformers in neural networks. Overall, DSPy's unique approach allows it to routinely teach powerful models like GPT-3.5 or GPT-J with its automatic optimization capabilities.\n",
      "\n",
      "Paragraph:\u001b[32m DSPy is a framework for algorithmically optimizing LM prompts and weights, especially when LMs are used multiple times within a pipeline. Unlike higher-level libraries that provide predefined prompts and integrations, DSPy separates the flow of a program from its parameters, allowing for independent optimization of each step using new `optimizers` driven by LMs. These optimizers can tune both prompts and weights based on a desired metric, making it easier to implement complex systems built with LMs without manual tweaking. DSPy's lightweight programming model is automatically optimizing, similar to the distinction between PyTorch and HuggingFace Transformers in neural networks. With its automatic optimization capabilities, DSPy can routinely teach powerful models like GPT-3.5 or GPT-J.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: 7. Conclusion and future developments in DSPy.\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m explain what DSPy is and how it differs from other libraries for working with LMs. First, we need to understand that DSPy separates the flow of a program (modules) from its parameters (LM prompts and weights). This separation allows us to optimize each step independently using DSPy's new `optimizers`, which are algorithms driven by LMs that can tune both prompts and weights based on a desired metric. By doing so, DSPy enables the automatic optimization of complex systems built with LMs, making it easier for users to implement their own short programs without having to manually tweak each step. Compared to other libraries like HuggingFace Transformers or custom LM/RM clients, DSPy provides a lightweight and automatically-optimizing programming model that is well-suited for algorithmically optimizing LM prompts and weights in pipelines where LMs are used multiple times. In summary, DSPy is a framework for algorithmically optimizing LM prompts and weights, especially when LMs are used one or more times within a pipeline, while other libraries provide predefined prompts and integrations.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: 7. Conclusion and future developments in DSPy.\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to explain what DSPy is and how it differs from other libraries for working with LMs. First, we need to understand that DSPy separates the flow of a program (modules) from its parameters (LM prompts and weights). This separation allows us to optimize each step independently using DSPy's new `optimizers`, which are algorithms driven by LMs that can tune both prompts and weights based on a desired metric. By doing so, DSPy enables the automatic optimization of complex systems built with LMs, making it easier for users to implement their own short programs without having to manually tweak each step. Compared to other libraries like HuggingFace Transformers or custom LM/RM clients, DSPy provides a lightweight and automatically-optimizing programming model that is well-suited for algorithmically optimizing LM prompts and weights in pipelines where LMs are used multiple times. In summary, DSPy is a framework for algorithmically optimizing LM prompts and weights, especially when LMs are used one or more times within a pipeline, while other libraries provide predefined prompts and integrations.\n",
      "\n",
      "Paragraph:\u001b[32m DSPy is a framework that separates the flow of a program from its parameters using modules, allowing for independent optimization of each step using new `optimizers` driven by LMs. These optimizers can tune both prompts and weights based on a desired metric, making it easier to implement complex systems built with LMs without manual tweaking. Compared to other libraries like HuggingFace Transformers or custom LM/RM clients, DSPy provides a lightweight and automatically-optimizing programming model well-suited for algorithmically optimizing LM prompts and weights in pipelines where LMs are used multiple times. DSPy can routinely teach powerful models like GPT-3.5 or GPT-J.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Proofread a blog post and output a more well written version of the original post.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Blog Post: ${blog_post}\n",
      "Reasoning: Let's think step by step in order to ${produce the proofread_blog_post}. We ...\n",
      "Proofread Blog Post: ${proofread_blog_post}\n",
      "\n",
      "---\n",
      "\n",
      "Blog Post: DSPy is a framework for algorithmically optimizing LM prompts and weights, especially when LMs are used one or more times within a pipeline. Unlike higher-level libraries that provide predefined prompts and integrations, DSPy separates the flow of a program from its parameters and introduces new optimizers that can tune those parameters based on a desired metric. This lightweight programming model allows for automatically optimizing LM prompts and weights without requiring extensive implementation or extension efforts. With DSPy, powerful models like GPT-3.5 and GPT- can be taught routinely. DSPy is a framework for algorithmically optimizing LM prompts and weights in programs that use LMs, especially when they are used multiple times within a pipeline. It separates the flow of a program from its parameters, which are the LM prompts and weights used in each step, allowing for more flexibility and control over these parameters. DSPy introduces new optimizers, which are LM-driven algorithms that can tune the prompts and/or weights of LM calls given a specific metric to maximize. This makes it possible to teach powerful models like GPT-3.5 or GPT- DSPy is a framework that separates the flow of a program from its parameters, which are the prompts and weights for each step. It introduces new optimizers that can tune these parameters using LM-driven algorithms to maximize specific metrics. By introducing new optimizers, DSPy improves the performance of programs that use LMs, especially when they are used multiple times within a pipeline. Unlike higher-level libraries like HuggingFace Transformers, which provide predefined prompts and integrations, DSPy is designed for users who need a lightweight but automatically-optimizing programming model. It's particularly powerful for algorithmically optimizing LMs in complex systems where multiple LM calls are required. DSPy, a framework for algorithmically optimizing LM prompts and weights, offers a unique programming model that separates the flow of a program from its parameters, which are the prompts and weights used for each step involving an LM call. This separation allows for greater flexibility and modularity in building complex systems using LMs. DSPy introduces new optimizers that utilize LM-driven algorithms to tune the prompts and/or weights of LM calls based on a desired metric to be maximized, making it possible to routinely teach powerful models like GPT-3.5 or GPT-3 to optimize metrics in various applications. DSPy is a framework for algorithmically optimizing LM prompts and weights, especially in complex systems and pipelines where multiple LMs are used. It separates the flow of a program from its parameters, allowing for more flexibility and control over these parameters through new optimizers that use LM-driven algorithms to maximize desired metrics. DSPy can routinely teach powerful models like GPT-3.5 or GPT- DSPy is a framework for algorithmically optimizing LM prompts and weights, especially when LMs are used multiple times within a pipeline. Unlike higher-level libraries that provide predefined prompts and integrations, DSPy separates the flow of a program from its parameters, allowing for independent optimization of each step using new `optimizers` driven by LMs. These optimizers can tune both prompts and weights based on a desired metric, making it easier to implement complex systems built with LMs without manual tweaking. DSPy's lightweight programming model is automatically optimizing, similar to the distinction between PyTorch and HuggingFace Transformers in neural networks. With its automatic optimization capabilities, DSPy can routinely teach powerful models like GPT-3.5 or GPT-J. DSPy is a framework that separates the flow of a program from its parameters using modules, allowing for independent optimization of each step using new `optimizers` driven by LMs. These optimizers can tune both prompts and weights based on a desired metric, making it easier to implement complex systems built with LMs without manual tweaking. Compared to other libraries like HuggingFace Transformers or custom LM/RM clients, DSPy provides a lightweight and automatically-optimizing programming model well-suited for algorithmically optimizing LM prompts and weights in pipelines where LMs are used multiple times. DSPy can routinely teach powerful models like GPT-3.5 or GPT-J.\n",
      "Reasoning: Let's think step by step in order to\u001b[32m understand how DSPy works. Firstly, DSPy separates the flow of a program from its parameters using modules. This means that we can define separate functions for each step in our pipeline, and pass in only the necessary parameters instead of having to include the entire pipeline in every function call. This makes it easier to manage complex systems built with LMs, as we can optimize each step independently without having to worry about the rest of the pipeline.\n",
      "Secondly, DSPy introduces new `optimizers` that use LM-driven algorithms to tune prompts and weights based on a desired metric. This allows us to automatically optimize our models for specific tasks or metrics, rather than manually tweaking them. For example, we might want to maximize the accuracy of our model on a certain dataset, or minimize the number of parameters in our model to reduce training time.\n",
      "Finally, DSPy can routinely teach powerful models like GPT-3.5 or GPT-J using these `optimizers`. This means that we can train our models to optimize specific metrics without having to manually fine-tune them for each task. This is especially useful in complex systems where multiple LMs are used, as we can optimize each step independently and then combine the results to achieve better overall performance.\n",
      "Overall, DSPy provides a lightweight and automatically-optimizing programming model that makes it easier to implement complex systems built with LMs without manual tweaking or fine-tuning. Its `optimizers` allow us to automatically optimize our models for specific tasks or metrics, making it a powerful tool for building high-performance NLP applications.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Proofread a blog post and output a more well written version of the original post.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Blog Post: ${blog_post}\n",
      "Reasoning: Let's think step by step in order to ${produce the proofread_blog_post}. We ...\n",
      "Proofread Blog Post: ${proofread_blog_post}\n",
      "\n",
      "---\n",
      "\n",
      "Blog Post: DSPy is a framework for algorithmically optimizing LM prompts and weights, especially when LMs are used one or more times within a pipeline. Unlike higher-level libraries that provide predefined prompts and integrations, DSPy separates the flow of a program from its parameters and introduces new optimizers that can tune those parameters based on a desired metric. This lightweight programming model allows for automatically optimizing LM prompts and weights without requiring extensive implementation or extension efforts. With DSPy, powerful models like GPT-3.5 and GPT- can be taught routinely. DSPy is a framework for algorithmically optimizing LM prompts and weights in programs that use LMs, especially when they are used multiple times within a pipeline. It separates the flow of a program from its parameters, which are the LM prompts and weights used in each step, allowing for more flexibility and control over these parameters. DSPy introduces new optimizers, which are LM-driven algorithms that can tune the prompts and/or weights of LM calls given a specific metric to maximize. This makes it possible to teach powerful models like GPT-3.5 or GPT- DSPy is a framework that separates the flow of a program from its parameters, which are the prompts and weights for each step. It introduces new optimizers that can tune these parameters using LM-driven algorithms to maximize specific metrics. By introducing new optimizers, DSPy improves the performance of programs that use LMs, especially when they are used multiple times within a pipeline. Unlike higher-level libraries like HuggingFace Transformers, which provide predefined prompts and integrations, DSPy is designed for users who need a lightweight but automatically-optimizing programming model. It's particularly powerful for algorithmically optimizing LMs in complex systems where multiple LM calls are required. DSPy, a framework for algorithmically optimizing LM prompts and weights, offers a unique programming model that separates the flow of a program from its parameters, which are the prompts and weights used for each step involving an LM call. This separation allows for greater flexibility and modularity in building complex systems using LMs. DSPy introduces new optimizers that utilize LM-driven algorithms to tune the prompts and/or weights of LM calls based on a desired metric to be maximized, making it possible to routinely teach powerful models like GPT-3.5 or GPT-3 to optimize metrics in various applications. DSPy is a framework for algorithmically optimizing LM prompts and weights, especially in complex systems and pipelines where multiple LMs are used. It separates the flow of a program from its parameters, allowing for more flexibility and control over these parameters through new optimizers that use LM-driven algorithms to maximize desired metrics. DSPy can routinely teach powerful models like GPT-3.5 or GPT- DSPy is a framework for algorithmically optimizing LM prompts and weights, especially when LMs are used multiple times within a pipeline. Unlike higher-level libraries that provide predefined prompts and integrations, DSPy separates the flow of a program from its parameters, allowing for independent optimization of each step using new `optimizers` driven by LMs. These optimizers can tune both prompts and weights based on a desired metric, making it easier to implement complex systems built with LMs without manual tweaking. DSPy's lightweight programming model is automatically optimizing, similar to the distinction between PyTorch and HuggingFace Transformers in neural networks. With its automatic optimization capabilities, DSPy can routinely teach powerful models like GPT-3.5 or GPT-J. DSPy is a framework that separates the flow of a program from its parameters using modules, allowing for independent optimization of each step using new `optimizers` driven by LMs. These optimizers can tune both prompts and weights based on a desired metric, making it easier to implement complex systems built with LMs without manual tweaking. Compared to other libraries like HuggingFace Transformers or custom LM/RM clients, DSPy provides a lightweight and automatically-optimizing programming model well-suited for algorithmically optimizing LM prompts and weights in pipelines where LMs are used multiple times. DSPy can routinely teach powerful models like GPT-3.5 or GPT-J.\n",
      "Reasoning: Let's think step by step in order to understand how DSPy works. Firstly, DSPy separates the flow of a program from its parameters using modules. This means that we can define separate functions for each step in our pipeline, and pass in only the necessary parameters instead of having to include the entire pipeline in every function call. This makes it easier to manage complex systems built with LMs, as we can optimize each step independently without having to worry about the rest of the pipeline. Secondly, DSPy introduces new `optimizers` that use LM-driven algorithms to tune prompts and weights based on a desired metric. This allows us to automatically optimize our models for specific tasks or metrics, rather than manually tweaking them. For example, we might want to maximize the accuracy of our model on a certain dataset, or minimize the number of parameters in our model to reduce training time. Finally, DSPy can routinely teach powerful models like GPT-3.5 or GPT-J using these `optimizers`. This means that we can train our models to optimize specific metrics without having to manually fine-tune them for each task. This is especially useful in complex systems where multiple LMs are used, as we can optimize each step independently and then combine the results to achieve better overall performance. Overall, DSPy provides a lightweight and automatically-optimizing programming model that makes it easier to implement complex systems built with LMs without manual tweaking or fine-tuning. Its `optimizers` allow us to automatically optimize our models for specific tasks or metrics, making it a powerful tool for building high-performance NLP applications.\n",
      "Proofread Blog Post:\u001b[32m Yes, please proofread the blog post and suggest any improvements in terms of clarity, style, and grammar.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Write a title for a blog post given a description of the topics the blog covers as input.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Blog Outline: ${blog_outline}\n",
      "Reasoning: Let's think step by step in order to ${produce the title}. We ...\n",
      "Title: ${title}\n",
      "\n",
      "---\n",
      "\n",
      "Blog Outline: 1. Introduction to DSPy 2. Separation of flow and parameters in DSPy 3. Introducing new optimizers in DSPy 4. Maximizing metrics with DSPy's LM-driven algorithms 5. Examples of using DSPy for complex systems and pipelines 6. Comparison between DSPy and higher-level libraries like HuggingFace Transformers 7. Conclusion and future developments in DSPy.\n",
      "Reasoning: Let's think step by step in order to\u001b[32m write a catchy title that accurately reflects the content of this blog post. First, we introduce the reader to DSPy, a powerful library for deep learning speech processing. Then, we explain how DSPy separates flow and parameters, making it easier to use and more flexible than other libraries. Next, we introduce new optimizers in DSPy that help maximize metrics and improve performance. We also explore LM-driven algorithms, which allow us to further optimize our models for specific tasks. Throughout the post, we provide examples of using DSPy for complex systems and pipelines, demonstrating its versatility and ease of use. We compare DSPy to higher-level libraries like HuggingFace Transformers, highlighting its unique strengths and advantages. Finally, we conclude with a look at future developments in DSPy and encourage readers to try it out for themselves. Based on this outline, a possible title could be \"Mastering Speech Processing with DSPy: From Flow Separation to LM-driven Algorithms\".\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Write a title for a blog post given a description of the topics the blog covers as input.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Blog Outline: ${blog_outline}\n",
      "Reasoning: Let's think step by step in order to ${produce the title}. We ...\n",
      "Title: ${title}\n",
      "\n",
      "---\n",
      "\n",
      "Blog Outline: 1. Introduction to DSPy 2. Separation of flow and parameters in DSPy 3. Introducing new optimizers in DSPy 4. Maximizing metrics with DSPy's LM-driven algorithms 5. Examples of using DSPy for complex systems and pipelines 6. Comparison between DSPy and higher-level libraries like HuggingFace Transformers 7. Conclusion and future developments in DSPy.\n",
      "Reasoning: Let's think step by step in order to write a catchy title that accurately reflects the content of this blog post. First, we introduce the reader to DSPy, a powerful library for deep learning speech processing. Then, we explain how DSPy separates flow and parameters, making it easier to use and more flexible than other libraries. Next, we introduce new optimizers in DSPy that help maximize metrics and improve performance. We also explore LM-driven algorithms, which allow us to further optimize our models for specific tasks. Throughout the post, we provide examples of using DSPy for complex systems and pipelines, demonstrating its versatility and ease of use. We compare DSPy to higher-level libraries like HuggingFace Transformers, highlighting its unique strengths and advantages. Finally, we conclude with a look at future developments in DSPy and encourage readers to try it out for themselves. Based on this outline, a possible title could be \"Mastering Speech Processing with DSPy: From Flow Separation to LM-driven Algorithms\".\n",
      "Title:\u001b[32m Mastering Speech Processing with DSPy: From Flow Separation to LM-driven Algorithms\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mastering Speech Processing with DSPy: From Flow Separation to LM-driven Algorithms \n",
      " \n",
      " Yes, please proofread the blog post and suggest any improvements in terms of clarity, style, and grammar.\n"
     ]
    }
   ],
   "source": [
    "dspy_topic = \"What is DSPy fopr LLMs?\"\n",
    "dspy_blog = BlogPostWriter()(dspy_topic)\n",
    "print(dspy_blog.blog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Improving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlogRater(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Rate a blog post on a scale of 1 to 5 based on how well written it is.\n",
    "    \"\"\"\n",
    "\n",
    "    blog = dspy.InputField(desc=\"a blog post\")\n",
    "    rating = dspy.OutputField(desc=\"a quality rating on a scale of 1 to 5. IMPORTANT!! ONLY OUTPUT THE RATING AS A FLOAT VALUE AND NOTHING ELSE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricProgram(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.rater = dspy.ChainOfThought(BlogRater)\n",
    "\n",
    "    def forward(self, gold, pred, trace=None):\n",
    "        blog = pred.blog\n",
    "        gold = gold.question\n",
    "        return float(self.rater(blog=blog).rating[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_wrapper(gold, pred, trace=None):\n",
    "    return MetricProgram()(gold=gold, pred=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is DSPy fopr LLMs?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dspy_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    blog='Mastering Speech Processing with DSPy: From Flow Separation to LM-driven Algorithms \\n \\n Yes, please proofread the blog post and suggest any improvements in terms of clarity, style, and grammar.'\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dspy_blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': 'What is DSPy fopr LLMs?'}) (input_keys={'question'})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dspy.Example(question=dspy_topic).with_inputs(\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MetricProgram()(gold=dspy.Example(question=dspy_topic).with_inputs(\"question\"), pred=dspy_blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Proofread a blog post and output a more well written version of the original post.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Blog Post: ${blog_post}\n",
      "Reasoning: Let's think step by step in order to ${produce the proofread_blog_post}. We ...\n",
      "Proofread Blog Post: ${proofread_blog_post}\n",
      "\n",
      "---\n",
      "\n",
      "Blog Post: DSPy is a framework for algorithmically optimizing LM prompts and weights, especially when LMs are used one or more times within a pipeline. Unlike higher-level libraries that provide predefined prompts and integrations, DSPy separates the flow of a program from its parameters and introduces new optimizers that can tune those parameters based on a desired metric. This lightweight programming model allows for automatically optimizing LM prompts and weights without requiring extensive implementation or extension efforts. With DSPy, powerful models like GPT-3.5 and GPT- can be taught routinely. DSPy is a framework for algorithmically optimizing LM prompts and weights in programs that use LMs, especially when they are used multiple times within a pipeline. It separates the flow of a program from its parameters, which are the LM prompts and weights used in each step, allowing for more flexibility and control over these parameters. DSPy introduces new optimizers, which are LM-driven algorithms that can tune the prompts and/or weights of LM calls given a specific metric to maximize. This makes it possible to teach powerful models like GPT-3.5 or GPT- DSPy is a framework that separates the flow of a program from its parameters, which are the prompts and weights for each step. It introduces new optimizers that can tune these parameters using LM-driven algorithms to maximize specific metrics. By introducing new optimizers, DSPy improves the performance of programs that use LMs, especially when they are used multiple times within a pipeline. Unlike higher-level libraries like HuggingFace Transformers, which provide predefined prompts and integrations, DSPy is designed for users who need a lightweight but automatically-optimizing programming model. It's particularly powerful for algorithmically optimizing LMs in complex systems where multiple LM calls are required. DSPy, a framework for algorithmically optimizing LM prompts and weights, offers a unique programming model that separates the flow of a program from its parameters, which are the prompts and weights used for each step involving an LM call. This separation allows for greater flexibility and modularity in building complex systems using LMs. DSPy introduces new optimizers that utilize LM-driven algorithms to tune the prompts and/or weights of LM calls based on a desired metric to be maximized, making it possible to routinely teach powerful models like GPT-3.5 or GPT-3 to optimize metrics in various applications. DSPy is a framework for algorithmically optimizing LM prompts and weights, especially in complex systems and pipelines where multiple LMs are used. It separates the flow of a program from its parameters, allowing for more flexibility and control over these parameters through new optimizers that use LM-driven algorithms to maximize desired metrics. DSPy can routinely teach powerful models like GPT-3.5 or GPT- DSPy is a framework for algorithmically optimizing LM prompts and weights, especially when LMs are used multiple times within a pipeline. Unlike higher-level libraries that provide predefined prompts and integrations, DSPy separates the flow of a program from its parameters, allowing for independent optimization of each step using new `optimizers` driven by LMs. These optimizers can tune both prompts and weights based on a desired metric, making it easier to implement complex systems built with LMs without manual tweaking. DSPy's lightweight programming model is automatically optimizing, similar to the distinction between PyTorch and HuggingFace Transformers in neural networks. With its automatic optimization capabilities, DSPy can routinely teach powerful models like GPT-3.5 or GPT-J. DSPy is a framework that separates the flow of a program from its parameters using modules, allowing for independent optimization of each step using new `optimizers` driven by LMs. These optimizers can tune both prompts and weights based on a desired metric, making it easier to implement complex systems built with LMs without manual tweaking. Compared to other libraries like HuggingFace Transformers or custom LM/RM clients, DSPy provides a lightweight and automatically-optimizing programming model well-suited for algorithmically optimizing LM prompts and weights in pipelines where LMs are used multiple times. DSPy can routinely teach powerful models like GPT-3.5 or GPT-J.\n",
      "Reasoning: Let's think step by step in order to understand how DSPy works. Firstly, DSPy separates the flow of a program from its parameters using modules. This means that we can define separate functions for each step in our pipeline, and pass in only the necessary parameters instead of having to include the entire pipeline in every function call. This makes it easier to manage complex systems built with LMs, as we can optimize each step independently without having to worry about the rest of the pipeline. Secondly, DSPy introduces new `optimizers` that use LM-driven algorithms to tune prompts and weights based on a desired metric. This allows us to automatically optimize our models for specific tasks or metrics, rather than manually tweaking them. For example, we might want to maximize the accuracy of our model on a certain dataset, or minimize the number of parameters in our model to reduce training time. Finally, DSPy can routinely teach powerful models like GPT-3.5 or GPT-J using these `optimizers`. This means that we can train our models to optimize specific metrics without having to manually fine-tune them for each task. This is especially useful in complex systems where multiple LMs are used, as we can optimize each step independently and then combine the results to achieve better overall performance. Overall, DSPy provides a lightweight and automatically-optimizing programming model that makes it easier to implement complex systems built with LMs without manual tweaking or fine-tuning. Its `optimizers` allow us to automatically optimize our models for specific tasks or metrics, making it a powerful tool for building high-performance NLP applications.\n",
      "Proofread Blog Post:\u001b[32m Yes, please proofread the blog post and suggest any improvements in terms of clarity, style, and grammar.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Write a title for a blog post given a description of the topics the blog covers as input.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Blog Outline: ${blog_outline}\n",
      "Reasoning: Let's think step by step in order to ${produce the title}. We ...\n",
      "Title: ${title}\n",
      "\n",
      "---\n",
      "\n",
      "Blog Outline: 1. Introduction to DSPy 2. Separation of flow and parameters in DSPy 3. Introducing new optimizers in DSPy 4. Maximizing metrics with DSPy's LM-driven algorithms 5. Examples of using DSPy for complex systems and pipelines 6. Comparison between DSPy and higher-level libraries like HuggingFace Transformers 7. Conclusion and future developments in DSPy.\n",
      "Reasoning: Let's think step by step in order to\u001b[32m write a catchy title that accurately reflects the content of this blog post. First, we introduce the reader to DSPy, a powerful library for deep learning speech processing. Then, we explain how DSPy separates flow and parameters, making it easier to use and more flexible than other libraries. Next, we introduce new optimizers in DSPy that help maximize metrics and improve performance. We also explore LM-driven algorithms, which allow us to further optimize our models for specific tasks. Throughout the post, we provide examples of using DSPy for complex systems and pipelines, demonstrating its versatility and ease of use. We compare DSPy to higher-level libraries like HuggingFace Transformers, highlighting its unique strengths and advantages. Finally, we conclude with a look at future developments in DSPy and encourage readers to try it out for themselves. Based on this outline, a possible title could be \"Mastering Speech Processing with DSPy: From Flow Separation to LM-driven Algorithms\".\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Write a title for a blog post given a description of the topics the blog covers as input.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Blog Outline: ${blog_outline}\n",
      "Reasoning: Let's think step by step in order to ${produce the title}. We ...\n",
      "Title: ${title}\n",
      "\n",
      "---\n",
      "\n",
      "Blog Outline: 1. Introduction to DSPy 2. Separation of flow and parameters in DSPy 3. Introducing new optimizers in DSPy 4. Maximizing metrics with DSPy's LM-driven algorithms 5. Examples of using DSPy for complex systems and pipelines 6. Comparison between DSPy and higher-level libraries like HuggingFace Transformers 7. Conclusion and future developments in DSPy.\n",
      "Reasoning: Let's think step by step in order to write a catchy title that accurately reflects the content of this blog post. First, we introduce the reader to DSPy, a powerful library for deep learning speech processing. Then, we explain how DSPy separates flow and parameters, making it easier to use and more flexible than other libraries. Next, we introduce new optimizers in DSPy that help maximize metrics and improve performance. We also explore LM-driven algorithms, which allow us to further optimize our models for specific tasks. Throughout the post, we provide examples of using DSPy for complex systems and pipelines, demonstrating its versatility and ease of use. We compare DSPy to higher-level libraries like HuggingFace Transformers, highlighting its unique strengths and advantages. Finally, we conclude with a look at future developments in DSPy and encourage readers to try it out for themselves. Based on this outline, a possible title could be \"Mastering Speech Processing with DSPy: From Flow Separation to LM-driven Algorithms\".\n",
      "Title:\u001b[32m Mastering Speech Processing with DSPy: From Flow Separation to LM-driven Algorithms\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rate a blog post on a scale of 1 to 5 based on how well written it is.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Blog: a blog post\n",
      "Reasoning: Let's think step by step in order to ${produce the rating}. We ...\n",
      "Rating: a quality rating on a scale of 1 to 5. IMPORTANT!! ONLY OUTPUT THE RATING AS A FLOAT VALUE AND NOTHING ELSE!\n",
      "\n",
      "---\n",
      "\n",
      "Blog: Mastering Speech Processing with DSPy: From Flow Separation to LM-driven Algorithms Yes, please proofread the blog post and suggest any improvements in terms of clarity, style, and grammar.\n",
      "Reasoning: Let's think step by step in order to\u001b[32m rate this blog post based on its writing quality. Firstly, we need to check if the author has a clear introduction that captures the reader's attention and provides an overview of what will be discussed in the post. Secondly, we should analyze whether the author uses appropriate language and structure to convey complex concepts related to speech processing. Thirdly, we need to evaluate if the author presents examples and illustrations that help the reader understand the concepts better. Lastly, we should check if the author concludes the post with a clear summary of the main points discussed and provides some insights or recommendations for further reading or exploration in this field. Based on these criteria, we will rate the blog post on a scale of 1 to 5.\n",
      "Rating: 4.9 (rounded down to 4)\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = [\n",
    "    dspy.Example(question=\"How do you set up an LM client?\").with_inputs(\"question\"),\n",
    "    dspy.Example(question=\"Can you make class based signatures?\").with_inputs(\"question\"),\n",
    "    dspy.Example(question=\"How do you create typed predictors?\").with_inputs(\"question\"),\n",
    "]\n",
    "testset = [\n",
    "    dspy.Example(question=\"What is a signature in DSPy?\").with_inputs(\"question\"),\n",
    "    dspy.Example(question=\"How does DSPy handle assertions?\").with_inputs(\"question\"),    \n",
    "    dspy.Example(question=\"What are some evaluation metrics in DSPy?\").with_inputs(\"question\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluate(devset=testset, num_threads=1, display_progress=True, display_table=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.0 / 3  (466.7): 100%|██████████| 3/3 [06:36<00:00, 132.09s/it]\n",
      "/home/peter-legion-wsl2/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/dspy/evaluate/evaluate.py:187: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n",
      "/home/peter-legion-wsl2/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/dspy/evaluate/evaluate.py:263: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['4.5' '4.5' '5.0']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, metric_name] = df[metric_name].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.0 / 3  (466.7%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4b998 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_4b998 td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_4b998_row0_col0, #T_4b998_row0_col1, #T_4b998_row0_col2, #T_4b998_row1_col0, #T_4b998_row1_col1, #T_4b998_row1_col2, #T_4b998_row2_col0, #T_4b998_row2_col1, #T_4b998_row2_col2 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4b998\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4b998_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_4b998_level0_col1\" class=\"col_heading level0 col1\" >blog</th>\n",
       "      <th id=\"T_4b998_level0_col2\" class=\"col_heading level0 col2\" >metric_wrapper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4b998_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_4b998_row0_col0\" class=\"data row0 col0\" >What is a signature in DSPy?</td>\n",
       "      <td id=\"T_4b998_row0_col1\" class=\"data row0 col1\" >Mastering Signatures in DSPy: From Basics to Advanced Techniques \n",
       " \n",
       " </td>\n",
       "      <td id=\"T_4b998_row0_col2\" class=\"data row0 col2\" >4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4b998_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_4b998_row1_col0\" class=\"data row1 col0\" >How does DSPy handle assertions?</td>\n",
       "      <td id=\"T_4b998_row1_col1\" class=\"data row1 col1\" >Mastering Assertions: Unleashing the Power of Debugging and Validation in DSPy Programming \n",
       " \n",
       " </td>\n",
       "      <td id=\"T_4b998_row1_col2\" class=\"data row1 col2\" >4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4b998_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_4b998_row2_col0\" class=\"data row2 col0\" >What are some evaluation metrics in DSPy?</td>\n",
       "      <td id=\"T_4b998_row2_col1\" class=\"data row2 col1\" >Mastering Metrics: A Comprehensive Guide to Understanding, Defining, and Optimizing Metrics in DSPy. \n",
       " \n",
       " </td>\n",
       "      <td id=\"T_4b998_row2_col2\" class=\"data row2 col2\" >5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fad22fe3610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "466.67"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(BlogPostWriter(), metric=metric_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "teleprompter = BootstrapFewShot(metric=metric_wrapper, max_bootstrapped_demos=1, max_rounds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [02:11<04:23, 131.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to run or to evaluate example Example({'question': 'How do you set up an LM client?'}) (input_keys={'question'}) with <function metric_wrapper at 0x7fad22b044c0> due to could not convert string to float: '4.5/'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [03:42<01:51, 111.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compiled_blog_writer = teleprompter.compile(BlogPostWriter(), trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_blog_writer.save('./data/compiled_blog_writer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
