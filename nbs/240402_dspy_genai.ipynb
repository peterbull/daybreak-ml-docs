{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "# from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "from dspy.retrieve.pgvector_rm import PgVectorRM\n",
    "from dspy.retrieve.chromadb_rm import ChromadbRM\n",
    "# import weaviate\n",
    "# import weaviate.classes as wvc\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions \n",
    "import fnmatch\n",
    "import uuid\n",
    "import torch\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "import re\n",
    "from dspy.evaluate.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn_string = f\"postgresql://daybreak:daybreak@localhost:5432/daybreak\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = psycopg2.connect(conn_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add pgvector extension and create test table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with conn.cursor() as cur:\n",
    "#     cur.execute('CREATE EXTENSION IF NOT EXISTS vector')\n",
    "#     register_vector(conn)\n",
    "#     cur.execute(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS text(\n",
    "#         id SERIAL PRIMARY KEY,\n",
    "#         text TEXT\n",
    "#         );\n",
    "#     \"\"\")\n",
    "# conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_md_files(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for basename in files:\n",
    "            if fnmatch.fnmatch(basename, '*.md'):\n",
    "                filename = os.path.join(root, basename)\n",
    "                yield filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in find_md_files(\"./repos\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = [filename for filename in find_md_files(\"./repos\")] \n",
    "doc_contents = []\n",
    "for doc in find_md_files(\"./repos\"):\n",
    "    with open(doc, 'r') as f:\n",
    "        doc_contents.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = \"\".join(doc_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "691"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_splitter.split_text(all_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = text_splitter.split_text(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up local llm and retriever client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = dspy.OllamaLocal(\"zephyr-7b-beta\", max_tokens=3000, stop=['\\n\\n'], model_type=\"chat\")\n",
    "llm = dspy.OllamaLocal(\"open-hermes-2-4_0\", max_tokens=3000, model_type=\"chat\")\n",
    "chroma_client = client = chromadb.PersistentClient(path=\"./chroma\")\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "collection = chroma_client.get_or_create_collection(name=\"docs\", embedding_function=default_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(name=docs)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.add(ids=[str(uuid.uuid4()) for _ in range(len(split_docs))], documents=split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_model = ChromadbRM(\"docs\", \"./chroma\", embedding_function=default_ef, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.settings.configure(lm=llm, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello! I'm doing well, thank you for asking. How can I assist you today?\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"hi how are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resp = requests.get('https://raw.githubusercontent.com/weaviate-tutorials/quickstart/main/data/jeopardy_tiny.json')\n",
    "# data = json.loads(resp.text)  \n",
    "\n",
    "# question_objs = list()\n",
    "# for i, d in enumerate(data):\n",
    "#     question_objs.append({\n",
    "#         \"answer\": d[\"Answer\"],\n",
    "#         \"question\": d[\"Question\"],\n",
    "#         \"category\": d[\"Category\"],\n",
    "\n",
    "# questions = weaviate_client.collections.get(\"Question\")\n",
    "# questions.data.insert_many(question_objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionToBlogOutline(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Your task is to write a blog post that will help answer the given question. \n",
    "    Please use the contexts to evaluate the structure of the blog post.\n",
    "    \"\"\"\n",
    "\n",
    "    question = dspy.InputField()\n",
    "    context = dspy.InputField()\n",
    "    blog_outline = dspy.OutputField(desc=\"A comma separated list of topics. IMPORTANT!! ONLY SEPARATE TOPICS WITH A COMMA!! ONLY OUTPUT THE TOPICS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "691"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(collection.get(include=['embeddings'])['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_question = \"What is DSPy used for?\"\n",
    "ex_context = dspy.Retrieve(k=5)(ex_question).passages\n",
    "ex_context = \"\".join(ex_context)\n",
    "cot = dspy.ChainOfThought(QuestionToBlogOutline)(question=ex_question, context=ex_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    rationale='write a blog post that will help answer the given question. We need to understand what DSPy is used for, consider its differences from other libraries and frameworks, discuss how much data is needed and how to collect it, and finally, explore other DSPy modules and their usage.',\n",
       "    blog_outline='Differences between DSPy and other libraries, Data requirements and collection, DSPy Modules'\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task is to write a blog post that will help answer the given question. \n",
      "    Please use the contexts to evaluate the structure of the blog post.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Context: ${context}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the blog_outline}. We ...\n",
      "\n",
      "Blog Outline: A comma separated list of topics. IMPORTANT!! ONLY SEPARATE TOPICS WITH A COMMA!! ONLY OUTPUT THE TOPICS\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is DSPy used for?\n",
      "\n",
      "Context:\n",
      "'re willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs to the API References for DSPy! This is where you'll find easy-to-understand information about all the parts of DSPy that you can use in your projects. We've got guides on different tools and helpers that DSPy has, like modules and optimizers. Everything is sorted so you can quickly find what you need. If you're making something and need to quickly get started with DSPy to do certain tasks, this place will show you how to set it Data\n",
      "\n",
      "DSPy is a machine learning framework, so working in it involves training sets, development sets, and test sets.\n",
      "\n",
      "For each example in your data, we distinguish typically between three types of values: the inputs, the intermediate labels, and the final label. You can use DSPy effectively without any intermediate or final labels, but you will need at least a few example inputs.\n",
      "\n",
      "## How much data do I need and how do I collect data for my tasky** philosophy and abstraction differ significantly from other libraries and frameworks, so it's usually straightforward to decide when **DSPy** is (or isn't) the right framework for your usecase. If you're a NLP/AI researcher (or a practitioner exploring new pipelines or new tasks), the answer is generally an invariable **yes**. If you're a practitioner doing other things, please read on.\n",
      "\n",
      "**DSPy vs. thin wrappers for prompts (OpenAI`\n",
      "\n",
      "\n",
      "## What other DSPy modules are there? How can I use them?\n",
      "\n",
      "The others are very similar. They mainly change the internal behavior with which your signature is implemented!\n",
      "\n",
      "1. **`dspy.Predict`**: Basic predictor. Does not modify the signature. Handles the key forms of learning (i.e., storing the instructions and demonstrations and updates to the LM).\n",
      "\n",
      "2. **`dspy.ChainOfTh\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m write a blog post that will help answer the given question. We need to understand what DSPy is used for, consider its differences from other libraries and frameworks, discuss how much data is needed and how to collect it, and finally, explore other DSPy modules and their usage.\n",
      "\n",
      "Blog Outline: \n",
      "Differences between DSPy and other libraries, Data requirements and collection, DSPy Modules\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'re willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\\n\\n**DSPy vs to the API References for DSPy! This is where you'll find easy-to-understand information about all the parts of DSPy that you can use in your projects. We've got guides on different tools and helpers that DSPy has, like modules and optimizers. Everything is sorted so you can quickly find what you need. If you're making something and need to quickly get started with DSPy to do certain tasks, this place will show you how to set it Data\\n\\nDSPy is a machine learning framework, so working in it involves training sets, development sets, and test sets.\\n\\nFor each example in your data, we distinguish typically between three types of values: the inputs, the intermediate labels, and the final label. You can use DSPy effectively without any intermediate or final labels, but you will need at least a few example inputs.\\n\\n## How much data do I need and how do I collect data for my tasky** philosophy and abstraction differ significantly from other libraries and frameworks, so it's usually straightforward to decide when **DSPy** is (or isn't) the right framework for your usecase. If you're a NLP/AI researcher (or a practitioner exploring new pipelines or new tasks), the answer is generally an invariable **yes**. If you're a practitioner doing other things, please read on.\\n\\n**DSPy vs. thin wrappers for prompts (OpenAI`\\n\\n\\n## What other DSPy modules are there? How can I use them?\\n\\nThe others are very similar. They mainly change the internal behavior with which your signature is implemented!\\n\\n1. **`dspy.Predict`**: Basic predictor. Does not modify the signature. Handles the key forms of learning (i.e., storing the instructions and demonstrations and updates to the LM).\\n\\n2. **`dspy.ChainOfTh\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Differences, Data Collection, Modules'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cot = dspy.ChainOfThought(QuestionToBlogOutline)\n",
    "cot(question=ex_question, context=ex_context).blog_outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChainOfThought(QuestionToBlogOutline(question, context -> blog_outline\n",
       "    instructions='\\n    Your task is to write a blog post that will help answer the given question. \\n    Please use the contexts to evaluate the structure of the blog post.\\n    '\n",
       "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
       "    context = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Context:', 'desc': '${context}'})\n",
       "    blog_outline = Field(annotation=str required=True json_schema_extra={'desc': 'A comma separated list of topics. IMPORTANT!! ONLY SEPARATE TOPICS WITH A COMMA!! ONLY OUTPUT THE TOPICS', '__dspy_field_type': 'output', 'prefix': 'Blog Outline:'})\n",
       "))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'long_text': \"'re willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\\n\\n**DSPy vs\"},\n",
       " {'long_text': ' Data\\n\\nDSPy is a machine learning framework, so working in it involves training sets, development sets, and test sets.\\n\\nFor each example in your data, we distinguish typically between three types of values: the inputs, the intermediate labels, and the final label. You can use DSPy effectively without any intermediate or final labels, but you will need at least a few example inputs.\\n\\n## How much data do I need and how do I collect data for my task'},\n",
       " {'long_text': \"y** philosophy and abstraction differ significantly from other libraries and frameworks, so it's usually straightforward to decide when **DSPy** is (or isn't) the right framework for your usecase. If you're a NLP/AI researcher (or a practitioner exploring new pipelines or new tasks), the answer is generally an invariable **yes**. If you're a practitioner doing other things, please read on.\\n\\n**DSPy vs. thin wrappers for prompts (OpenAI\"},\n",
       " {'long_text': \" to the API References for DSPy! This is where you'll find easy-to-understand information about all the parts of DSPy that you can use in your projects. We've got guides on different tools and helpers that DSPy has, like modules and optimizers. Everything is sorted so you can quickly find what you need. If you're making something and need to quickly get started with DSPy to do certain tasks, this place will show you how to set it\"},\n",
       " {'long_text': 'xiv.org/abs/2310.03714) | N/A | Sections 3, 5, 6, and 7 of the DSPy paper can be consumed as a tutorial. They include explained code snippets, results, and discussions of the abstractions and API.\\n| Intermediate | [**DSPy Assertions**](https://arxiv.org/abs/2312.13382) | [<img align=\"center\" src=\"https://'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_model(\"What is dspy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task is to write a blog post that will help answer the given question. \n",
      "    Please use the contexts to evaluate the structure of the blog post.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Context: ${context}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the blog_outline}. We ...\n",
      "\n",
      "Blog Outline: A comma separated list of topics. IMPORTANT!! ONLY SEPARATE TOPICS WITH A COMMA!! ONLY OUTPUT THE TOPICS\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is DSPy used for?\n",
      "\n",
      "Context:\n",
      "'re willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs to the API References for DSPy! This is where you'll find easy-to-understand information about all the parts of DSPy that you can use in your projects. We've got guides on different tools and helpers that DSPy has, like modules and optimizers. Everything is sorted so you can quickly find what you need. If you're making something and need to quickly get started with DSPy to do certain tasks, this place will show you how to set it Data\n",
      "\n",
      "DSPy is a machine learning framework, so working in it involves training sets, development sets, and test sets.\n",
      "\n",
      "For each example in your data, we distinguish typically between three types of values: the inputs, the intermediate labels, and the final label. You can use DSPy effectively without any intermediate or final labels, but you will need at least a few example inputs.\n",
      "\n",
      "## How much data do I need and how do I collect data for my tasky** philosophy and abstraction differ significantly from other libraries and frameworks, so it's usually straightforward to decide when **DSPy** is (or isn't) the right framework for your usecase. If you're a NLP/AI researcher (or a practitioner exploring new pipelines or new tasks), the answer is generally an invariable **yes**. If you're a practitioner doing other things, please read on.\n",
      "\n",
      "**DSPy vs. thin wrappers for prompts (OpenAI`\n",
      "\n",
      "\n",
      "## What other DSPy modules are there? How can I use them?\n",
      "\n",
      "The others are very similar. They mainly change the internal behavior with which your signature is implemented!\n",
      "\n",
      "1. **`dspy.Predict`**: Basic predictor. Does not modify the signature. Handles the key forms of learning (i.e., storing the instructions and demonstrations and updates to the LM).\n",
      "\n",
      "2. **`dspy.ChainOfTh\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m write a blog post that will help answer the given question. We need to understand what DSPy is used for, consider its differences from other libraries and frameworks, discuss how much data is needed and how to collect it, and finally, explore other DSPy modules and their usage.\n",
      "\n",
      "Blog Outline: Differences, Data Collection, Modules\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicToParagraph(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
    "    \"\"\"\n",
    "\n",
    "    topic = dspy.InputField(desc=\"A topic to write a paragraph about based on the information given.\")\n",
    "    context = dspy.InputField(desc=\"contains relevant information about the topic.\")\n",
    "    paragraph = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DSPy is a lightweight and automatically-optimizing programming model that enables users to implement their own short programs. It offers a lower-level approach compared to higher-level libraries like HuggingFace Transformers, similar to PyTorch. The API References for DSPy provide comprehensive guides on various tools and helpers, including modules and optimizers, organized for easy access. This makes it an ideal choice when you need a lightweight programming model with automatic optimization capabilities.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_topic = \"An Overview of DSPy's features.\"\n",
    "ex_topic_context = dspy.Retrieve(k=5)(ex_topic).passages\n",
    "ex_topic_context = \"\".join(ex_topic_context)\n",
    "dspy.ChainOfThought(TopicToParagraph)(topic=ex_topic, context=ex_topic_context).paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: An Overview of DSPy's features.\n",
      "\n",
      "Context:\n",
      "'re willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs to the API References for DSPy! This is where you'll find easy-to-understand information about all the parts of DSPy that you can use in your projects. We've got guides on different tools and helpers that DSPy has, like modules and optimizers. Everything is sorted so you can quickly find what you need. If you're making something and need to quickly get started with DSPy to do certain tasks, this place will show you how to set itxiv.org/abs/2310.03714) | N/A | Sections 3, 5, 6, and 7 of the DSPy paper can be consumed as a tutorial. They include explained code snippets, results, and discussions of the abstractions and API.\n",
      "| Intermediate | [**DSPy Assertions**](https://arxiv.org/abs/2312.13382) | [<img align=\"center\" src=\"https:// need and how do I collect data for my task?\n",
      "\n",
      "Concretely, you can use DSPy optimizers usefully with as few as 10 example inputs, but having 50-100 examples (or even better, 300-500 examples) goes a long way.\n",
      "\n",
      "How can you get examples like these? If your task is extremely unusual, please invest in preparing ~10 examples by hand. Often times, depending on your metric below, you just need inputs and not labels,`\n",
      "\n",
      "\n",
      "## What other DSPy modules are there? How can I use them?\n",
      "\n",
      "The others are very similar. They mainly change the internal behavior with which your signature is implemented!\n",
      "\n",
      "1. **`dspy.Predict`**: Basic predictor. Does not modify the signature. Handles the key forms of learning (i.e., storing the instructions and demonstrations and updates to the LM).\n",
      "\n",
      "2. **`dspy.ChainOfTh\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m Topic: An Overview of DSPy's features.\n",
      "\n",
      "Context: DSPy is a lightweight, automatically-optimizing programming model that allows users to implement their own short programs. It can be compared to PyTorch, which represents a lower-level approach compared to higher-level libraries like HuggingFace Transformers. The API References for DSPy provide easy-to-understand information about its tools and helpers, such as modules and optimizers, sorted for quick access.\n",
      "\n",
      "Paragraph: DSPy is a lightweight and automatically-optimizing programming model that enables users to implement their own short programs. It offers a lower-level approach compared to higher-level libraries like HuggingFace Transformers, similar to PyTorch. The API References for DSPy provide comprehensive guides on various tools and helpers, including modules and optimizers, organized for easy access. This makes it an ideal choice when you need a lightweight programming model with automatic optimization capabilities.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProofRead(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Proofread a blog post and output a more well written version of the original post.\n",
    "    \"\"\"\n",
    "\n",
    "    blog_post = dspy.InputField()\n",
    "    proofread_blog_post = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleGenerator(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Write a title for a blog post given a description of the topics the blog covers as input.\n",
    "    \"\"\"\n",
    "\n",
    "    blog_outline = dspy.InputField()\n",
    "    title = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining for iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlogPostWriter(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.question_to_blog_outline = dspy.ChainOfThought(QuestionToBlogOutline)\n",
    "        self.topic_to_paragraph = dspy.ChainOfThought(TopicToParagraph)\n",
    "        self.proof_reader = dspy.ChainOfThought(ProofRead)\n",
    "        self.title_generator = dspy.ChainOfThought(TitleGenerator)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = dspy.Retrieve(k=5)(question).passages\n",
    "        context = \"\".join(context)\n",
    "        raw_blog_outline = self.question_to_blog_outline(question=question, context=context).blog_outline\n",
    "        blog_outline = raw_blog_outline.split(\",\")\n",
    "        blog = \"\"\n",
    "        for topic in blog_outline:\n",
    "            topic_contexts = dspy.Retrieve(k=5)(topic).passages\n",
    "            topic_contexts = \"\".join(topic_contexts)\n",
    "            blog += self.topic_to_paragraph(topic=topic, context=context).paragraph\n",
    "            blog += \"\\n \\n\"\n",
    "        blog = self.proof_reader(blog_post=blog).proofread_blog_post\n",
    "        title = self.title_generator(blog_outline=raw_blog_outline).title\n",
    "        final_blog = f\"{title} \\n \\n {blog}\"\n",
    "        return dspy.Prediction(blog=final_blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Exploring Modules, DSPy, and the Differences Between DSPy and LM Client\" \n",
      " \n",
      " A DSPy module is a fundamental component for programs that employ Language Models (LMs). It separates the flow of your program from its parameters, such as LM prompts and weights. This enables DSPy to introduce new `optimizers`, which are algorithms driven by LMs capable of adjusting the prompts and/or weights of your LM calls based on a specific metric you want to maximize. Consequently, **DSPy** distinguishes itself from other frameworks like HuggingFace Transformers, as it offers a lightweight yet automatically-optimizing programming model, making it ideal for constructing intricate systems using LMs without requiring the implementation of custom short programs or manual adjustments to the steps.\n"
     ]
    }
   ],
   "source": [
    "dspy_topic = \"What is DSPy fopr LLMs?\"\n",
    "dspy_blog = BlogPostWriter()(dspy_topic)\n",
    "print(dspy_blog.blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "hi how are you\u001b[32m Hello! I'm doing well, thank you for asking. How can I assist you today?\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task is to write a blog post that will help answer the given question. \n",
      "    Please use the contexts to evaluate the structure of the blog post.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Context: ${context}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the blog_outline}. We ...\n",
      "\n",
      "Blog Outline: A comma separated list of topics. IMPORTANT!! ONLY SEPARATE TOPICS WITH A COMMA!! ONLY OUTPUT THE TOPICS\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is DSPy used for?\n",
      "\n",
      "Context:\n",
      "'re willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs to the API References for DSPy! This is where you'll find easy-to-understand information about all the parts of DSPy that you can use in your projects. We've got guides on different tools and helpers that DSPy has, like modules and optimizers. Everything is sorted so you can quickly find what you need. If you're making something and need to quickly get started with DSPy to do certain tasks, this place will show you how to set it Data\n",
      "\n",
      "DSPy is a machine learning framework, so working in it involves training sets, development sets, and test sets.\n",
      "\n",
      "For each example in your data, we distinguish typically between three types of values: the inputs, the intermediate labels, and the final label. You can use DSPy effectively without any intermediate or final labels, but you will need at least a few example inputs.\n",
      "\n",
      "## How much data do I need and how do I collect data for my tasky** philosophy and abstraction differ significantly from other libraries and frameworks, so it's usually straightforward to decide when **DSPy** is (or isn't) the right framework for your usecase. If you're a NLP/AI researcher (or a practitioner exploring new pipelines or new tasks), the answer is generally an invariable **yes**. If you're a practitioner doing other things, please read on.\n",
      "\n",
      "**DSPy vs. thin wrappers for prompts (OpenAI`\n",
      "\n",
      "\n",
      "## What other DSPy modules are there? How can I use them?\n",
      "\n",
      "The others are very similar. They mainly change the internal behavior with which your signature is implemented!\n",
      "\n",
      "1. **`dspy.Predict`**: Basic predictor. Does not modify the signature. Handles the key forms of learning (i.e., storing the instructions and demonstrations and updates to the LM).\n",
      "\n",
      "2. **`dspy.ChainOfTh\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m write a blog post that will help answer the given question. We need to understand what DSPy is used for, consider its differences from other libraries and frameworks, discuss how much data is needed and how to collect it, and finally, explore other DSPy modules and their usage.\n",
      "\n",
      "Blog Outline: Differences, Data Collection, Modules\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: An Overview of DSPy's features.\n",
      "\n",
      "Context:\n",
      "'re willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs to the API References for DSPy! This is where you'll find easy-to-understand information about all the parts of DSPy that you can use in your projects. We've got guides on different tools and helpers that DSPy has, like modules and optimizers. Everything is sorted so you can quickly find what you need. If you're making something and need to quickly get started with DSPy to do certain tasks, this place will show you how to set itxiv.org/abs/2310.03714) | N/A | Sections 3, 5, 6, and 7 of the DSPy paper can be consumed as a tutorial. They include explained code snippets, results, and discussions of the abstractions and API.\n",
      "| Intermediate | [**DSPy Assertions**](https://arxiv.org/abs/2312.13382) | [<img align=\"center\" src=\"https:// need and how do I collect data for my task?\n",
      "\n",
      "Concretely, you can use DSPy optimizers usefully with as few as 10 example inputs, but having 50-100 examples (or even better, 300-500 examples) goes a long way.\n",
      "\n",
      "How can you get examples like these? If your task is extremely unusual, please invest in preparing ~10 examples by hand. Often times, depending on your metric below, you just need inputs and not labels,`\n",
      "\n",
      "\n",
      "## What other DSPy modules are there? How can I use them?\n",
      "\n",
      "The others are very similar. They mainly change the internal behavior with which your signature is implemented!\n",
      "\n",
      "1. **`dspy.Predict`**: Basic predictor. Does not modify the signature. Handles the key forms of learning (i.e., storing the instructions and demonstrations and updates to the LM).\n",
      "\n",
      "2. **`dspy.ChainOfTh\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m Topic: An Overview of DSPy's features.\n",
      "\n",
      "Context: DSPy is a lightweight, automatically-optimizing programming model that allows users to implement their own short programs. It can be compared to PyTorch, which represents a lower-level approach compared to higher-level libraries like HuggingFace Transformers. The API References for DSPy provide easy-to-understand information about its tools and helpers, such as modules and optimizers, sorted for quick access.\n",
      "\n",
      "Paragraph: DSPy is a lightweight and automatically-optimizing programming model that enables users to implement their own short programs. It offers a lower-level approach compared to higher-level libraries like HuggingFace Transformers, similar to PyTorch. The API References for DSPy provide comprehensive guides on various tools and helpers, including modules and optimizers, organized for easy access. This makes it an ideal choice when you need a lightweight programming model with automatic optimization capabilities.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task is to write a blog post that will help answer the given question. \n",
      "    Please use the contexts to evaluate the structure of the blog post.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Context: ${context}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the blog_outline}. We ...\n",
      "\n",
      "Blog Outline: A comma separated list of topics. IMPORTANT!! ONLY SEPARATE TOPICS WITH A COMMA!! ONLY OUTPUT THE TOPICS\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is DSPy fopr LLMs?\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m Blog Outline: Modules, About DSPy, DSPy vs LM client, About DSPy\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task is to write a blog post that will help answer the given question. \n",
      "    Please use the contexts to evaluate the structure of the blog post.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Context: ${context}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the blog_outline}. We ...\n",
      "\n",
      "Blog Outline: A comma separated list of topics. IMPORTANT!! ONLY SEPARATE TOPICS WITH A COMMA!! ONLY OUTPUT THE TOPICS\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is DSPy fopr LLMs?\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to Blog Outline: Modules, About DSPy, DSPy vs LM client, About DSPy\n",
      "\n",
      "Blog Outline:\u001b[32m Modules, About DSPy, DSPy vs LM client, About DSPy\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: Modules\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the paragraph. We will first define what a DSPy module is and its purpose, then explain how it differs from other frameworks like HuggingFace Transformers, and finally discuss the benefits of using DSPy for optimizing LM prompts and weights.\n",
      "\n",
      "Paragraph: A **DSPy module** is a building block for programs that use LMs (Language Models). It separates the flow of your program from the parameters of each step, such as LM prompts and weights. This allows DSPy to introduce new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize. By doing so, **DSPy** differs from other frameworks like HuggingFace Transformers, as it provides a lightweight but automatically-optimizing programming model. This makes DSPy ideal for when you need to build complex systems using LMs without having to implement your own short program or manually tweak the steps.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: About DSPy\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the paragraph. We will first define what a DSPy module is, then explain its purpose and how it differs from other frameworks like HuggingFace Transformers. Finally, we will discuss the benefits of using DSPy for optimizing LM prompts and weights in complex systems.\n",
      "\n",
      "Paragraph: DSPy is a framework designed to algorithmically optimize LM prompts and weights, especially when LMs are used one or more times within a pipeline. Unlike other frameworks like HuggingFace Transformers, DSPy provides a lightweight, automatically-optimizing programming model. A DSPy module abstracts the building blocks for programs that use LMs, separating the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. This separation allows for new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize. By using DSPy, you can routinely teach powerful models like GPT-3.5 or GPT-\n",
      "\n",
      "---\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: DSPy vs LM client\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the paragraph. We will first understand the context provided and then create a coherent paragraph based on that information.\n",
      "\n",
      "Topic: DSPy vs LM client\n",
      "\n",
      "Context: The context given is about DSPy, which is a framework for algorithmically optimizing LM prompts and weights, especially when LMs are used one or more times within a pipeline. It also mentions the difference between DSPy and LM client, as well as Custom RM client.\n",
      "\n",
      "Paragraph: DSPy is a powerful framework designed to optimize Language Model (LM) prompts and weights, particularly in scenarios where LMs are used multiple times within a pipeline. Unlike the LM client or Custom RM client, DSPy separates the flow of your program into modules and introduces new optimizers that can tune prompts and/or weights based on a specific metric you want to maximize. This allows for more efficient and lightweight programming models without relying on predefined prompts and integrations. Think of it as the difference between using PyTorch and HuggingFace Transformers - DSPy offers more flexibility and control over your LM-driven programs.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: About DSPy\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the paragraph. We will first define what a DSPy module is, then explain its purpose and how it differs from other frameworks like HuggingFace Transformers. Finally, we will discuss the benefits of using DSPy for optimizing LM prompts and weights in complex systems.\n",
      "\n",
      "Paragraph: DSPy is a framework designed to algorithmically optimize LM prompts and weights, especially when LMs are used one or more times within a pipeline. Unlike other frameworks like HuggingFace Transformers, DSPy provides a lightweight, automatically-optimizing programming model. A DSPy module abstracts the building blocks for programs that use LMs, separating the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. This separation allows for new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize. By using DSPy, you can routinely teach powerful models like GPT-3.5 or GPT-\n",
      "\n",
      "---\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Proofread a blog post and output a more well written version of the original post.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Blog Post: ${blog_post}\n",
      "Reasoning: Let's think step by step in order to ${produce the proofread_blog_post}. We ...\n",
      "Proofread Blog Post: ${proofread_blog_post}\n",
      "\n",
      "---\n",
      "\n",
      "Blog Post: A **DSPy module** is a building block for programs that use LMs (Language Models). It separates the flow of your program from the parameters of each step, such as LM prompts and weights. This allows DSPy to introduce new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize. By doing so, **DSPy** differs from other frameworks like HuggingFace Transformers, as it provides a lightweight but automatically-optimizing programming model. This makes DSPy ideal for when you need to build complex systems using LMs without having to implement your own short program or manually tweak the steps. DSPy is a framework designed to algorithmically optimize LM prompts and weights, especially when LMs are used one or more times within a pipeline. Unlike other frameworks like HuggingFace Transformers, DSPy provides a lightweight, automatically-optimizing programming model. A DSPy module abstracts the building blocks for programs that use LMs, separating the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. This separation allows for new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize. By using DSPy, you can routinely teach powerful models like GPT-3.5 or GPT- --- DSPy is a powerful framework designed to optimize Language Model (LM) prompts and weights, particularly in scenarios where LMs are used multiple times within a pipeline. Unlike the LM client or Custom RM client, DSPy separates the flow of your program into modules and introduces new optimizers that can tune prompts and/or weights based on a specific metric you want to maximize. This allows for more efficient and lightweight programming models without relying on predefined prompts and integrations. Think of it as the difference between using PyTorch and HuggingFace Transformers - DSPy offers more flexibility and control over your LM-driven programs. DSPy is a framework designed to algorithmically optimize LM prompts and weights, especially when LMs are used one or more times within a pipeline. Unlike other frameworks like HuggingFace Transformers, DSPy provides a lightweight, automatically-optimizing programming model. A DSPy module abstracts the building blocks for programs that use LMs, separating the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. This separation allows for new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize. By using DSPy, you can routinely teach powerful models like GPT-3.5 or GPT- ---\n",
      "Reasoning: Let's think step by step in order to\u001b[32m Blog Post: A **DSPy module** is a building block for programs that use LMs (Language Models). It separates the flow of your program from the parameters of each step, such as LM prompts and weights. This allows DSPy to introduce new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize. By doing so, **DSPy** differs from other frameworks like HuggingFace Transformers, as it provides a lightweight but automatically-optimizing programming model. This makes DSPy ideal for when you need to build complex systems using LMs without having to implement your own short program or manually tweak the steps.\n",
      "\n",
      "Proofread Blog Post: A DSPy module is a fundamental component for programs that employ Language Models (LMs). It separates the flow of your program from its parameters, such as LM prompts and weights. This enables DSPy to introduce new `optimizers`, which are algorithms driven by LMs capable of adjusting the prompts and/or weights of your LM calls based on a specific metric you want to maximize. Consequently, **DSPy** distinguishes itself from other frameworks like HuggingFace Transformers, as it offers a lightweight yet automatically-optimizing programming model, making it ideal for constructing intricate systems using LMs without requiring the implementation of custom short programs or manual adjustments to the steps.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Write a title for a blog post given a description of the topics the blog covers as input.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Blog Outline: ${blog_outline}\n",
      "Reasoning: Let's think step by step in order to ${produce the title}. We ...\n",
      "Title: ${title}\n",
      "\n",
      "---\n",
      "\n",
      "Blog Outline: Modules, About DSPy, DSPy vs LM client, About DSPy\n",
      "Reasoning: Let's think step by step in order to\u001b[32m Blog Outline: Modules, About DSPy, DSPy vs LM client, About DSPy\n",
      "Reasoning: Let's think step by step in order to create an informative and engaging title for this blog post. We will first introduce the topic of modules, as they are a fundamental aspect of the content. Then, we will discuss DSPy, which is the main focus of the blog. Next, we will compare DSPy with the LM client, highlighting their differences and similarities. Finally, we will provide more information about DSPy to give readers a comprehensive understanding of the topic.\n",
      "\n",
      "Title: \"Exploring Modules, DSPy, and the Differences Between DSPy and LM Client\"\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Improving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlogRater(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Rate a blog post on a scale of 1 to 5 based on how well written it is.\n",
    "    IMPORTANT!! ONLY OUTPUT THE RATING AS A NUMERIC FLOAT VALUE AND NOTHING ELSE!!\n",
    "    \"\"\"\n",
    "\n",
    "    blog = dspy.InputField(desc=\"a blog post\")\n",
    "    rating = dspy.OutputField(desc=\"a quality rating on a scale of 1 to 5.\")\n",
    "\n",
    "\n",
    "class MetricProgram(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rater = dspy.ChainOfThought(BlogRater)\n",
    "\n",
    "    def forward(self, gold, pred, trace=None):\n",
    "        blog = pred.blog\n",
    "        gold = gold.question\n",
    "        return float(self.rater(blog=blog).rating)\n",
    "\n",
    "def metric_wrapper(gold, pred, trace=None):\n",
    "    return MetricProgram()(gold=gold, pred=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is DSPy fopr LLMs?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dspy_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    blog='\"Exploring Modules, DSPy, and the Differences Between DSPy and LM Client\" \\n \\n A DSPy module is a fundamental component for programs that employ Language Models (LMs). It separates the flow of your program from its parameters, such as LM prompts and weights. This enables DSPy to introduce new `optimizers`, which are algorithms driven by LMs capable of adjusting the prompts and/or weights of your LM calls based on a specific metric you want to maximize. Consequently, **DSPy** distinguishes itself from other frameworks like HuggingFace Transformers, as it offers a lightweight yet automatically-optimizing programming model, making it ideal for constructing intricate systems using LMs without requiring the implementation of custom short programs or manual adjustments to the steps.'\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dspy_blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': 'What is DSPy fopr LLMs?'}) (input_keys={'question'})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dspy.Example(question=dspy_topic).with_inputs(\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MetricProgram()(gold=dspy.Example(question=dspy_topic).with_inputs(\"question\"), pred=dspy_blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Your task it to write a paragraph that explains a topic based on the retrieved contexts.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Topic: A topic to write a paragraph about based on the information given.\n",
      "\n",
      "Context: contains relevant information about the topic.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the paragraph}. We ...\n",
      "\n",
      "Paragraph: ${paragraph}\n",
      "\n",
      "---\n",
      "\n",
      "Topic: About DSPy\n",
      "\n",
      "Context:\n",
      "://dspy-docs.vercel.app/docs/building-blocks/optimizers#what-does-a-dspy-optimizer-tune-how-does-it-tune-them) and finetunes.\n",
      "---\n",
      "sidebar_position: 3\n",
      "---\n",
      "\n",
      "# Modules\n",
      "\n",
      "A **DSPy module** is a building block for programs that use LMs.\n",
      "\n",
      "- Each built-in module abstracts a\n",
      "# About DSPy\n",
      "\n",
      "**DSPy is a framework for algorithmically optimizing LM prompts and weights**, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system _without_ DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n",
      "\n",
      "**DSPy vs LM client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client) and [Custom RM client](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client).\n",
      "---\n",
      "sidebar_position: 1\n",
      "---\n",
      "\n",
      "# About DSPy\n",
      "\n",
      "**D things. First, it separates the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. Second, **DSPy** introduces new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize.\n",
      "\n",
      "**DSPy** can routinely teach powerful models like `GPT-3.5` or `GPT-\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the paragraph. We will first define what a DSPy module is, then explain its purpose and how it differs from other frameworks like HuggingFace Transformers. Finally, we will discuss the benefits of using DSPy for optimizing LM prompts and weights in complex systems.\n",
      "\n",
      "Paragraph: DSPy is a framework designed to algorithmically optimize LM prompts and weights, especially when LMs are used one or more times within a pipeline. Unlike other frameworks like HuggingFace Transformers, DSPy provides a lightweight, automatically-optimizing programming model. A DSPy module abstracts the building blocks for programs that use LMs, separating the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. This separation allows for new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize. By using DSPy, you can routinely teach powerful models like GPT-3.5 or GPT-\n",
      "\n",
      "---\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Proofread a blog post and output a more well written version of the original post.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Blog Post: ${blog_post}\n",
      "Reasoning: Let's think step by step in order to ${produce the proofread_blog_post}. We ...\n",
      "Proofread Blog Post: ${proofread_blog_post}\n",
      "\n",
      "---\n",
      "\n",
      "Blog Post: A **DSPy module** is a building block for programs that use LMs (Language Models). It separates the flow of your program from the parameters of each step, such as LM prompts and weights. This allows DSPy to introduce new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize. By doing so, **DSPy** differs from other frameworks like HuggingFace Transformers, as it provides a lightweight but automatically-optimizing programming model. This makes DSPy ideal for when you need to build complex systems using LMs without having to implement your own short program or manually tweak the steps. DSPy is a framework designed to algorithmically optimize LM prompts and weights, especially when LMs are used one or more times within a pipeline. Unlike other frameworks like HuggingFace Transformers, DSPy provides a lightweight, automatically-optimizing programming model. A DSPy module abstracts the building blocks for programs that use LMs, separating the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. This separation allows for new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize. By using DSPy, you can routinely teach powerful models like GPT-3.5 or GPT- --- DSPy is a powerful framework designed to optimize Language Model (LM) prompts and weights, particularly in scenarios where LMs are used multiple times within a pipeline. Unlike the LM client or Custom RM client, DSPy separates the flow of your program into modules and introduces new optimizers that can tune prompts and/or weights based on a specific metric you want to maximize. This allows for more efficient and lightweight programming models without relying on predefined prompts and integrations. Think of it as the difference between using PyTorch and HuggingFace Transformers - DSPy offers more flexibility and control over your LM-driven programs. DSPy is a framework designed to algorithmically optimize LM prompts and weights, especially when LMs are used one or more times within a pipeline. Unlike other frameworks like HuggingFace Transformers, DSPy provides a lightweight, automatically-optimizing programming model. A DSPy module abstracts the building blocks for programs that use LMs, separating the flow of your program (`modules`) from the parameters (LM prompts and weights) of each step. This separation allows for new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize. By using DSPy, you can routinely teach powerful models like GPT-3.5 or GPT- ---\n",
      "Reasoning: Let's think step by step in order to\u001b[32m Blog Post: A **DSPy module** is a building block for programs that use LMs (Language Models). It separates the flow of your program from the parameters of each step, such as LM prompts and weights. This allows DSPy to introduce new `optimizers`, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a `metric` you want to maximize. By doing so, **DSPy** differs from other frameworks like HuggingFace Transformers, as it provides a lightweight but automatically-optimizing programming model. This makes DSPy ideal for when you need to build complex systems using LMs without having to implement your own short program or manually tweak the steps.\n",
      "\n",
      "Proofread Blog Post: A DSPy module is a fundamental component for programs that employ Language Models (LMs). It separates the flow of your program from its parameters, such as LM prompts and weights. This enables DSPy to introduce new `optimizers`, which are algorithms driven by LMs capable of adjusting the prompts and/or weights of your LM calls based on a specific metric you want to maximize. Consequently, **DSPy** distinguishes itself from other frameworks like HuggingFace Transformers, as it offers a lightweight yet automatically-optimizing programming model, making it ideal for constructing intricate systems using LMs without requiring the implementation of custom short programs or manual adjustments to the steps.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Write a title for a blog post given a description of the topics the blog covers as input.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Blog Outline: ${blog_outline}\n",
      "Reasoning: Let's think step by step in order to ${produce the title}. We ...\n",
      "Title: ${title}\n",
      "\n",
      "---\n",
      "\n",
      "Blog Outline: Modules, About DSPy, DSPy vs LM client, About DSPy\n",
      "Reasoning: Let's think step by step in order to\u001b[32m Blog Outline: Modules, About DSPy, DSPy vs LM client, About DSPy\n",
      "Reasoning: Let's think step by step in order to create an informative and engaging title for this blog post. We will first introduce the topic of modules, as they are a fundamental aspect of the content. Then, we will discuss DSPy, which is the main focus of the blog. Next, we will compare DSPy with the LM client, highlighting their differences and similarities. Finally, we will provide more information about DSPy to give readers a comprehensive understanding of the topic.\n",
      "\n",
      "Title: \"Exploring Modules, DSPy, and the Differences Between DSPy and LM Client\"\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rate a blog post on a scale of 1 to 5 based on how well written it is.\n",
      "    IMPORTANT!! ONLY OUTPUT THE RATING AS A NUMERIC FLOAT VALUE AND NOTHING ELSE!!\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Blog: a blog post\n",
      "Reasoning: Let's think step by step in order to ${produce the rating}. We ...\n",
      "Rating: a quality rating on a scale of 1 to 5.\n",
      "\n",
      "---\n",
      "\n",
      "Blog: \"Exploring Modules, DSPy, and the Differences Between DSPy and LM Client\" A DSPy module is a fundamental component for programs that employ Language Models (LMs). It separates the flow of your program from its parameters, such as LM prompts and weights. This enables DSPy to introduce new `optimizers`, which are algorithms driven by LMs capable of adjusting the prompts and/or weights of your LM calls based on a specific metric you want to maximize. Consequently, **DSPy** distinguishes itself from other frameworks like HuggingFace Transformers, as it offers a lightweight yet automatically-optimizing programming model, making it ideal for constructing intricate systems using LMs without requiring the implementation of custom short programs or manual adjustments to the steps.\n",
      "Reasoning: Let's think step by step in order to\u001b[32m Blog: \"Exploring Modules, DSPy, and the Differences Between DSPy and LM Client\" A DSPy module is a fundamental component for programs that employ Language Models (LMs). It separates the flow of your program from its parameters, such as LM prompts and weights. This enables DSPy to introduce new `optimizers`, which are algorithms driven by LMs capable of adjusting the prompts and/or weights of your LM calls based on a specific metric you want to maximize. Consequently, **DSPy** distinguishes itself from other frameworks like HuggingFace Transformers, as it offers a lightweight yet automatically-optimizing programming model, making it ideal for constructing intricate systems using LMs without requiring the implementation of custom short programs or manual adjustments to the steps.\n",
      "Reasoning: Let's think step by step in order to produce the rating. We first need to analyze the content and structure of the blog post. The post provides a clear explanation of DSPy modules, their purpose, and how they differ from other frameworks like HuggingFace Transformers. It also highlights the benefits of using DSPy for constructing complex systems using LMs.\n",
      "Rating: 4.5\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = [\n",
    "    dspy.Example(question=\"How do you set up an LM client?\").with_inputs(\"question\"),\n",
    "    dspy.Example(question=\"Can you make class based signatures?\").with_inputs(\"question\"),\n",
    "    dspy.Example(question=\"How do you create typed predictors?\").with_inputs(\"question\"),\n",
    "]\n",
    "testset = [\n",
    "    dspy.Example(question=\"What is a signature in DSPy?\").with_inputs(\"question\"),\n",
    "    dspy.Example(question=\"How does DSPy handle assertions?\").with_inputs(\"question\"),    \n",
    "    dspy.Example(question=\"What are some evaluation metrics in DSPy?\").with_inputs(\"question\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluate(devset=testset, num_threads=1, display_progress=True, display_table=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 1  (0.0):  33%|███▎      | 1/3 [03:02<06:04, 182.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t could not convert string to float: 'Rating: 4.5'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBlogPostWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_wrapper\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/dspy/evaluate/evaluate.py:160\u001b[0m, in \u001b[0;36mEvaluate.__call__\u001b[0;34m(self, program, metric, devset, num_threads, display_progress, display_table, display, return_all_scores, return_outputs)\u001b[0m\n\u001b[1;32m    157\u001b[0m devset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(devset))\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_threads \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 160\u001b[0m     reordered_devset, ncorrect, ntotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_single_thread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrapped_program\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     reordered_devset, ncorrect, ntotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_multi_thread(\n\u001b[1;32m    164\u001b[0m         wrapped_program,\n\u001b[1;32m    165\u001b[0m         devset,\n\u001b[1;32m    166\u001b[0m         num_threads,\n\u001b[1;32m    167\u001b[0m         display_progress,\n\u001b[1;32m    168\u001b[0m     )\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/dspy/evaluate/evaluate.py:61\u001b[0m, in \u001b[0;36mEvaluate._execute_single_thread\u001b[0;34m(self, wrapped_program, devset, display_progress)\u001b[0m\n\u001b[1;32m     58\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(devset), dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     59\u001b[0m                  disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m display_progress)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, arg \u001b[38;5;129;01min\u001b[39;00m devset:\n\u001b[0;32m---> 61\u001b[0m     example_idx, example, prediction, score \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped_program\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     reordered_devset\u001b[38;5;241m.\u001b[39mappend((example_idx, example, prediction, score))\n\u001b[1;32m     63\u001b[0m     ncorrect \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m score\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/dspy/evaluate/evaluate.py:132\u001b[0m, in \u001b[0;36mEvaluate.__call__.<locals>.wrapped_program\u001b[0;34m(example_idx, example)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# print(threading.get_ident(), dsp.settings.stack_by_thread[threading.get_ident()])\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# print(type(example), example)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mprogram\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     score \u001b[38;5;241m=\u001b[39m metric(\n\u001b[1;32m    134\u001b[0m         example,\n\u001b[1;32m    135\u001b[0m         prediction,\n\u001b[1;32m    136\u001b[0m     )  \u001b[38;5;66;03m# FIXME: TODO: What's the right order? Maybe force name-based kwargs!\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# increment assert and suggest failures to program's attributes\u001b[39;00m\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/dspy/primitives/program.py:26\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 18\u001b[0m, in \u001b[0;36mBlogPostWriter.forward\u001b[0;34m(self, question)\u001b[0m\n\u001b[1;32m     16\u001b[0m     topic_contexts \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mRetrieve(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)(topic)\u001b[38;5;241m.\u001b[39mpassages\n\u001b[1;32m     17\u001b[0m     topic_contexts \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(topic_contexts)\n\u001b[0;32m---> 18\u001b[0m     blog \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopic_to_paragraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mparagraph\n\u001b[1;32m     19\u001b[0m     blog \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m blog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproof_reader(blog_post\u001b[38;5;241m=\u001b[39mblog)\u001b[38;5;241m.\u001b[39mproofread_blog_post\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/dspy/predict/predict.py:49\u001b[0m, in \u001b[0;36mPredict.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py:59\u001b[0m, in \u001b[0;36mChainOfThought.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     signature \u001b[38;5;241m=\u001b[39m new_signature\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# template = dsp.Template(self.signature.instructions, **new_signature)\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/dspy/predict/predict.py:91\u001b[0m, in \u001b[0;36mPredict.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m template \u001b[38;5;241m=\u001b[39m signature_to_template(signature)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     x, C \u001b[38;5;241m=\u001b[39m \u001b[43mdsp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# Note: query_only=True means the instructions and examples are not included.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# I'm not really sure why we'd want to do that, but it's there.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mcontext(lm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm, query_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/dsp/primitives/predict.py:77\u001b[0m, in \u001b[0;36m_generate.<locals>.do_generate\u001b[0;34m(example, stage, max_depth, original_example)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Generate and extract the fields.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m prompt \u001b[38;5;241m=\u001b[39m template(example)\n\u001b[0;32m---> 77\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[Example] \u001b[38;5;241m=\u001b[39m [template\u001b[38;5;241m.\u001b[39mextract(example, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m completions]\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Find the completions that are most complete.\u001b[39;00m\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/dsp/modules/ollama.py:171\u001b[0m, in \u001b[0;36mOllamaLocal.__call__\u001b[0;34m(self, prompt, only_completed, return_sorted, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m only_completed, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor now\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m return_sorted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor now\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 171\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m choices \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    175\u001b[0m completed_choices \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m choices \u001b[38;5;28;01mif\u001b[39;00m c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinish_reason\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/dsp/modules/ollama.py:145\u001b[0m, in \u001b[0;36mOllamaLocal.request\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/dsp/modules/ollama.py:97\u001b[0m, in \u001b[0;36mOllamaLocal.basic_request\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m tot_eval_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m---> 97\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murlstr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout_s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# Check if the request was successful (HTTP status code 200)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;66;03m# If the request was not successful, print an error message\u001b[39;00m\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/urllib3/connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate(BlogPostWriter(), metric=metric_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teleprompter = BootstrapFewShot(metric=metric_wrapper, max_bootstrapped_demos=1, max_rounds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_blog_writer = teleprompter.compile(BlogPostWriter(), trainset=trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_blog_writer.save('./data/compiled_blog_writer.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_blog_writer = BlogPostWriter()\n",
    "compiled_blog_writer.load(\"./data/compiled_blog_writer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LGTM test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporarily remove \\n\\n stop to print whole blog\n",
    "llm = dspy.OllamaLocal(\"zephyr-7b-beta\", max_tokens=3000, model_type=\"chat\")\n",
    "dspy.settings.configure(lm=llm, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compiled_blog_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgtm = compiled_blog_writer(\"What is a signature?\").blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(compiled_blog_writer, metric=metric_wrapper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
