{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "# from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "from dspy.retrieve.pgvector_rm import PgVectorRM\n",
    "from dspy.retrieve.chromadb_rm import ChromadbRM\n",
    "# import weaviate\n",
    "# import weaviate.classes as wvc\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions \n",
    "import fnmatch\n",
    "import uuid\n",
    "import torch\n",
    "from langchain_text_splitters import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn_string = f\"postgresql://daybreak:daybreak@localhost:5432/daybreak\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = psycopg2.connect(conn_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add pgvector extension and create test table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with conn.cursor() as cur:\n",
    "#     cur.execute('CREATE EXTENSION IF NOT EXISTS vector')\n",
    "#     register_vector(conn)\n",
    "#     cur.execute(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS text(\n",
    "#         id SERIAL PRIMARY KEY,\n",
    "#         text TEXT\n",
    "#         );\n",
    "#     \"\"\")\n",
    "# conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_md_files(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for basename in files:\n",
    "            if fnmatch.fnmatch(basename, '*.md'):\n",
    "                filename = os.path.join(root, basename)\n",
    "                yield filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [filename for filename in find_md_files(\"./repos\")] \n",
    "doc_contents = []\n",
    "for i in range(20):\n",
    "    with open(docs[i], 'r') as f:\n",
    "        doc_contents.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = \"\".join(doc_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1332"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_splitter.split_text(all_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = text_splitter.split_text(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up local llm and retriever client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = dspy.OllamaLocal(\"zephyr-7b-beta\", max_tokens=500, stop=['\\n\\n'], model_type=\"chat\")\n",
    "chroma_client = client = chromadb.PersistentClient(path=\"./chroma\")\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "collection = chroma_client.get_or_create_collection(name=\"docs\", embedding_function=default_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(name=docs)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# DSPy Documentation\\n\\nThis website is built using [Docusaurus](https://docusaurus.io/), a modern static website generator.\\n\\n## Contributing to the `docs` Folder\\n\\nThis guide is for contributors looking to make changes to the documentation in the `dspy/docs` folder. \\n\\n1. **Pull the up-to-date version of the website**: Please pull the latest version of the live documentation site via its subtree repository with the following command:\\n\\n```bash\\n#Ensure you are in the top-level dspy/ folder\\ngit subtree pull --prefix=docs https://github.com/krypticmouse/dspy-docs master\\n```\\n\\n2. **Push your new changes on a new branch**: Feel free to add or edit existing documentation and open a PR for your changes. Once your PR is reviewed and approved, the changes will be ready to merge into main. \\n\\n3. **Updating the website**: Once your changes are merged to main, they need to be pushed to the subtree repository that hosts the live documentation site. This step will eventually be done automatically, but for now, please run the following command to push the updated `docs` content to the website subtree repository:\\n\\n```bash\\n#Ensure you are in the top-level dspy/ folder\\ngit subtree push --prefix=docs https://github.com/krypticmouse/dspy-docs master\\n```\\n---\\nsidebar_position: 7\\n---\\n\\n# DSPy Assertions\\n\\nLanguage models (LMs) have transformed how we interact with machine learning, offering vast capabilities in natural language understanding and generation. However, ensuring these models adhere to domain-specific constraints remains a challenge. Despite the growth of techniques like fine-tuning or “prompt engineering”, these approaches are extremely tedious and rely on heavy, manual hand-waving to guide the LMs in adhering to specific constraints. Even DSPy\\'s modularity of programming prompting pipelines lacks mechanisms to effectively and automatically enforce these constraints. \\n\\nTo address this, we introduce DSPy Assertions, a feature within the DSPy framework designed to automate the enforcement of computational constraints on LMs. DSPy Assertions empower developers to guide LMs towards desired outcomes with minimal manual intervention, enhancing the reliability, predictability, and correctness of LM outputs.\\n\\n## dspy.Assert and dspy.Suggest API\\n\\nWe introduce two primary constructs within DSPy Assertions:\\n\\n- **`dspy.Assert`**:\\n  - **Parameters**: \\n    - `constraint (bool)`: Outcome of Python-defined boolean validation check.\\n    - `msg (Optional[str])`: User-defined error message providing feedback or correction guidance.\\n    - `backtrack (Optional[module])`: Specifies target module for retry attempts upon constraint failure. The default backtracking module is the last module before the assertion.\\n  - **Behavior**: Initiates retry  upon failure, dynamically adjusting the pipeline\\'s execution. If failures persist, it halts execution and raises a `dspy.AssertionError`.\\n\\n- **`dspy.Suggest`**:\\n  - **Parameters**: Similar to `dspy.Assert`.\\n  - **Behavior**: Encourages self-refinement through retries without enforcing hard stops. Logs failures after maximum backtracking attempts and continues execution.\\n\\n- **dspy.Assert vs. Python Assertions**: Unlike conventional Python `assert` statements that terminate the program upon failure, `dspy.Assert` conducts a sophisticated retry mechanism, allowing the pipeline to adjust. \\n\\nSpecifically, when a constraint is not met:\\n\\n- Backtracking Mechanism: An under-the-hood backtracking is initiated, offering the model a chance to self-refine and proceed, which is done through\\n- Dynamic Signature Modification: internally modifying your DSPy program’s Signature by adding the following fields:\\n    - Past Output: your model\\'s past output that did not pass the validation_fn\\n    - Instruction: your user-defined feedback message on what went wrong and what possibly to fix\\n\\nIf the error continues past the `max_backtracking_attempts`, then `dspy.Assert` will halt the pipeline execution, altering you with an `dspy.AssertionError`. This ensures your program doesn\\'t continue executing with “bad” LM behavior and immediately highlights sample failure outputs for user assessment. \\n\\n- **dspy.Suggest vs. dspy.Assert**: `dspy.Suggest` on the other hand offers a softer approach. It maintains the same retry backtracking as `dspy.Assert` but instead serves as a gentle nudger. If the model outputs cannot pass the model constraints after the `max_backtracking_attempts`, `dspy.Suggest` will log the persistent failure and continue execution of the program on the rest of the data. This ensures the LM pipeline works in a \"best-effort\" manner without halting execution. \\n\\n- **`dspy.Suggest`** are best utilized as \"helpers\" during the evaluation phase, offering guidance and potential corrections without halting the pipeline.\\n- **`dspy.Assert`** are recommended during the development stage as \"checkers\" to ensure the LM behaves as expected, providing a robust mechanism for identifying and addressing errors early in the development cycle.\\n\\n\\n## Use Case: Including Assertions in DSPy Programs\\n\\nWe start with using an example of a multi-hop QA SimplifiedBaleen pipeline as defined in the intro walkthrough. \\n\\n```python\\nclass SimplifiedBaleen(dspy.Module):\\n    def __init__(self, passages_per_hop=2, max_hops=2):\\n        super().__init__()\\n\\n        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\\n        self.max_hops = max_hops\\n\\n    def forward(self, question):\\n        context = []\\n        prev_queries = [question]\\n\\n        for hop in range(self.max_hops):\\n            query = self.generate_query[hop](context=context, question=question).query\\n            prev_queries.append(query)\\n            passages = self.retrieve(query).passages\\n            context = deduplicate(context + passages)\\n        \\n        pred = self.generate_answer(context=context, question=question)\\n        pred = dspy.Prediction(context=context, answer=pred.answer)\\n        return pred\\n\\nbaleen = SimplifiedBaleen()\\n\\nbaleen(question = \"Which award did Gary Zukav\\'s first book receive?\")\\n```\\n\\nTo include DSPy Assertions, we simply define our validation functions and declare our assertions following the respective model generation. \\n\\nFor this use case, suppose we want to impose the following constraints:\\n    1. Length - each query should be less than 100 characters\\n    2. Uniqueness - each generated query should differ from previously-generated queries. \\n    \\nWe can define these validation checks as boolean functions:\\n\\n```python\\n#simplistic boolean check for query length\\nlen(query) <= 100\\n\\n#Python function for validating distinct queries\\ndef validate_query_distinction_local(previous_queries, query):\\n    \"\"\"check if query is distinct from previous queries\"\"\"\\n    if previous_queries == []:\\n        return True\\n    if dspy.evaluate.answer_exact_match_str(query, previous_queries, frac=0.8):\\n        return False\\n    return True\\n```\\n\\nWe can declare these validation checks through `dspy.Suggest` statements (as we want to test the program in a best-effort demonstration). We want to keep these after the query generation `query = self.generate_query[hop](context=context, question=question).query`.\\n\\n```python\\ndspy.Suggest(\\n    len(query) <= 100,\\n    \"Query should be short and less than 100 characters\",\\n)\\n\\ndspy.Suggest(\\n    validate_query_distinction_local(prev_queries, query),\\n    \"Query should be distinct from: \"\\n    + \"; \".join(f\"{i+1}) {q}\" for i, q in enumerate(prev_queries)),\\n)\\n```\\n\\nIt is recommended to define a program with assertions separately than your original program if you are doing comparative evaluation for the effect of assertions. If not, feel free to set Assertions away!\\n\\nLet\\'s take a look at how the SimplifiedBaleen program will look with Assertions included:\\n\\n```python\\nclass SimplifiedBaleenAssertions(dspy.Module):\\n    def __init__(self, passages_per_hop=2, max_hops=2):\\n        super().__init__()\\n        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\\n        self.max_hops = max_hops\\n\\n    def forward(self, question):\\n        context = []\\n        prev_queries = [question]\\n\\n        for hop in range(self.max_hops):\\n            query = self.generate_query[hop](context=context, question=question).query\\n\\n            dspy.Suggest(\\n                len(query) <= 100,\\n                \"Query should be short and less than 100 characters\",\\n            )\\n\\n            dspy.Suggest(\\n                validate_query_distinction_local(prev_queries, query),\\n                \"Query should be distinct from: \"\\n                + \"; \".join(f\"{i+1}) {q}\" for i, q in enumerate(prev_queries)),\\n            )\\n\\n            prev_queries.append(query)\\n            passages = self.retrieve(query).passages\\n            context = deduplicate(context + passages)\\n        \\n        if all_queries_distinct(prev_queries):\\n            self.passed_suggestions += 1\\n\\n        pred = self.generate_answer(context=context, question=question)\\n        pred = dspy.Prediction(context=context, answer=pred.answer)\\n        return pred\\n```\\n\\nNow calling programs with DSPy Assertions requires one last step, and that is transforming the program to wrap it with internal assertions backtracking and Retry logic. \\n\\n```python\\nfrom dspy.primitives.assertions import assert_transform_module, backtrack_handler\\n\\nbaleen_with_assertions = assert_transform_module(SimplifiedBaleenAssertions(), backtrack_handler)\\n\\n# backtrack_handler is parameterized over a few settings for the backtracking mechanism\\n# To change the number of max retry attempts, you can do\\nbaleen_with_assertions_retry_once = assert_transform_module(SimplifiedBaleenAssertions(), \\n    functools.partial(backtrack_handler, max_backtracks=1))\\n```\\n\\nAlternatively, you can also directly call `activate_assertions` on the program with `dspy.Assert/Suggest` statements using the default backtracking mechanism (`max_backtracks=2`):\\n\\n```python\\nbaleen_with_assertions = SimplifiedBaleenAssertions().activate_assertions()\\n```\\n\\nNow let\\'s take a look at the internal LM backtracking by inspecting the history of the LM query generations. Here we see that when a query fails to pass the validation check of being less than 100 characters, its internal `GenerateSearchQuery` signature is dynamically modified during the backtracking+Retry process to include the past query and the corresponding user-defined instruction: `\"Query should be short and less than 100 characters\"`.\\n\\n\\n```\\nWrite a simple search query that will help answer a complex question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: may contain relevant facts\\n\\nQuestion: ${question}\\n\\nReasoning: Let\\'s think step by step in order to ${produce the query}. We ...\\n\\nQuery: ${query}\\n\\n---\\n\\nContext:\\n[1] «Kerry Condon | Kerry Condon (born 4 January 1983) is [...]»\\n[2] «Corona Riccardo | Corona Riccardo (c. 1878October 15, 1917) was [...]»\\n\\nQuestion: Who acted in the shot film The Shore and is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of \"Hamlet.\" ?\\n\\nReasoning: Let\\'s think step by step in order to find the answer to this question. First, we need to identify the actress who played Ophelia in a Royal Shakespeare Company production of \"Hamlet.\" Then, we need to find out if this actress also acted in the short film \"The Shore.\"\\n\\nQuery: \"actress who played Ophelia in Royal Shakespeare Company production of Hamlet\" + \"actress in short film The Shore\"\\n\\n\\n\\nWrite a simple search query that will help answer a complex question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: may contain relevant facts\\n\\nQuestion: ${question}\\n\\nPast Query: past output with errors\\n\\nInstructions: Some instructions you must satisfy\\n\\nQuery: ${query}\\n\\n---\\n\\nContext:\\n[1] «Kerry Condon | Kerry Condon (born 4 January 1983) is an Irish television and film actress, best known for her role as Octavia of the Julii in the HBO/BBC series \"Rome,\" as Stacey Ehrmantraut in AMC\\'s \"Better Call Saul\" and as the voice of F.R.I.D.A.Y. in various films in the Marvel Cinematic Universe. She is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of \"Hamlet.\"»\\n[2] «Corona Riccardo | Corona Riccardo (c. 1878October 15, 1917) was an Italian born American actress who had a brief Broadway stage career before leaving to become a wife and mother. Born in Naples she came to acting in 1894 playing a Mexican girl in a play at the Empire Theatre. Wilson Barrett engaged her for a role in his play \"The Sign of the Cross\" which he took on tour of the United States. Riccardo played the role of Ancaria and later played Berenice in the same play. Robert B. Mantell in 1898 who struck by her beauty also cast her in two Shakespeare plays, \"Romeo and Juliet\" and \"Othello\". Author Lewis Strang writing in 1899 said Riccardo was the most promising actress in America at the time. Towards the end of 1898 Mantell chose her for another Shakespeare part, Ophelia im Hamlet. Afterwards she was due to join Augustin Daly\\'s Theatre Company but Daly died in 1899. In 1899 she gained her biggest fame by playing Iras in the first stage production of Ben-Hur.»\\n\\nQuestion: Who acted in the shot film The Shore and is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of \"Hamlet.\" ?\\n\\nPast Query: \"actress who played Ophelia in Royal Shakespeare Company production of Hamlet\" + \"actress in short film The Shore\"\\n\\nInstructions: Query should be short and less than 100 characters\\n\\nQuery: \"actress Ophelia RSC Hamlet\" + \"actress The Shore\"\\n\\n```\\n\\n\\n## Assertion-Driven Optimizations\\n\\nDSPy Assertions work with optimizations that DSPy offers, particularly with `BootstrapFewShotWithRandomSearch`, including the following settings:\\n\\n- Compilation with Assertions\\n    This includes assertion-driven example bootstrapping and counterexample bootstrapping during compilation. The teacher model for bootstrapping few-shot demonstrations can make use of DSPy Assertions to offer robust bootstrapped examples for the student model to learn from during inference. In this setting, the student model does not perform assertion aware optimizations (backtracking and retry) during inference.\\n- Compilation + Inference with Assertions\\n    -This includes assertion-driven optimizations in both compilation and inference. Now the teacher model offers assertion-driven examples but the student can further optimize with assertions of its own during inference time. \\n```python\\nteleprompter = BootstrapFewShotWithRandomSearch(\\n    metric=validate_context_and_answer_and_hops,\\n    max_bootstrapped_demos=max_bootstrapped_demos,\\n    num_candidate_programs=6,\\n)\\n\\n#Compilation with Assertions\\ncompiled_with_assertions_baleen = teleprompter.compile(student = baleen, teacher = baleen_with_assertions, trainset = trainset, valset = devset)\\n\\n#Compilation + Inference with Assertions\\ncompiled_baleen_with_assertions = teleprompter.compile(student=baleen_with_assertions, teacher = baleen_with_assertions, trainset=trainset, valset=devset)\\n\\n```---\\nsidebar_position: 1\\n---\\n\\n# API References\\n\\nWelcome to the API References for DSPy! This is where you\\'ll find easy-to-understand information about all the parts of DSPy that you can use in your projects. We\\'ve got guides on different tools and helpers that DSPy has, like modules and optimizers. Everything is sorted so you can quickly find what you need. If you\\'re making something and need to quickly get started with DSPy to do certain tasks, this place will show you how to set it up and get it working just right.---\\nsidebar_position: 9\\n---\\n\\n# dsp.Mistral\\n\\n### Usage\\n\\n```python\\nlm = dsp.Mistral(model=\\'mistral-medium-latest\\', api_key=\"your-mistralai-api-key\")\\n```\\n\\n### Constructor\\n\\nThe constructor initializes the base class `LM` and verifies the `api_key` provided or defined through the `MISTRAL_API_KEY` environment variable.\\n\\n```python\\nclass Mistral(LM):\\n    def __init__(\\n        self,\\n        model: str = \"mistral-medium-latest\",\\n        api_key: Optional[str] = None,\\n        **kwargs,\\n    ):\\n```\\n\\n**Parameters:**\\n- `model` (_str_): Mistral AI pretrained models. Defaults to `mistral-medium-latest`.\\n- `api_key` (_Optional[str]_, _optional_): API provider from Mistral AI. Defaults to None.\\n- `**kwargs`: Additional language model arguments to pass to the API provider.\\n\\n### Methods\\n\\nRefer to [`dspy.Mistral`](#) documentation.\\n---\\nsidebar_position: 8\\n---\\n\\n# dspy.Databricks\\n\\n### Usage\\n```python\\nlm = dspy.Databricks(model=\"databricks-mpt-30b-instruct\")\\n```\\n\\n### Constructor\\n\\nThe constructor inherits from the `GPT3` class and verifies the Databricks authentication credentials for using Databricks Model Serving API through the OpenAI SDK.\\nWe expect the following environment variables to be set:\\n- `openai.api_key`: Databricks API key.\\n- `openai.base_url`: Databricks Model Endpoint url\\n\\nThe `kwargs` attribute is initialized with default values for relevant text generation parameters needed for communicating with the Databricks OpenAI SDK, such as `temperature`, `max_tokens`, `top_p`, and `n`. However, it removes the `frequency_penalty` and `presence_penalty` arguments as these are not currently supported by the Databricks API.\\n\\n```python\\nclass Databricks(GPT3):\\n    def __init__(\\n        self,\\n        model: str,\\n        api_key: Optional[str] = None,\\n        api_base: Optional[str] = None,\\n        model_type: Literal[\"chat\", \"text\"] = None,\\n        **kwargs,\\n    ):\\n```\\n\\n**Parameters:**\\n- `model` (_str_): models hosted on Databricks.\\n- `stop` (_List[str]_, _optional_): List of stopping tokens to end generation.\\n- `api_key` (_Optional[str]_): Databricks API key. Defaults to None\\n- `api_base` (_Optional[str]_): Databricks Model Endpoint url Defaults to None.\\n- `model_type` (_Literal[\"chat\", \"text\", \"embeddings\"]_): Specified model type to use.\\n- `**kwargs`: Additional language model arguments to pass to the API provider.\\n\\n### Methods\\n\\nRefer to [`dspy.OpenAI`](https://dspy-docs.vercel.app/api/language_model_clients/OpenAI) documentation.---\\nsidebar_position: 4\\n---\\n\\n# dspy.HFClientTGI\\n\\n### Usage\\n\\n```python\\nlm = dspy.HFClientTGI(model=\"meta-llama/Llama-2-7b-hf\", port=8080, url=\"http://localhost\")\\n```\\n\\n### Prerequisites\\n\\nRefer to the [Text Generation-Inference Server](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/local_models/HFClientTGI) section of the `Using Local Models` documentation.\\n\\n### Constructor\\n\\nThe constructor initializes the `HFModel` base class and configures the client for communicating with the TGI server. It requires a `model` instance, communication `port` for the server, and the `url` for the server to host generate requests. Additional configuration can be provided via keyword arguments in `**kwargs`.\\n\\n```python\\nclass HFClientTGI(HFModel):\\n    def __init__(self, model, port, url=\"http://future-hgx-1\", **kwargs):\\n```\\n\\n**Parameters:**\\n- `model` (_HFModel_): Instance of Hugging Face model connected to the TGI server.\\n- `port` (_int_): Port for TGI server.\\n- `url` (_str_): Base URL where the TGI server is hosted. \\n- `**kwargs`: Additional keyword arguments to configure the client.\\n\\n### Methods\\n\\nRefer to [`dspy.OpenAI`](https://dspy-docs.vercel.app/api/language_model_clients/OpenAI) documentation.---\\nsidebar_position: 3\\n---\\n\\n# dsp.Cohere\\n\\n### Usage\\n\\n```python\\nlm = dsp.Cohere(model=\\'command-nightly\\')\\n```\\n\\n### Constructor\\n\\nThe constructor initializes the base class `LM` and verifies the `api_key` to set up Cohere request retrieval.\\n\\n```python\\nclass Cohere(LM):\\n    def __init__(\\n        self,\\n        model: str = \"command-nightly\",\\n        api_key: Optional[str] = None,\\n        stop_sequences: List[str] = [],\\n    ):\\n```\\n\\n**Parameters:**\\n- `model` (_str_): Cohere pretrained models. Defaults to `command-nightly`.\\n- `api_key` (_Optional[str]_, _optional_): API provider from Cohere. Defaults to None.\\n- `stop_sequences` (_List[str]_, _optional_): List of stopping tokens to end generation.\\n\\n### Methods\\n\\nRefer to [`dspy.OpenAI`](https://dspy-docs.vercel.app/api/language_model_clients/OpenAI) documentation.\\n---\\nsidebar_position: 2\\n---\\n\\n# dspy.AzureOpenAI\\n\\n### Usage\\n\\n```python\\nlm = dspy.AzureOpenAI(api_base=\\'...\\', api_version=\\'2023-12-01-preview\\', model=\\'gpt-3.5-turbo\\')\\n```\\n\\n### Constructor\\n\\nThe constructor initializes the base class `LM` and verifies the provided arguments like the `api_provider`, `api_key`, and `api_base` to set up OpenAI request retrieval through Azure. The `kwargs` attribute is initialized with default values for relevant text generation parameters needed for communicating with the GPT API, such as `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, and `n`.\\n\\n```python\\nclass AzureOpenAI(LM):\\n    def __init__(\\n        self,\\n        api_base: str,\\n        api_version: str,\\n        model: str = \"gpt-3.5-turbo-instruct\",\\n        api_key: Optional[str] = None,\\n        model_type: Literal[\"chat\", \"text\"] = None,\\n        **kwargs,\\n    ):\\n```\\n\\n\\n\\n**Parameters:** \\n- `api_base` (str): Azure Base URL.\\n- `api_version` (str): Version identifier for Azure OpenAI API.\\n- `api_key` (_Optional[str]_, _optional_): API provider authentication token. Retrieves from `AZURE_OPENAI_KEY` environment variable if None.\\n- `model_type` (_Literal[\"chat\", \"text\"]_): Specified model type to use, defaults to \\'chat\\'.\\n- `**kwargs`: Additional language model arguments to pass to the API provider.\\n\\n### Methods\\n\\n#### `__call__(self, prompt: str, only_completed: bool = True, return_sorted: bool = False, **kwargs) -> List[Dict[str, Any]]`\\n\\nRetrieves completions from Azure OpenAI Endpoints by calling `request`. \\n\\nInternally, the method handles the specifics of preparing the request prompt and corresponding payload to obtain the response.\\n\\nAfter generation, the completions are post-processed based on the `model_type` parameter. If the parameter is set to \\'chat\\', the generated content look like `choice[\"message\"][\"content\"]`. Otherwise, the generated text will be `choice[\"text\"]`.\\n\\n**Parameters:**\\n- `prompt` (_str_): Prompt to send to Azure OpenAI.\\n- `only_completed` (_bool_, _optional_): Flag to return only completed responses and ignore completion due to length. Defaults to True.\\n- `return_sorted` (_bool_, _optional_): Flag to sort the completion choices using the returned averaged log-probabilities. Defaults to False.\\n- `**kwargs`: Additional keyword arguments for completion request.\\n\\n**Returns:**\\n- `List[Dict[str, Any]]`: List of completion choices.---\\nsidebar_position: 7\\n---\\n\\n# dspy.Together\\n\\n### Usage\\n\\n```python\\nlm = dspy.Together(model=\"mistralai/Mistral-7B-v0.1\")\\n```\\n\\n### Constructor\\n\\nThe constructor initializes the base class `LM` and verifies the `api_key` for using Together API.\\nWe expect the following environment variables to be set:\\n- `TOGETHER_API_KEY`: API key for Together.\\n- `TOGETHER_API_BASE`: API base URL for Together.\\n\\n\\n```python\\nclass Together(HFModel):\\n    def __init__(self, model, **kwargs):\\n```\\n\\n**Parameters:**\\n- `model` (_str_): models hosted on Together.\\n- `stop` (_List[str]_, _optional_): List of stopping tokens to end generation.\\n\\n### Methods\\n\\nRefer to [`dspy.OpenAI`](https://dspy-docs.vercel.app/api/language_model_clients/OpenAI) documentation.---\\nsidebar_position: 6\\n---\\n\\n# dspy.Anyscale\\n\\n### Usage\\n\\n```python\\nlm = dspy.Anyscale(model=\"mistralai/Mistral-7B-Instruct-v0.1\")\\n```\\n\\n### Constructor\\n\\nThe constructor initializes the base class `LM` and verifies the `api_key` for using Anyscale API.\\nWe expect the following environment variables to be set:\\n- `ANYSCALE_API_KEY`: API key for Together.\\n- `ANYSCALE_API_BASE`: API base URL for Together.\\n\\n\\n```python\\nclass Anyscale(HFModel):\\n    def __init__(self, model, **kwargs):\\n```\\n\\n**Parameters:**\\n- `model` (_str_): models hosted on Together.\\n\\n### Methods\\n\\nRefer to [`dspy.OpenAI`](https://dspy-docs.vercel.app/api/language_model_clients/OpenAI) documentation.\\n---\\nsidebar_position: 1\\n---\\n\\n# dspy.OpenAI\\n\\n### Usage\\n\\n```python\\nlm = dspy.OpenAI(model=\\'gpt-3.5-turbo\\')\\n```\\n\\n### Constructor\\n\\nThe constructor initializes the base class `LM` and verifies the provided arguments like the `api_provider`, `api_key`, and `api_base` to set up OpenAI request retrieval. The `kwargs` attribute is initialized with default values for relevant text generation parameters needed for communicating with the GPT API, such as `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, and `n`.\\n\\n```python\\nclass OpenAI(LM):\\n    def __init__(\\n        self,\\n        model: str = \"text-davinci-002\",\\n        api_key: Optional[str] = None,\\n        api_provider: Literal[\"openai\"] = \"openai\",\\n        model_type: Literal[\"chat\", \"text\"] = None,\\n        **kwargs,\\n    ):\\n```\\n\\n\\n\\n**Parameters:** \\n- `api_key` (_Optional[str]_, _optional_): API provider authentication token. Defaults to None.\\n- `api_provider` (_Literal[\"openai\"]_, _optional_): API provider to use. Defaults to \"openai\".\\n- `model_type` (_Literal[\"chat\", \"text\"]_): Specified model type to use.\\n- `**kwargs`: Additional language model arguments to pass to the API provider.\\n\\n### Methods\\n\\n#### `__call__(self, prompt: str, only_completed: bool = True, return_sorted: bool = False, **kwargs) -> List[Dict[str, Any]]`\\n\\nRetrieves completions from OpenAI by calling `request`. \\n\\nInternally, the method handles the specifics of preparing the request prompt and corresponding payload to obtain the response.\\n\\nAfter generation, the completions are post-processed based on the `model_type` parameter. If the parameter is set to \\'chat\\', the generated content look like `choice[\"message\"][\"content\"]`. Otherwise, the generated text will be `choice[\"text\"]`.\\n\\n**Parameters:**\\n- `prompt` (_str_): Prompt to send to OpenAI.\\n- `only_completed` (_bool_, _optional_): Flag to return only completed responses and ignore completion due to length. Defaults to True.\\n- `return_sorted` (_bool_, _optional_): Flag to sort the completion choices using the returned averaged log-probabilities. Defaults to False.\\n- `**kwargs`: Additional keyword arguments for completion request.\\n\\n**Returns:**\\n- `List[Dict[str, Any]]`: List of completion choices.---\\nsidebar_position: 5\\n---\\n\\n# dspy.HFClientVLLM\\n\\n### Usage\\n\\n```python\\nlm = dspy.HFClientVLLM(model=\"meta-llama/Llama-2-7b-hf\", port=8080, url=\"http://localhost\")\\n```\\n\\n### Prerequisites\\n\\nRefer to the [vLLM Server](https://dspy-docs.vercel.app/api/language_model_clients/HFClientVLLM) section of the `Using Local Models` documentation.\\n\\n### Constructor\\n\\nRefer to [`dspy.TGI`](https://dspy-docs.vercel.app/api/language_model_clients/TGI) documentation. Replace with `HFClientVLLM`.\\n\\n### Methods\\n\\nRefer to [`dspy.OpenAI`](https://dspy-docs.vercel.app/api/language_model_clients/OpenAI) documentation.---\\nsidebar_position: 4\\n---\\n\\n# retrieve.FaissRM\\n\\n### Constructor\\n\\nInitialize an instance of FaissRM by providing it with a vectorizer and a list of strings\\n\\n```python\\nFaissRM(\\n    document_chunks: List[str],\\n    vectorizer: dsp.modules.sentence_vectorizer.BaseSentenceVectorizer,\\n    k: int = 3\\n)\\n```\\n\\n**Parameters:**\\n- `document_chunks` (_List[str]_): a list of strings that comprises the corpus to search. You cannot add/insert/upsert to this list after creating this FaissRM object.\\n- `vectorizer` (_dsp.modules.sentence_vectorizer.BaseSentenceVectorizer_, _optional_): If not provided, a dsp.modules.sentence_vectorizer.SentenceTransformersVectorizer object is created and used.\\n- `k` (_int_, _optional_): The number of top passages to retrieve. Defaults to 3.\\n\\n### Methods\\n\\n#### `forward(self, query_or_queries: Union[str, List[str]]) -> dspy.Prediction`\\n\\nSearch the FaissRM vector database for the top `k` passages matching the given query or queries, using embeddings generated via the vectorizer specified at FaissRM construction time\\n\\n**Parameters:**\\n- `query_or_queries` (_Union[str, List[str]]_): The query or list of queries to search for.\\n\\n**Returns:**\\n- `dspy.Prediction`: Contains the retrieved passages, each represented as a `dotdict` with a `long_text` attribute and an `index` attribute. The `index` attribute is the index in the document_chunks array provided to this FaissRM object at construction time.\\n\\n### Quickstart with the default vectorizer\\n\\nThe **FaissRM** module provides a retriever that uses an in-memory Faiss vector database. This module does not include a vectorizer; instead it supports any subclass of **dsp.modules.sentence_vectorizer.BaseSentenceVectorizer**. If a vectorizer is not provided, an instance of **dsp.modules.sentence_vectorizer.SentenceTransformersVectorizer** is created and used by **FaissRM**. Note that the default embedding model for **SentenceTransformersVectorizer** is **all-MiniLM-L6-v2**\\n\\n\\n```python\\nimport dspy\\nfrom dspy.retrieve import FaissRM\\n\\ndocument_chunks = [\\n    \"The superbowl this year was played between the San Francisco 49ers and the Kanasas City Chiefs\",\\n    \"Pop corn is often served in a bowl\",\\n    \"The Rice Bowl is a Chinese Restaurant located in the city of Tucson, Arizona\",\\n    \"Mars is the fourth planet in the Solar System\",\\n    \"An aquarium is a place where children can learn about marine life\",\\n    \"The capital of the United States is Washington, D.C\",\\n    \"Rock and Roll musicians are honored by being inducted in the Rock and Roll Hall of Fame\",\\n    \"Music albums were published on Long Play Records in the 70s and 80s\",\\n    \"Sichuan cuisine is a spicy cuisine from central China\",\\n    \"The interest rates for mortgages is considered to be very high in 2024\",\\n]\\n\\nfrm = FaissRM(document_chunks)\\nturbo = dspy.OpenAI(model=\"gpt-3.5-turbo\")\\ndspy.settings.configure(lm=turbo, rm=frm)\\nprint(frm([\"I am in the mood for Chinese food\"]))\\n```\\n# retrieve.neo4j_rm\\n\\n### Constructor\\n\\nInitialize an instance of the `Neo4jRM` class.\\n\\n```python\\nNeo4jRM(\\n    index_name: str,\\n    text_node_property: str,\\n    k: int = 5,\\n    retrieval_query: str = None,\\n    embedding_provider: str = \"openai\",\\n    embedding_model: str = \"text-embedding-ada-002\",\\n)\\n```\\n\\n**Environment Variables:**\\n\\nYou need to define the credentials as environment variables:\\n\\n- `NEO4J_USERNAME` (_str_): Specifies the username required for authenticating with the Neo4j database. This is a crucial security measure to ensure that only authorized users can access the database.\\n\\n- `NEO4J_PASSWORD` (_str_): Defines the password associated with the `NEO4J_USERNAME` for authentication purposes. This password should be kept secure to prevent unauthorized access to the database.\\n\\n- `NEO4J_URI` (_str_): Indicates the Uniform Resource Identifier (URI) used to connect to the Neo4j database. This URI typically includes the protocol, hostname, and port, providing the necessary information to establish a connection to the database.\\n\\n- `NEO4J_DATABASE` (_str_, optional): Specifies the name of the database to connect to within the Neo4j instance. If not set, the system defaults to using `\"neo4j\"` as the database name. This allows for flexibility in connecting to different databases within a single Neo4j server.\\n\\n- `OPENAI_API_KEY` (_str_): Specifies the API key required for authenticiating with OpenAI\\'s services.\\n\\n**Parameters:**\\n- `index_name` (_str_): Specifies the name of the vector index to be used within Neo4j for organizing and querying data.\\n- `text_node_property` (_str_, _optional_): Defines the specific property of nodes that will be returned.\\n- `k` (_int_, _optional_): The number of top results to return from the retrieval operation. It defaults to 5 if not explicitly specified.\\n- `retrieval_query` (_str_, _optional_): A custom query string provided for retrieving data. If not provided, a default query tailored to the `text_node_property` will be used.\\n- `embedding_provider` (_str_, _optional_): The name of the service provider for generating embeddings. Defaults to \"openai\" if not specified.\\n- `embedding_model` (_str_, _optional_): The specific embedding model to use from the provider. By default, it uses the \"text-embedding-ada-002\" model from OpenAI.\\n\\n\\n### Methods\\n\\n#### `forward(self, query: [str], k: Optional[int] = None) -> dspy.Prediction`\\n\\nSearch the neo4j vector index for the top `k` passages matching the given query or queries, using embeddings generated via the specified `embedding_model`.\\n\\n**Parameters:**\\n- `query` (str_): The query.\\n- `k` (_Optional[int]_, _optional_): The number of results to retrieve. If not specified, defaults to the value set during initialization.\\n\\n**Returns:**\\n- `dspy.Prediction`: Contains the retrieved passages as a list of string with the prediction signature.\\n\\nex:\\n```python\\nPrediction(\\n    passages=[\\'Passage 1 Lorem Ipsum awesome\\', \\'Passage 2 Lorem Ipsum Youppidoo\\', \\'Passage 3 Lorem Ipsum Yassssss\\']\\n)\\n```\\n\\n### Quick Example how to use Neo4j in a local environment. \\n\\n\\n```python\\nfrom dspy.retrieve.neo4j_rm import Neo4jRM\\nimport os\\n\\nos.environ[\"NEO4J_URI\"] = \\'bolt://localhost:7687\\'\\nos.environ[\"NEO4J_USERNAME\"] = \\'neo4j\\'\\nos.environ[\"NEO4J_PASSWORD\"] = \\'password\\'\\nos.environ[\"OPENAI_API_KEY\"] = \\'sk-\\'\\n\\nretriever_model = Neo4jRM(\\n    index_name=\"vector\",\\n    text_node_property=\"text\"\\n)\\n\\nresults = retriever_model(\"Explore the significance of quantum computing\", k=3)\\n\\nfor passage in results:\\n    print(\"Document:\", passage, \"\\\\n\")\\n```\\n---\\nsidebar_position: 3\\n---\\n\\n# retrieve.AzureCognitiveSearch\\n\\n### Constructor\\n\\nThe constructor initializes an instance of the `AzureCognitiveSearch` class and sets up parameters for sending queries and retreiving results with the Azure Cognitive Search server.\\n\\n```python\\nclass AzureCognitiveSearch:\\n    def __init__(\\n        self,\\n        search_service_name: str,\\n        search_api_key: str,\\n        search_index_name: str,\\n        field_text: str,\\n        field_score: str, # required field to map with \"score\" field in dsp framework\\n    ):\\n```\\n\\n**Parameters:**\\n\\n- `search_service_name` (_str_): Name of Azure Cognitive Search server.\\n- `search_api_key` (_str_): API Authentication token for accessing Azure Cognitive Search server.\\n- `search_index_name` (_str_): Name of search index in the Azure Cognitive Search server.\\n- `field_text` (_str_): Field name that maps to DSP \"content\" field.\\n- `field_score` (_str_): Field name that maps to DSP \"score\" field.\\n\\n### Methods\\n\\nRefer to [ColBERTv2](/api/retrieval_model_clients/ColBERTv2) documentation. Keep in mind there is no `simplify` flag for AzureCognitiveSearch.\\n\\nAzureCognitiveSearch supports sending queries and processing the received results, mapping content and scores to a correct format for the Azure Cognitive Search server.\\n\\n### Deprecation Notice\\n\\nThis module is scheduled for removal in future releases. Please use the AzureAISearchRM class from dspy.retrieve.azureaisearch_rm instead.For more information, refer to the updated documentation(docs/docs/deep-dive/retrieval_models_clients/Azure.mdx).\\n---\\nsidebar_position: 1\\n---\\n\\n# dspy.ColBERTv2\\n\\n### Constructor\\n\\nThe constructor initializes the `ColBERTv2` class instance and sets up the request parameters for interacting with the ColBERTv2 server.\\n\\n```python\\nclass ColBERTv2:\\n    def __init__(\\n        self,\\n        url: str = \"http://0.0.0.0\",\\n        port: Optional[Union[str, int]] = None,\\n        post_requests: bool = False,\\n    ):\\n```\\n\\n**Parameters:**\\n- `url` (_str_): URL for ColBERTv2 server.\\n- `port` (_Union[str, int]_, _Optional_): Port endpoint for ColBERTv2 server. Defaults to `None`.\\n- `post_requests` (_bool_, _Optional_): Flag for using HTTP POST requests. Defaults to `False`.\\n\\n### Methods\\n\\n#### `__call__(self, query: str, k: int = 10, simplify: bool = False) -> Union[list[str], list[dotdict]]`\\n\\nEnables making queries to the ColBERTv2 server for retrieval. Internally, the method handles the specifics of preparing the request prompt and corresponding payload to obtain the response. The function handles the retrieval of the top-k passages based on the provided query.\\n\\n**Parameters:**\\n- `query` (_str_): Query string used for retrieval.\\n- `k` (_int_, _optional_): Number of passages to retrieve. Defaults to 10.\\n- `simplify` (_bool_, _optional_): Flag for simplifying output to a list of strings. Defaults to False.\\n\\n**Returns:**\\n- `Union[list[str], list[dotdict]]`: Depending on `simplify` flag, either a list of strings representing the passage content (`True`) or a list of `dotdict` instances containing passage details (`False`).\\n\\n### Quickstart\\n\\n```python\\nimport dspy\\n\\ncolbertv2_wiki17_abstracts = dspy.ColBERTv2(url=\\'http://20.102.90.50:2017/wiki17_abstracts\\')\\n\\nretrieval_response = colbertv2_wiki17_abstracts(\\'When was the first FIFA World Cup held?\\', k=5)\\n\\nfor result in retrieval_response:\\n    print(\"Text:\", result[\\'text\\'], \"\\\\n\")\\n```\\n---\\nsidebar_position: 2\\n---\\n\\n# retrieve.ChromadbRM\\n\\n### Constructor\\n\\nInitialize an instance of the `ChromadbRM` class, with the option to use OpenAI\\'s embeddings or any alternative supported by chromadb, as detailed in the official [chromadb embeddings documentation](https://docs.trychroma.com/embeddings).\\n\\n```python\\nChromadbRM(\\n    collection_name: str,\\n    persist_directory: str,\\n    embedding_function: Optional[EmbeddingFunction[Embeddable]] = OpenAIEmbeddingFunction(),\\n    k: int = 7,\\n)\\n```\\n\\n**Parameters:**\\n- `collection_name` (_str_): The name of the chromadb collection.\\n- `persist_directory` (_str_): Path to the directory where chromadb data is persisted.\\n- `embedding_function` (_Optional[EmbeddingFunction[Embeddable]]_, _optional_): The function used for embedding documents and queries. Defaults to `DefaultEmbeddingFunction()` if not specified.\\n- `k` (_int_, _optional_): The number of top passages to retrieve. Defaults to 7.\\n\\n### Methods\\n\\n#### `forward(self, query_or_queries: Union[str, List[str]], k: Optional[int] = None) -> dspy.Prediction`\\n\\nSearch the chromadb collection for the top `k` passages matching the given query or queries, using embeddings generated via the specified `embedding_function`.\\n\\n**Parameters:**\\n- `query_or_queries` (_Union[str, List[str]]_): The query or list of queries to search for.\\n- `k` (_Optional[int]_, _optional_): The number of results to retrieve. If not specified, defaults to the value set during initialization.\\n\\n**Returns:**\\n- `dspy.Prediction`: Contains the retrieved passages, each represented as a `dotdict` with schema `[{\"id\": str, \"score\": float, \"long_text\": str, \"metadatas\": dict }]`\\n\\n### Quickstart with OpenAI Embeddings\\n\\nChromadbRM have the flexibility from a variety of embedding functions as outlined in the [chromadb embeddings documentation](https://docs.trychroma.com/embeddings). While different options are available, this example demonstrates how to utilize OpenAI embeddings specifically.\\n\\n```python\\nfrom dspy.retrieve.chromadb_rm import ChromadbRM\\nimport os\\nimport openai\\nfrom chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\\n\\nembedding_function = OpenAIEmbeddingFunction(\\n    api_key=os.environ.get(\\'OPENAI_API_KEY\\'),\\n    model_name=\"text-embedding-ada-002\"\\n)\\n\\nretriever_model = ChromadbRM(\\n    \\'your_collection_name\\',\\n    \\'/path/to/your/db\\',\\n    embedding_function=embedding_function,\\n    k=5\\n)\\n\\nresults = retriever_model(\"Explore the significance of quantum computing\", k=5)\\n\\nfor result in results:\\n    print(\"Document:\", result.long_text, \"\\\\n\")\\n```\\n# dspy.OllamaLocal\\n\\n:::note\\nAdapted from documentation provided by https://github.com/insop\\n:::\\n\\nOllama is a good software tool that allows you to run LLMs locally, such as Mistral, Llama2, and Phi.\\nThe following are the instructions to install and run Ollama.\\n\\n### Prerequisites\\n\\nInstall Ollama by following the instructions from this page:\\n\\n- https://ollama.ai\\n\\nDownload model: `ollama pull`\\n\\nDownload a model by running the `ollama pull` command. You can download Mistral, Llama2, and Phi.\\n\\n```bash\\n# download mistral\\nollama pull mistral\\n```\\n\\nHere is the list of other models you can download:\\n- https://ollama.ai/library\\n\\n### Running Ollama model\\n\\nRun model: `ollama run`\\n\\nYou can test a model by running the model with the `ollama run` command.\\n\\n```bash\\n# run mistral\\nollama run mistral\\n```\\n\\n### Sending requests to the server\\n\\nHere is the code to load a model through Ollama:\\n\\n```python\\nlm = dspy.OllamaLocal(model=\\'mistral\\')\\n```\\n# dspy.HFModel\\n\\nInitialize `HFModel` within your program with the desired model to load in. Here\\'s an example call:\\n\\n```python\\nllama = dspy.HFModel(model = \\'meta-llama/Llama-2-7b-hf\\')\\n```# dspy.HFClientTGI\\n\\n## Prerequisites\\n\\n- Docker must be installed on your system. If you don\\'t have Docker installed, you can get it from [here](https://docs.docker.com/get-docker/).\\n\\n## Setting up the Text-Generation-Inference Server\\n\\n1. Clone the Text-Generation-Inference repository from GitHub by executing the following command:\\n\\n   ```\\n   git clone https://github.com/huggingface/text-generation-inference.git\\n   ```\\n\\n2. Change into the cloned repository directory:\\n\\n   ```\\n   cd text-generation-inference\\n   ```\\n\\n3. Execute the Docker command under the \"Get Started\" section to run the server:\\n\\n\\n   ```\\n   model=meta-llama/Llama-2-7b-hf # set to the specific Hugging Face model ID you wish to use.\\n   num_shard=2 # set to the number of shards you wish to use.\\n   volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\\n\\n   docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:0.9 --model-id $model --num-shard $num_shard\\n   ```\\n\\n   This command will start the server and make it accessible at `http://localhost:8080`.\\n\\nIf you want to connect to [Meta Llama 2 models](https://huggingface.co/meta-llama), make sure to use version 9.3 (or higher) of the docker image (ghcr.io/huggingface/text-generation-inference:0.9.3) and pass in your huggingface token as an environment variable.\\n\\n```\\n   docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data -e HUGGING_FACE_HUB_TOKEN={your_token} ghcr.io/huggingface/text-generation-inference:0.9.3 --model-id $model --num-shard $num_shard\\n```\\n\\n## Sending requests to the server\\n\\nAfter setting up the text-generation-inference server and ensuring that it displays \"Connected\" when it\\'s running, you can interact with it using the `HFClientTGI`.\\n\\nInitialize the `HFClientTGI` within your program with the desired parameters. Here is an example call:\\n\\n   ```python\\n   lm = dspy.HFClientTGI(model=\"meta-llama/Llama-2-7b-hf\", port=8080, url=\"http://localhost\")\\n   ```\\n\\n   Customize the `model`, `port`, and `url` according to your requirements. The `model` parameter should be set to the specific Hugging Face model ID you wish to use. \\n\\n\\n### FAQs\\n\\n1. If your model doesn\\'t require any shards, you still need to set a value for `num_shard`, but you don\\'t need to include the parameter `--num-shard` on the command line.\\n\\n2. If your model runs into any \"token exceeded\" issues, you can set the following parameters on the command line to adjust the input length and token limit:\\n   - `--max-input-length`: Set the maximum allowed input length for the text.\\n   - `--max-total-tokens`: Set the maximum total tokens allowed for text generation.\\n\\nPlease refer to the [official Text-Generation-Inference repository](https://github.com/huggingface/text-generation-inference) for more detailed information and documentation.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of documents 1 must match number of ids 43260",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43muuid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muuid4\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_docs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_docs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/chromadb/api/models/Collection.py:146\u001b[0m, in \u001b[0;36mCollection.add\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    106\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     uris: Optional[OneOrMany[URI]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    117\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m        ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     (\n\u001b[1;32m    140\u001b[0m         ids,\n\u001b[1;32m    141\u001b[0m         embeddings,\n\u001b[1;32m    142\u001b[0m         metadatas,\n\u001b[1;32m    143\u001b[0m         documents,\n\u001b[1;32m    144\u001b[0m         images,\n\u001b[1;32m    145\u001b[0m         uris,\n\u001b[0;32m--> 146\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_embedding_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# We need to compute the embeddings if they're not provided\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;66;03m# At this point, we know that one of documents or images are provided from the validation above\u001b[39;00m\n",
      "File \u001b[0;32m~/peter-projects/daybreak-ml-docs/.venv/lib/python3.10/site-packages/chromadb/api/models/Collection.py:595\u001b[0m, in \u001b[0;36mCollection._validate_embedding_set\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris, require_embeddings_or_data)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    592\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of metadatas \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(valid_metadatas)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must match number of ids \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(valid_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    593\u001b[0m     )\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(valid_documents) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(valid_ids):\n\u001b[0;32m--> 595\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    596\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of documents \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(valid_documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must match number of ids \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(valid_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    597\u001b[0m     )\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(valid_images) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(valid_ids):\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    600\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of images \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(valid_images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must match number of ids \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(valid_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    601\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Number of documents 1 must match number of ids 43260"
     ]
    }
   ],
   "source": [
    "collection.add(ids=[str(uuid.uuid4()) for _ in range(len(all_docs))], documents=all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_model = ChromadbRM(\"docs\", \"./chroma\", embedding_function=default_ef, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.settings.configure(lm=llm, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"hi how are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resp = requests.get('https://raw.githubusercontent.com/weaviate-tutorials/quickstart/main/data/jeopardy_tiny.json')\n",
    "# data = json.loads(resp.text)  \n",
    "\n",
    "# question_objs = list()\n",
    "# for i, d in enumerate(data):\n",
    "#     question_objs.append({\n",
    "#         \"answer\": d[\"Answer\"],\n",
    "#         \"question\": d[\"Question\"],\n",
    "#         \"category\": d[\"Category\"],\n",
    "\n",
    "# questions = weaviate_client.collections.get(\"Question\")\n",
    "# questions.data.insert_many(question_objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionToBlogOutline(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Your task is to write a blog post that will help answer the given question. \n",
    "    Please use the contexts to evaluate the structure of the blog post.\n",
    "    \"\"\"\n",
    "\n",
    "    question = dspy.InputField()\n",
    "    context = dspy.InputField()\n",
    "    blog_outline = dspy.OutputField(desc=\"A comma separated list of topics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(collection.get(include=['embeddings'])['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_question = \"How does hybrid search in Weaviate work?\"\n",
    "ex_context = dspy.Retrieve(k=5)(\"What is DSPy?\").passages\n",
    "ex_context = \"\".join(ex_context)\n",
    "dspy.ChainOfThought(QuestionToBlogOutline)(question=ex_question, context=ex_context).blog_outline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dspy.Retrieve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
