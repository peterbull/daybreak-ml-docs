version: "3.8"
services:
  ollama:
    networks:
      - backend_my_network
    build:
      context: .
      dockerfile: ollama.dockerfile
    command:
      [
        "bash",
        "-c",
        "ollama create ${OLLAMA_MODEL_ALIAS} -f /usr/share/modelfiles/Modelfile",
      ]
    # command: serve
    # entrypoint: ["tail", "-f", "/dev/null"]
    env_file:
      - .env
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ./ollama:/root/.ollama
      # - ./.ollama:/usr/share/ollama
      - ./downloads:/usr/share/downloads
      - ./modelfiles:/usr/share/modelfiles
    ports:
      - 11434:11434
    container_name: ollama

  app:
    networks:
      - backend_my_network
    build:
      context: .
      dockerfile: regenrequester.dockerfile
    environment:
      - POETRY_VERSION=1.7.1
      - PYTHONPATH=/usr/src/app
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - COLLECTOR_ENDPOINT=http://phoenix:6006/v1/traces
      - PROD_CORS_ORIGIN=http://localhost:3000
      # Set INSTRUMENT_LLAMA_INDEX=false to disable instrumentation
      - INSTRUMENT_LLAMA_INDEX=false
    env_file:
      - .env
    entrypoint: ["tail", "-f", "/dev/null"]
    volumes:
      - .:/usr/src/app
    depends_on:
      - ollama

  phoenix:
    networks:
      - backend_my_network
    image: arizephoenix/phoenix:latest
    ports:
      - "6006:6006"

volumes:
  ollama:

networks:
  backend_my_network:
    external:
      name: backend_my_network
